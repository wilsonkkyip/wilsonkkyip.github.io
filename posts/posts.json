[
  {
    "path": "posts/2023-08-09-sample-youtube-channels-id-with-async-functions/",
    "title": "Sampling YouTube Channels' Id with Async Functions",
    "description": "A quick way to collect YouTube channels' id from YouTube home page asynchronously.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-09",
    "categories": [
      "python",
      "async",
      "web-scraping"
    ],
    "contents": "\n\nContents\nIntroduction\nAsyncio\nCoroutines\nTasks\nEvent loops\nSemaphore\n\nScraping YouTube asynchronously\n\nIntroduction\nWhen exchanging information between nodes (like making http request, or querying a database, etc.), the client side usually is required to wait for the response from the server. In synchronous programming, commands / statements are executed line by line, meaning that if a statement is required to wait for a particular amount of time, every other statements below cannot be executed because they are blocked by this long waiting statement to complete.\nFor example if we want to query two tables from two different databases and process them afterwords, in synchronous programming, we will need to wait for the first query to complete in order to start the second query. If the response time of the both queries are 10 seconds, we need to spend 20 seconds just for waiting. Asynchronous programming allows the processor to execute other tasks when a particular command is required to wait.\nThis article will first introduce the asyncio library in Python. Then we will use this method to sample channel ids from YouTube for later use.\nAsyncio\nCoroutines\nThe first thing in asynchronous programming is to create an async function. To do so in Python, we simply add the keyword async before the def keyword. Yet, you cannot simply execute an async function. When execute an async function, it always returns a coroutine object. A coroutine is an awaitable and therefore can be awaited from other coroutines. Notice that the await keyword can only be used in an async function.\nBelow shows the awaitability of synchronous (time.sleep) and asynchronous (foo) functions.\n\nimport time\nimport asyncio\n\nasync def foo1():\n    time.sleep(1)   # time.sleep is not an async function and cannot be awaited\n    print(\"Hello world!\")\n\ntype(foo1())\n\n# <stdin>:1: RuntimeWarning: coroutine 'foo1' was never awaited\n# RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n# <class 'coroutine'>\n\nasync def foo2():\n    await foo1()    # foo1 is an async function and can be awaited\n\nTo execute a coroutine object, asyncio.run is required.\n\nasyncio.run(foo1())\n\n# Hello world!\n\ntype(asyncio.run(foo1()))\n\n# Hello world!\n# <class 'NoneType'>\n\nTasks\nTasks are used to run coroutines in event loops. In other words, a coroutine is required to be wrapped into a task in order to pass to an event loop for execution. A task can be created via asyncio.create_task or loop.create_task methods. Whenever a task is created via these methods, they will execute whenever the running theread is available. Besides, if we do not await a task to be finished, the task will be dropped when the last line of the async function is executed.\n\nimport asyncio\n\nasync def foo1():\n    for i in range(10):\n        print(i)\n        await asyncio.sleep(0.5)\n\nasync def foo2():\n    await asyncio.sleep(2)\n    print(\"Hi from foo2\")\n\nasync def main():\n    task1 = asyncio.create_task(foo1())\n    task2 = asyncio.create_task(foo2())\n    \n    # The above tasks will not be executed before this line\n    # Because the running thread is not available before this\n    print(\"The thread is busy before this\")  \n    \n    await task2    # this task will be awaited. But not task1\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n# The thread is busy before this\n# 0\n# 1\n# 2\n# 3\n# Hi from foo2\n\nEvent loops\nAn event loop is an object to manage a list of tasks. It identifies if the running thread is available for execute other tasks. The method asyncio.run simply acquires an event loop, convert the coroutine into a task, execute it and closing the threadpool. Below shows the same example as above but better illustrate the event loop.\n\nimport asyncio\n\nasync def foo1():\n    for i in range(10):\n        print(i)\n        await asyncio.sleep(0.5)\n\nasync def foo2():\n    await asyncio.sleep(2)\n    print(\"Hi from foo2\")\n\nasync def main():\n    task1 = asyncio.create_task(foo1())\n    task2 = asyncio.create_task(foo2())\n\n    print(\"The thread is busy before this\")\n    \n    await task2\n    print(type(asyncio.get_event_loop()))\n\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()  # Acquire an event loop\n    loop.run_until_complete(main())  # Execute the tasks\n    loop.close()                     # Close the loop\n\nSemaphore\nBy default, an event loop will put all the tasks to execute whenever there are resources. But sometimes we want to limit the number of concurrent tasks. In this case, asyncio.Semaphore may come in handy.\n\nimport asyncio\n\nasync def async_func(task_no, sem):\n    async with sem:\n        print(f'{task_no} :Hello ...')\n        await asyncio.sleep(1)\n        print(f'{task_no}... world!')\n\nasync def main(sem):\n    tasks = [\n        asyncio.create_task(async_func(x, sem)) for x in [\"t1\", \"t2\", \"t3\"]\n    ]\n    await asyncio.gather(*tasks)\n\nif __name__ == \"__main__\":\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    sem = asyncio.Semaphore(2)\n    result = asyncio.run(main(sem))\n    loop.close()\n\n# t1 :Hello ...\n# t2 :Hello ...\n# t1... world!\n# t2... world!\n# t3 :Hello ...\n# t3... world!\n\nScraping YouTube asynchronously\nIn this section, the code snippet below sample channel ids from YouTube main page using async functions. I am using httpx instead of requests because the later library does not provide async http client. I also set the cookie \"CONSENT\": \"YES+yt.463627267.en-GB+FX+553\" to consent YouTube using cookies tracking. Yet, I remove all cookies except this one every time I make the request to avoid YouTube gives me the same channels. Lastly, I use regex to extract all the channel ids from the html.\nThe whole script only used 7.7 seconds to complete 100 requests, which is much faster to use the synchronous client.\n\nimport re\nimport httpx\nimport asyncio\nfrom asyncio import Semaphore, create_task, gather\nfrom httpx import AsyncClient\nfrom time import time\nfrom itertools import chain\n\n\nasync def async_request_youtube(sem: Semaphore, client: AsyncClient):\n    async with sem:\n        client.cookies = {\"CONSENT\": \"YES+yt.463627267.en-GB+FX+553\"} \n        client.headers = {\n            'accept': '*/*', \n            'accept-encoding': 'gzip, deflate', \n            'connection': 'keep-alive', \n            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.5400.117 Safari/537.36'\n        }\n\n        response = await client.get(\"https://www.youtube.com\")\n        return response\n\nasync def sample_youtube_channels(n: int, sem: Semaphore):\n    async with httpx.AsyncClient() as client:\n        client.timeout = 10\n        tasks = [\n            create_task(async_request_youtube(sem, client)) for i in range(n)\n        ]\n        result = await gather(*tasks)\n    \n    return result\n\ndef request_youtube(n: int, concurrency: int = 50):\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    sem = asyncio.Semaphore(concurrency)\n    result = asyncio.run(sample_youtube_channels(n, sem))\n    loop.close()\n    return result\n\nif __name__ == \"__main__\":\n    start_ts = time()\n    result = request_youtube(100, 50)\n    channel_id_re = re.compile('\"(UC[A-z0-9_-]{22})\"')\n    channel_ids = [\n        channel_id_re.findall(res.text) for res in result\n    ]\n    channel_ids = list(set(list(chain(*channel_ids))))\n    with open(\"./youtube_channel_ids.txt\", \"w\") as f:\n        for cid in channel_ids:\n            _ = f.write(f\"{cid}\\n\")\n    end_ts = time()\n    print(f\"Time used: {end_ts - start_ts} seconds\")\n\n# Time used: 7.707166910171509 seconds\n\n\n\n\n",
    "preview": "posts/2023-08-09-sample-youtube-channels-id-with-async-functions/img/preview.png",
    "last_modified": "2023-08-13T14:36:09+00:00",
    "input_file": "sample-youtube-channels-id-with-async-functions.knit.md"
  },
  {
    "path": "posts/2023-08-08-recursive-programming-querying-nested-json-in-python/",
    "title": "Recursive Programming - Querying nested JSON in Python",
    "description": "Querying nested JSON in Python with recursive programming.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-08",
    "categories": [
      "python",
      "recursive-programming"
    ],
    "contents": "\n\nContents\nIntroduction\nRecursive function\nQuerying nested JSON dictionary\nExamples\nExample 1\nExample 2\nExample 3\n\n\nIntroduction\nAs we are requesting REST API heavily nowadays, we need to deal with JSON frequently. JSON objects can be transfer into Python dictionaries very easily by the json library. Yet, it is quite tedious to query nested JSON if it contains many layers.\n\nd = {\n    \"layer1_item1\": {\"layer2_item1\": {\"layer3_item1\": \"some_info\", \"layer3_item2\": \"another_info\"}},\n    \"layer1_item2\": {\"layer2_item1\": {\"layer3_item1\": \"some_info\", \"layer3_item2\": \"another_info\"}}\n}\n\nFor example if we want to query layer3_item2 from the above JSON,\n\nd.get(\"layer1_item1\").get(\"layer2_item1\").get(\"layer3_item2\")\n'another_info'\n\nIt involves so much brackets and quotes. I have written a recursive function to query these nested JSON dictionary in Python. But we will first take a look what a recursive functions is.\nRecursive function\nA recursive function means the function execute itself within its own definition. A simple but endless definition is shown below.\n\ndef foo():\n    foo()\n\nThe function foo calls itself within its own definition. It simply creates a loop (but an endless one in the above example).\nWe now take a look on a practical factorial function. Recall the factorial of \\(n\\) is given by\n\\[n! = n \\cdot (n - 1) \\cdot (n - 2) \\cdots 3 \\cdot 2 \\cdot 1.\\]\nIn Python, we can illustrate this by\n\ndef factorial(n):\n    if n == 1:\n        return n\n    return n * factorial(n - 1)\n\n\nfactorial(5)\n120\n\nQuerying nested JSON dictionary\nThe following function can query nested dictonary with syntax like key1.key2.key3 instead of multiple get, brackets and quotes. See some examples in the coming section.\n\nfrom typing import Any, Optional\n\ndef pjq(\n    json_dict: 'list | dict', \n    query: 'list[str] | str', \n    default: Optional[Any] = None, \n    sep: str = \".\", \n    idx_sep: str = \",\", \n    trim: bool = True, \n    prev_q: Optional[str] = None\n) -> Any:\n    query = query.split(sep) if isinstance(query, str) else query\n    # Cannot pop query index otherwise affecting the for loop in list\n    q: str = query[0]\n    query: list[str] = query[1:]\n    if json_dict == default:\n        # If `default` is set to a list of dict, it cannot go through this\n        return default\n\n    elif isinstance(json_dict, dict):\n        json_dict = json_dict.get(q, default)\n        \n        if query:\n            return pjq(json_dict, query, default, sep, idx_sep, trim=trim, prev_q=q)\n        return json_dict\n\n    elif isinstance(json_dict, list):\n        if q:\n            try:\n                idx: list[int] = [int(i) for i in q.split(idx_sep)]\n                json_dict = [json_dict[i] for i in idx]\n            except Exception:\n                return default\n        if query:\n            json_dict = [pjq(jd, query, default, sep, idx_sep, trim=trim, prev_q=q) for jd in json_dict]\n\n        if trim:\n            json_dict = json_dict[0] if len(json_dict) == 1 else json_dict\n\n        return json_dict\n\n    else:\n        return None \n\nExamples\nExample 1\n\nd = {\n    \"a\": {\n        \"b\": {\"b1\": 1, \"b2\": 2},\n        \"c\": {\"c1\": 3, \"c2\": 4}\n    }\n}\n\n\npjq(d, \"a.b.b2\")\n2\n\nExample 2\n\nd = {\n    \"a\": [\n        {\"b1\": 1, \"b2\": 2},\n        {\"b1\": 3, \"b2\": 4},\n        {\"b1\": 5, \"b2\": 6}\n    ]\n}\n\n\npjq(d, \"a.1,2.b2\")\n[4, 6]\n\n\npjq(d, \"a..b2\")\n[2, 4, 6]\n\nExample 3\n\nd = [\n    {\"a\": {\"b\": {\"x\": 1}, \"c\": {\"y\": 3}}},\n    {\"a\": {\"b\": {\"x\": 2}, \"c\": {\"y\": 4}}}\n]\n\n\npjq(d, \".a.c.y\")\n[3, 4]\n\n\n\n\n",
    "preview": "posts/2023-08-08-recursive-programming-querying-nested-json-in-python/img/preview.png",
    "last_modified": "2023-08-09T09:33:24+00:00",
    "input_file": "recursive-programming-querying-nested-json-in-python.knit.md"
  },
  {
    "path": "posts/2023-08-05-introduction-to-docker-with-airflow-and-postgres-stack/",
    "title": "Introduction to Docker with Airflow and Postgres stack",
    "description": "Deploying Docker stacks of Airflow and Postgres database with docker-compose.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-05",
    "categories": [
      "docker",
      "airflow",
      "postgres"
    ],
    "contents": "\n\nContents\nIntroduction\nDocker Compose YAML File\nService Name\nImage Name\nPorts\nEnvironment\nVolumes\nAnchor\nStarting containers\n\nAirflow and Postgres stack\nObtaining docker compose YAML template\nCustomise the YAML file (optional)\nCreate initial databases and tables (optional)\nStart the stack\n\n\nIntroduction\nDocker is a software hosting containers. A container is an isolated place on top of the running operating system (OS) where an application can run without affecting other applications on the OS. Developers have been creating images to run on Docker. An image is a template of an application, like Postgres, that are ready to run in a container. It allows users to take the image and run it in seconds in a container and save tremendous times to install the application from scratch.\nIn this document, we will install a stack of containers to run Apache Airflow, a workflow management tool, and Postgres database. In later articles, we will be writing ETL workflows on Airflow using this stack. You can find the code source from my Github repo. If you have not installed Docker and Docker Compose, please follow the documentation below:\nDocker: https://www.docker.com/products/docker-desktop/\nDocker Compose: https://docs.docker.com/compose/install/\nWe will first describe how to tell Docker what images we want to use in a docker-compose.yaml specification file. Then we will setup Airflow and Postgres as an example.\nDocker Compose YAML File\nIn this section, we will briefly introduce how to specify what images to use in a docker-compose.yaml. We first take a look on a sample of the YAML file for creating a Postgres container.\n\nversion: '3.8'           # Version of the specification file \n\nservices: \n  mypostgres:            # Name of the local container\n    image: postgres:13   # image_name:version\n    ports: \n      - \"5431:5432\"      # [local port]:[cotainer port]\n    environment:         # The key-value pairs depends on image\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}\n      POSTGRES_DB: airflow\n    volumes:\n      - /path/in/local:/path/in/container\n      - /another/path:/another/path\n    \n  myairflow:             # Another container\n    ...\n\nA full documentation on the sections can be found here. Below we will only introduce some of them.\nService Name\nFrom the above example, we have specified 2 containers, one is mypostgres and the other is myairflow. These are the local names of the containers, where you can see them from\n\ndocker container ls\n\nImage Name\nUnder the block mypostgres, we have specified to use the image postgres:13. We can find published images from https://hub.docker.com/ and put the image’s name in docker-compose.yaml. We can also specify the version of the image. In this example, we are using version 13.\n\n\n\nPorts\nWe have also put 5431:5432 as the port configuration, meaning that we can use the local port 5431 to connect to the port 5432 of the container. In other words, we can connect the database via the following command\n\npsql postgres://username:password@localhost:5431\n\nEnvironment\nThe environment section stores the environment variables to be passed to the container. In this example, we are passing POSTGRES_USER, POSTGRES_PASSWORD and POSTGRES_DB to the container. The image will pick up these environment variables to build the Postgres database in the container.\nWhen we are passing sensitive information like the password in this example, we can create and store key-value pairs in .env file besides the docker-compose.yaml. Then we can use the keys defind in the .env file in the specification file. The expression ${POSTGRES_PASSWORD:-password} means it will look for the key-value pair inside .env and password will be use instead if the key-value pair cannot be found. Below shows and example of the .env file.\n\nPOSTGRES_PASSWORD=\"AnotherPassword\"\n\nVolumes\nWe can share files and directories under the volumes session with the below format.\n\n/path_to_local_file:/path_to_container_file\n\nAnchor\nWhen working with a stack of containers, it is common to repeated use some configuration. For example to put the same environment variables into different containers. From the example below, we can both the services airflow-scheduler and airflow-website share the same set of configuration on image and environment from the airflow-common configuration.\n\nx-airflow-common:\n  &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.6.3}\n  environment:\n    AIRFLOW__CORE__EXECUTOR: LocalExecutor\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n  ...\n  \nservices:\n  airflow-scheduler:\n    <<: *airflow-common\n    command: scheduler\n    ...\n    \n  airflow-webserver:\n    <<: *airflow-common\n    command: webserver\n    ...\n\nStarting containers\nWe can start a single service from the stack with the following command:\n\ndocker-compose up <service-name>\n\nOr start the whole stack with the following command:\n\ndocker-compose up --build -d\n\nThe option -d lets the stack to run in background. We can check the status of each container with the following command:\n\ndocker container ls\n\n# CONTAINER ID   IMAGE                  ...   PORTS                    NAMES\n# 4c26970df34a   postgres:13            ...   0.0.0.0:5431->5432/tcp   postgres-mypostgres-1\n# 05365378f0f0   apache/airflow:2.6.3   ...   8080/tcp                 yt-docker-stack-airflow-triggerer-1\n# cf43be7e9755   apache/airflow:2.6.3   ...   8080/tcp                 yt-docker-stack-airflow-scheduler-1\n# 12fdbf84054e   apache/airflow:2.6.3   ...   0.0.0.0:8081->8080/tcp   yt-docker-stack-airflow-webserver-1\n# 73cfeea83789   postgres:13            ...   0.0.0.0:5432->5432/tcp   yt-docker-stack-postgres-1\n\nWe can also go into the bash environment of a container with the following command:\n\ndocker exec -it <container_id | container_name> bash\n\nFinally, if we want to close the whole stack of containers, we can use the following command:\n\ndocker-compose down -v\n\nThe option -v removes all the volumes used by the stack.\nAirflow and Postgres stack\nIn this section, we will show an example to build a stack of containers with Apache Airflow and Postgres database.\nObtaining docker compose YAML template\nWe need to first obtain a docker-compose.yaml template file from here. I was using Airflow version 2.6.3 at the moment. Hence the YAML file is\nhttps://airflow.apache.org/docs/apache-airflow/2.6.3/docker-compose.yaml\nCustomise the YAML file (optional)\nIn this example, I have customised the YAML file to fit my purposes. You can find my specification file from Github.\nI will be using the LocalExecutor instead of the CeleryExecutor. Hence I have changed the environment variable AIRFLOW__CORE__EXECUTOR from CeleryyExecutor to LocalExecutor under the airflow-common section, and removed the following items:\n\n# THE FOLLOWING SECTIONS ARE REMOVED\nx-airflow-common:\n  environment:\n    AIRFLOW__CELERY__RESULT_BACKEND: ...\n    AIRFLOW__CELERY__BROKER_URL: ...\n  depends_on:\n    redis:\n      ...\n\nservices:\n  redis:\n    ...\n\n  airflow-worker:\n    ...\n  \n  flower:\n    ...\n\nI have also changed / added the followings:\n\n# THE FOLLOWING VALUES ARE CHANGED / ADDED\nx-airflow-common:\n  environment:\n    AIRFLOW__CORE__EXECUTOR: LocalExecutor\n    AIRFLOW_INPUT_DIR: '/opt/airflow/dag-inputs'\n    POSTGRES_USER: airflow\n    POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n    GAPI_KEY: ${GAPI_KEY}\n    \nservices:\n  postgres:\n    ports:\n      - \"5432:5432\"\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: airflow\n    volumes:\n      - postgres-db-volume:/var/lib/postgresql/data\n      - ./init/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql\n      - ./init/postgres:/mnt/sources/init\n  \n  airflow-init:\n    environment:\n      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_PASSWORD}\n\nCreate initial databases and tables (optional)\nThe Postgres docker image will execute the files in /docker-entrypoint-initdb.d during initialisation. One can put some queries in this folder for initial execution, which is why I share some volumes in the above section.\n\nCREATE DATABASE youtube;\n\\c youtube;\n\\i /mnt/sources/init/enum_iso3166.sql;\nCREATE TABLE channel (\n    uuid UUID default gen_random_uuid() NOT NULL,\n    etag CHAR(27) NOT NULL,\n    id CHAR(24) NOT NULL,\n    title VARCHAR(100) NOT NULL,\n    \"description\" VARCHAR(5000),\n    custom_url VARCHAR(31),\n    published_at Timestamp,\n    country country_alpha2,\n    uploads CHAR(24),\n    view_count INTEGER,\n    subscriber INTEGER,\n    video_count INTEGER,\n    topic_category TEXT[],\n    updated_at TIMESTAMP NOT NULL default CURRENT_TIMESTAMP\n);\n\nStart the stack\nAs described in previous sections, I now start the stack with the following cammands:\n\ndocker-compose up airflow-init\n\ndocker-compose up --build -d\n\nAnd you can find Airflow is running on http://localhost:8080 or via docker container ls.\n\n\n\n\n\n\n",
    "preview": "posts/2023-08-05-introduction-to-docker-with-airflow-and-postgres-stack/img/preview.png",
    "last_modified": "2023-08-08T13:07:36+00:00",
    "input_file": "introduction-to-docker-with-airflow-and-postgres-stack.knit.md"
  },
  {
    "path": "posts/2023-08-03-github-actions-with-example/",
    "title": "Github Actions with Example",
    "description": "An introduction to Github Actions with an example to write a post whenever a new blog post is merged to the main branch.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-03",
    "categories": [
      "continuous-delivery",
      "cicd",
      "github-actions",
      "github"
    ],
    "contents": "\n\nContents\nIntroduction\nGithub Actions\nWorkflows\nEvents\nJobs\nActions\nRunners\nSecrets\nGithub Context\n\nLinkedIn API\nOAuth2\nCalling API\nIdentify User Id\nWrite Post\n\n\nAuto Posting Workflow\nThe Workflow\nThe Python Script\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\nIn modern software development, an engineer’s job does not end when a product is developed. Numerous times are spent on testing and deploying the product, no matter if the product is a website or a programming library or anything. Usually these tasks are repetitive and boring because these products are required to be maintained and updated. The same testing and deploying process will need to be rerun again throughout the life-cycle of the product.\nThe same problem happens on data scientists and machine learning engineers as well, where the models they have developed are also required to be tested and deployed (and updated and tested and deployed again and again). The concept of continuous integration and delivery came to automate these repetitive tasks and saves our precious time.\nThis article describes these concepts through an example – write a LinkedIn post whenever a new blog post is created in this blog. We will first briefly go through what Github Actions is, then we will talk about how to write a post on LinkedIn through its API. Finally we will create a workflow to check if there is a new blog post and write a LinkedIn post if there is.\nGithub Actions\nGithub Actions is a platform for continuous integration / continuous delivery (CI/CD). One can write workflows to automate build, testing, and deployment pipelines. Each workflow is triggered by one or more events and can be run by different runners. We will describe these concepts more below.\nEach workflow must be defined in the folder of .github/workflows in a repo and it must be specified in a YAML file like below. We will go through each section of the file.\n\n\n# Workflow Name\nname: Release Process\n\non:\n  # Events\n  push:                                   # One event\n    branches:\n      - main\n\n  workflow_run:                           # Another event\n    workflows: [pages-build-deployment]\n    types: \n      - completed\n\njobs:\n  # Job\n  generate-release:                 # Job id\n    name: Create GitHub Release     # Job name\n    runs-on: ubuntu-latest          # Runner\n    steps:\n    - name: Checkout Repository     # Step1\n      uses: actions/checkout@v2     # Actions\n      \n    - name: Run release code        # Step2\n      run: |\n        cd /target/directory\n        ./run-release-code\n  \n  # Another Job\n  another-job:                      # Job id\n    name: Another Job               # Job name\n    needs: [generate-release]       # Requires the job to complete successfully\n    runs-on: ubuntu-latest          # Runner\n    steps:\n    - name: Checkout Repository     # Step1\n      uses: actions/checkout@v2     # Actions\n      \n    - name: do other stuffs         # Step2\n      run: echo $CUSTOM_VAR\n      env: \n        CUSTOM_VAR: \"${{ secrets.CUSTOM_VAR }}\" # Secret value\n\n\nWorkflows\nThe entire YAML file specified in this code chunk is a workflow. There can be multiple workflows in different YAML files stored inside .github/workflows directory. Each workflow can be triggered by one or more events, or they can be triggered manually, or at a defined schedule. Each workflow can also contains one or more jobs.\nEvents\nAn event is an activity within the repository. For example, an event can be a pull / push request. It can also be the completion of another workflow or scheduled by cron syntax.\nThe above workflow will be triggered whenever one of the two specified events occurs. These two events are\nEvery time the main branch is pushed or merged from another branch, this workflow will be started.\nWhenever another workflow pages-build-deployment is completed, this workflow will be started.\nJobs\nA job is a series of steps that will be executed on the same runner. Each step is either a shell script or an action. The steps will be executed in order and dependent on each other. By default, each job will be run by a different runner and concurrently. One can specify the dependency of jobs by the key needs. The above example shows an implementation.\nAlso, one can also specify a strategy matrix to repeat the same job for different conditions. For example, the following job will be executed 6 times, namely\n{node-version: 10, os: ubuntu-22.04}\n{node-version: 10, os: ubuntu-20.04}\n{node-version: 12, os: ubuntu-22.04}\n{node-version: 12, os: ubuntu-20.04}\n{node-version: 14, os: ubuntu-22.04}\n{node-version: 14, os: ubuntu-20.04}\n\njobs:\n  example_matrix:\n    strategy:\n      matrix:\n        os: [ubuntu-22.04, ubuntu-20.04]\n        version: [10, 12, 14]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/setup-node@v3\n        with:\n          node-version: ${{ matrix.version }}\n\nActions\nActions are custom applications for GitHub Actions that perform complex but repetitive tasks. You can write an action from scratch or use an existing action available from the GitHub Marketplace in your workflow.\nRunners\nA runner is an OS on a virtual machine or container to execute a specific job. GitHub provides Ubuntu Linux, Microsoft Windows, and macOS runners to run the workflows. One can also host their own machine as runner.\nSecrets\nFor each step or job, one can specify an env session to define environment variables. But if we are dealing with credentials, this might not be a good choice. One can go to Settings of the repository, under Security, click Secrets and variables, then click Actions. Inside the page, one can define secrets for the repository and can access them within the env session inside a workflow as shown in the example.\nGithub Context\nContexts are a way to access information about workflow runs, variables, runner environments, jobs, and steps. For example the name of the working branch, the working directory of Github Actions, etc. The keyword secrets in the above section is also a context. See more from this page.\nLinkedIn API\nLinkedIn offers various API products for consumers to do various of things. One of which is to write posts on behalf of the users (see this documentation). To do that, we need to\nCreate a company on LinkedIn\nCreate an application on behalf of the company\nAuthenticate yourself and authorise the application to write posts on behalf of you\nThe process is similar to my previous blog post about OAuth2 for Google APIs. I will briefly describe the process here.\nOAuth2\nWe will first create a company on LinkedIn and the application.\nGo to https://developer.linkedin.com/ and click Create App (and login to your LinkedIn account)\nEnter the name of the application\nClick Create a new LinkedIn Page if you do not have a company on LinkedIn\nSelect Company\nEnter the name of the company, select the industry, company size, company type. Check the terms and click Create page\nGo back to the developer page and select the company just created\nUpload a logo for the application\nCheck the Legal agreement and click Create app\nClick Verify and follow the instruction\nClick Products, click Request access for both Share on LinkedIn and Sign in with LinkedIn\nClick Auth and copy the Client ID and Client Secret\nUnder OAuth 2.0 settings, enter the authorised redirect url\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we have the client_id, client_secret and redirect_uri ready, we can now authenticate ourselves and authorise the application. The following script will generate a url to login to your LinkedIn account. Then it will generate the access_token.\n\nimport os\nfrom urllib.parse import urlencode, urlparse\nfrom http.server import HTTPServer, BaseHTTPRequestHandler\nimport json\nimport requests\nimport webbrowser\n\nclient_id = os.getenv(\"CLIENT_ID\")\nclient_secret = os.getenv(\"CLIENT_SECRET\")\nredirect_uri = os.getenv(\"REDIRECT_URI\")\nscope = \"r_liteprofile w_member_social openid profile email\"\n\ndef parse_query(path):\n    parsed_url = urlparse(path)\n    query = parsed_url.query.split(\"&\")\n    query = [x.split(\"=\") for x in query]\n    query = {x[0]: x[1] for x in query}\n    return query\n\ndef auth_code(code, client_id, client_secret, redirect_uri):\n    params = {\n        \"grant_type\": \"authorization_code\",\n        \"code\": code,\n        \"redirect_uri\": redirect_uri,\n        \"client_id\": client_id,\n        \"client_secret\": client_secret\n    }\n    headers = {\n        \"content-type\": \"application/x-www-form-urlencoded\",\n        \"content-length\": \"0\"\n    }\n    url = \"https://www.linkedin.com/oauth/v2/accessToken\"\n    response = requests.post(url, params=params, headers=headers)\n    response.raise_for_status()\n    content = response.json()\n    return content\n\nclass NeuralHTTP(BaseHTTPRequestHandler):\n    def do_GET(self):\n        path = self.path\n        query = parse_query(path)\n\n        code = query.get(\"code\")\n        if code:\n            status_code = 200\n            content = auth_code(\n                code=query.get(\"code\"),\n                client_id=client_id,\n                client_secret=client_secret,\n                redirect_uri=redirect_uri\n            )\n            print(json.dumps(content, indent=4))\n        else:\n            status_code = 400\n            content = {\n                \"error\": \"code not found\"\n            }\n\n        self.send_response(status_code)\n        self.send_header(\"Content-Type\", \"application/json\")\n        self.end_headers()\n        self.wfile.write(bytes(json.dumps(content, indent=4), \"utf-8\"))\n    \n    def log_message(self, format, *args):\n        \"\"\"Silence log message. Can be ignored.\"\"\"\n        return\n\nif __name__ == \"__main__\":\n    with HTTPServer((\"127.0.0.1\", 8088), NeuralHTTP) as server:\n        auth_url = \"https://www.linkedin.com/oauth/v2/authorization\"\n        params = {\n            \"client_id\": client_id,\n            \"response_type\": \"code\",\n            \"redirect_uri\": redirect_uri,\n            \"scope\": scope,\n        }\n\n        url = f\"{auth_url}?{urlencode(params)}\"\n        webbrowser.open(url)\n        server.handle_request()\n\n\n# {\n#     \"access_token\": \"...\",\n#     \"expires_in\": 5183999,\n#     \"scope\": \"email,openid,profile,r_liteprofile,w_member_social\",\n#     \"token_type\": \"Bearer\",\n#     \"id_token\": \"...\"\n# }\n\nCalling API\nIdentify User Id\nTo write a post on LinkedIn, We need to first identify the author’s user_id. A GET request to https://api.linkedin.com/v2/userinfo with the access_token obtained from the above are needed.\n\nimport os\nimport requests \nimport json\n\nurl = \"https://api.linkedin.com/v2/userinfo\"\ntoken = os.getenv(\"LINKEDIN_ACCESS_TOKEN\")\n\nheaders = {\"Authorization\": f\"Bearer {token}\"}\n\nresponse = requests.get(url, headers=headers)\nresponse.raise_for_status()\ncontent = response.json()\nprint(json.dumps(content, indent=4))\n\n\n{\n    \"sub\": \"....\",\n    \"email_verified\": true,\n    \"name\": \"Wilson Yip\",\n    \"locale\": {\n        \"country\": \"US\",\n        \"language\": \"en\"\n    },\n    \"given_name\": \"Wilson\",\n    \"family_name\": \"Yip\",\n    \"email\": \"wilsonyip@elitemail.org\",\n    \"picture\": \"https://media.licdn.com/dms/image/C4E03AQGo1BKbUYmyBA/profile-displayphoto-shrink_100_100/0/1646639382257?e=1696464000&v=beta&t=6lhHrDK3vx6GOC01wIKkfVYAmCiSWoZtc8XpE0JoUmM\"\n}\n\nThe user_id is stored in the sub value.\nWrite Post\nWe will be calling the Share in LinkedIn endpoint to write a post in LinkedIn along with the specific request body to attach an article to the post. The following scripts shows an example.\n\nimport os\nimport requests \n\ndef build_post_body(\n    user_id, \n    post_content, \n    media_title, \n    media_description, \n    article_url\n):\n    body = {\n        \"author\": f\"urn:li:person:{user_id}\",\n        \"lifecycleState\": \"PUBLISHED\",\n        \"specificContent\": {\n            \"com.linkedin.ugc.ShareContent\": {\n            \"shareCommentary\": {\n                    \"text\": post_content\n                },\n                \"shareMediaCategory\": \"ARTICLE\",\n                \"media\": [\n                    {\n                        \"status\": \"READY\",\n                        \"description\": {\n                            \"text\": media_description\n                        },\n                        \"originalUrl\": article_url,\n                        \"title\": {\n                            \"text\": media_title\n                        }\n                    }\n                ]\n            }\n        },\n        \"visibility\": {\n            \"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"\n        }\n    }\n    return body\n\nif __name__ == \"__main__\":\n    linkedin_user_id = os.getenv(\"LINKEDIN_USER_ID\")    # user_id \n    linkedin_token = os.getenv(\"LINKEDIN_TOKEN\")        # access_token\n    linkedin_post_endpoint = \"https://api.linkedin.com/v2/ugcPosts\"\n\n    headers = {\n        \"X-Restli-Protocol-Version\": \"2.0.0\",\n        \"Authorization\": \"Bearer \" + linkedin_token \n    }\n\n    body = build_post_body(\n        user_id=linkedin_user_id,\n        post_content=\"Content of the LinkedIn post\",\n        media_title=\"The title of the article\",\n        media_description=\"The description of the article\",\n        article_url=\"https://www.link-to-article.com/article\"\n    )\n\n    response = requests.post(\n        url=linkedin_post_endpoint, \n        json=body, \n        headers=headers\n    )\n\nAuto Posting Workflow\nA workflow is created to write a post on LinkedIn whenever there is a new article merged to the main branch of a repository. The workflow is triggered every time after completion of the pages-build-deployment workflow, which is the workflow to build the website. Yet, there is a problem:\n\nWe need to keep tract which article was posted to LinkedIn already in order to define which article is new.\n\nFor simplicity, I have created a Google Sheet to store the article paths and the corresponding LinkedIn post_id. If an article’s path does not appear in the table, that is the new article and will further trigger the scripts.\nThe workflow is quite simple. It just runs a Python file. The Python file will check if there are any new article path, write a LinkedIn post if there is one, and update the log file.\nThe Workflow\n\nname: create-linkedin-post\n\non:\n  workflow_run:\n    workflows: [pages-build-deployment]\n    types: \n      - completed\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Chekcout\n        uses: actions/checkout@v3\n      \n      - name: Install python dependencies\n        run: pip install pyyaml\n      \n      - name: Create Linkedin Post\n        run: python ./tools/cd/linkedin_post.py\n        env: \n          LINKEDIN_USER_ID: ${{ secrets.LINKEDIN_USER_ID }}\n          LINKEDIN_TOKEN: ${{ secrets.LINKEDIN_TOKEN }}\n          GCP_CLIENT_EMAIL: ${{ secrets.GCP_CLIENT_EMAIL }}\n          GCP_PRIVATE_KEY_ID: ${{ secrets.GCP_PRIVATE_KEY_ID }}\n          GCP_PRIVATE_KEY: ${{ secrets.GCP_PRIVATE_KEY }}\n          LINKEDIN_POSTS_LOG_SSID: ${{ secrets.LINKEDIN_POSTS_LOG_SSID }}\n          LINKEDIN_POSTS_LOG_RANGE: ${{ secrets.LINKEDIN_POSTS_LOG_RANGE }}\n\nThe Python Script\n\n#!/usr/bin/python\n\nimport os\nimport requests \nimport json \nfrom time import time\nimport jwt\nimport yaml\n\ndef build_post_body(\n    user_id, \n    post_content, \n    media_title, \n    media_description, \n    article_url\n):\n    body = {\n        \"author\": f\"urn:li:person:{user_id}\",\n        \"lifecycleState\": \"PUBLISHED\",\n        \"specificContent\": {\n            \"com.linkedin.ugc.ShareContent\": {\n            \"shareCommentary\": {\n                    \"text\": post_content\n                },\n                \"shareMediaCategory\": \"ARTICLE\",\n                \"media\": [\n                    {\n                        \"status\": \"READY\",\n                        \"description\": {\n                            \"text\": media_description\n                        },\n                        \"originalUrl\": article_url,\n                        \"title\": {\n                            \"text\": media_title\n                        }\n                    }\n                ]\n            }\n        },\n        \"visibility\": {\n            \"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"\n        }\n    }\n    return body\n\ndef find_latest_missing_post(page_posts, linkedin_posts):\n    page_post_paths = [x.get(\"path\") for x in page_posts]\n    linkedin_post_paths = [x.get(\"path\") for x in linkedin_posts]\n    missing_idx = [\n        i for i, x in enumerate(page_post_paths) if x not in linkedin_post_paths\n    ]\n    \n    if missing_idx:\n        missing_paths = [page_post_paths[i] for i in missing_idx]\n        missing_post_dates = [page_posts[i].get(\"date\") for i in missing_idx]\n        latest_missing_post = missing_paths[missing_post_dates.index(max(missing_post_dates))]\n        latest_missing_post = page_posts[page_post_paths.index(latest_missing_post)]\n    else:\n        latest_missing_post = None\n\n    return latest_missing_post\n\ndef read_rmd_yml(path):\n    with open(path, \"r\") as f:\n        rmd_yml = f.readlines()\n    \n    yml_idx = [i for i, x in enumerate(rmd_yml) if x == \"---\\n\"]\n    return yaml.safe_load(\"\".join(rmd_yml[(yml_idx[0]+1):(yml_idx[1])]))\n\ndef auth_gapi_token(client_email, private_key_id, private_key):\n    payload: dict = {\n        \"iss\": client_email,\n        \"scope\": \"https://www.googleapis.com/auth/drive\",\n        \"aud\": \"https://oauth2.googleapis.com/token\",\n        \"iat\": int(time()),\n        \"exp\": int(time() + 3599)\n    }\n    headers: dict[str, str] = {'kid': private_key_id}\n\n    signed_jwt: bytes = jwt.encode(\n        payload=payload,\n        key=private_key.replace(\"\\\\n\", \"\\n\"),\n        algorithm=\"RS256\",\n        headers=headers\n    )\n\n    body: dict = {\n        \"grant_type\": \"urn:ietf:params:oauth:grant-type:jwt-bearer\",\n        \"assertion\": signed_jwt\n    }\n    response: requests.Response = requests.request(\n        \"POST\", \"https://oauth2.googleapis.com/token\", json=body\n    )\n\n    response.raise_for_status()\n\n    content = response.json()\n    return content.get('access_token')\n\ndef read_gsheet(ssid, ranges, token):\n    url = f\"https://sheets.googleapis.com/v4/spreadsheets/{ssid}/values/{ranges}\"\n    headers = {\n        \"Authorization\": f\"Bearer {token}\"\n    }\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n\ndef append_gsheet(ssid, ranges, data, token):\n    url = f\"https://sheets.googleapis.com/v4/spreadsheets/{ssid}/values/{ranges}:append\"\n\n    body = {\n        \"range\": ranges,\n        \"majorDimension\": \"ROWS\",\n        \"values\": data\n    }\n    headers = {\n        \"Authorization\": f\"Bearer {token}\"\n    }\n    response = requests.post(url, params={\"valueInputOption\": \"RAW\"}, headers=headers, json=body)\n    response.raise_for_status()\n\ndef create_linkedin_post(post):\n    linkedin_user_id = os.getenv(\"LINKEDIN_USER_ID\")\n    linkedin_token = os.getenv(\"LINKEDIN_TOKEN\")\n    linkedin_post_endpoint = \"https://api.linkedin.com/v2/ugcPosts\"\n\n    rmd_file = os.listdir(f\"./_{post['path']}\")\n    rmd_file = list(filter(lambda x: \".rmd\" in x.lower(), rmd_file))[0]\n\n    rmd_yml = read_rmd_yml(f\"./_{post['path']}/{rmd_file}\")\n    post_note = \"The post was created by Github Actions.\\nhttps://github.com/wilsonkkyip/wilsonkkyip.github.io\"\n    abstract = rmd_yml[\"abstract\"] + f\"\\n\\n{post_note}\"\n\n    body = build_post_body(\n        user_id=linkedin_user_id,\n        post_content=abstract,\n        media_title=rmd_yml[\"title\"],\n        media_description=rmd_yml[\"description\"],\n        article_url=f\"https://wilsonkkyip.github.io/{post['path']}\"\n    )\n\n    headers = {\n        \"X-Restli-Protocol-Version\": \"2.0.0\",\n        \"Authorization\": \"Bearer \" + linkedin_token \n    }\n\n    response = requests.post(\n        url=linkedin_post_endpoint, \n        json=body, \n        headers=headers\n    )\n\n    content = response.json()\n\n    return content\n\n\ndef main():\n    gcp_client_email = os.getenv(\"GCP_CLIENT_EMAIL\")\n    gcp_private_key_id = os.getenv(\"GCP_PRIVATE_KEY_ID\")\n    gcp_private_key = os.getenv(\"GCP_PRIVATE_KEY\")\n\n    log_ssid = os.getenv(\"LINKEDIN_POSTS_LOG_SSID\")\n    log_range = os.getenv(\"LINKEDIN_POSTS_LOG_RANGE\")\n\n    gcp_token = auth_gapi_token(\n        gcp_client_email, gcp_private_key_id, gcp_private_key\n    )\n\n    logs = read_gsheet(log_ssid, log_range, gcp_token)\n    linkedin_posts = [\n        {logs[\"values\"][0][0]: x[0], logs[\"values\"][0][1]: x[1]} for x in logs[\"values\"][1:]\n    ]\n\n    with open(\"./posts/posts.json\", \"r\") as file:\n        page_posts = json.loads(file.read())\n\n    missing_post = find_latest_missing_post(page_posts, linkedin_posts)\n\n    if missing_post:\n        response = create_linkedin_post(missing_post)\n        appending_data = [[missing_post[\"path\"], response.get(\"id\")]]\n        append_gsheet(log_ssid, log_range, appending_data, gcp_token)\n\nif __name__ == \"__main__\": \n    main()\n\n\n\n\n",
    "preview": "posts/2023-08-03-github-actions-with-example/img/github-actions.png",
    "last_modified": "2023-08-03T17:50:00+00:00",
    "input_file": "github-actions-with-example.knit.md"
  },
  {
    "path": "posts/2023-07-29-rust-gapi-oauth2/",
    "title": "Google OAuth2 Implementation on Rust Reqwest",
    "description": "An implementation of Google OAuth2 procedures on Rust reqwest for Server-side Web Apps and Service Accounts.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-07-29",
    "categories": [
      "rust",
      "oauth2",
      "google-oauth"
    ],
    "contents": "\n\nContents\nIntroduction\nOAuth2 Procedures\nCreate Google Cloud Project\nSelect Required Library\nCreate OAuth Client ID Secrets\nConfigure OAuth Consent Screen\nCreate OAuth Client ID for Users\n\nService Account\n\nRust Reqwest Implementation\nAuthorise Client Application (Client ID)\nAuthorise Client Application (Refresh Token)\nAuthorise Service Account\n\nExamples\n\n\n\n\nIntroduction\nIt comes to me on many occasions that Google APIs are required to complete my tasks. API keys may be an easy choice for those non-sensitive scopes (for example calling YouTube API for some public videos and channels). But when it comes to handling files in Google Drive, things become complicated as the service requires authentication and authorisation. This article aims to provide a solution on obtaining an authorised token to be put in http requests’ header for calling the Google APIs’ sensitive scopes in Rust environment.\nA Github repo was created for the purpose. It can be used in CLI environment and imported as rust crate as well. Below will first briefly describe the OAuth2 procedures, then walk through some important script, and finally will show some examples using of the crate.\nOAuth2 Procedures\nWhen I first encountered OAuth2, I was confused about what scopes and endpoints are because both scopes and endpoints are represented by url-like strings in Google APIs. In a nutshell, endpoints represent what services you want to use. For example there is a specific endpoint for reading the metadata of a file in Google Drive; there is another endpoint for you to update the file. On the other hand, scopes are the abilities of your authorised token. For example, is your token able to read the files from Google Drive? It depends on whether your token contains the specific scope.\nGoogle separates the authorisation method for server-to-server interactions and user-to-server interactions. We will use a service account for the prior situation and a client secret for the later one. Both can be represented by a JSON file. To obtain these JSON files, we first need to create a Google Cloud Project. Then within the project, we can create the secret JSON files.\n\n\n\n\n\n\nCreate Google Cloud Project\nGo to https://console.developers.google.com and click Select a project.\nClick New Project.\nEnter the Project name and click Create.\n\n\n\n\n\n\n\n\n\n\n\n\nSelect Required Library\nUnder APIs and service, click Library.\nSearch the API library(ies) you wish to use. In this example, we choose Google Drive API.\nClick the library you want.\nClick Enable.\n\n\n\n\n\n\n\n\n\n\n\n\nCreate OAuth Client ID Secrets\nNow we have created a project and picked the required libraries. This section will show how to obtain a client_secret of the application for users to authorise. In order to do so, we need to first configure an OAuth consent screen to inform users about the name of the application and which scopes will be used by the application when they do the authorisation. Then we will create the application secret (or client_secret) for this application.\nConfigure OAuth Consent Screen\nUnder APIs and services, Credentials, click Configure consent screen or OAuth consent screen.\nIf you are within an organisation, you can pick Internal or External as User Type. Otherwise, you can only pick External.\nFor internal apps, it is only available to users within the organisation. But the app is not required to have any privacy policy.\nFor external apps, you can add at most 100 test users for testing the application before published. But the refresh_token obtained from the authorisation and authentication process is only valid for 1 week only.\n\nEnter the App name and User support email.\nScroll down and enter the developer contact information and click Save and continue.\nClick Add or remove scopes.\nSelect the scopes you want to use.\nClick Save and continue.\n(For external apps only) Click Add users as test users for the application.\n(For external apps only) Enter the email address(es) for the test user(s). Then click Add.\nClick Save and continue.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate OAuth Client ID for Users\nUnder APIs and services, Credentials, click Create Credentials, then click OAuth client ID.\nSelect Web application as Application type and enter the name of the application.\nScroll down and enter the Authorised redirect URIs. Please put a slash (/) at the end of the uri. Then click Create.\nFinally click Download JSON.\n\n\n\n\n\n\n\n\n\n\n\n\nService Account\nThis section describe how to obtain a service account JSON. If you wish to handle users’ data, please follow this section.\nUnder APIs and services, Credentials, click Create Credentials, then click Service Account.\nInsert the name, account id, and description of the service account. Then click Done.\nClick the newly created service account email.\nClick Keys, then clickAdd key and Create new key.\nSelect JSON as key type and click Create to download the service account JSON.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRust Reqwest Implementation\nNow we have obtained the secret JSON (either a client_seceret or a service_account or both). Depends on which type of secret we have, the authorisation methods are different.\nAuthorise Client Application (Client ID)\nFor authorising a client application (see this figure), we need to\nBuild a url with the follow query parameters:\nclient_id (the identification of the client application)\nredirect_uri (those we specified in step 3 in this section, put 1 uri here only)\nscope (the scopes the application wants to use; space-delimited if more than one is used)\naccess_type (either online of offline. A refresh_token will be obtained in later step for acquiring updated access token without another consent from the users)\n\nYou can specified more parameters for different configuration. See more from here.\nSend a request to Google OAuth page using the above url. Google will also ask for user consent in this stage.\nGoogle returns an authorisation code to the redirect_uri we specified above.\nSend another request to Google with the authorisation code obtained from the last step to exchange an access_token (and refresh_token if specified in step 1).\nThis access_token can use used to access the authorised endpoints.\n\n\n\nFigure 1: Authorise client application\n\n\n\nAmong the JSON obtained from this section, we create the following struct for the key-value pairs.\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct ClientSecret {\n    pub client_id: String,\n    pub project_id: String,\n    pub auth_uri: String,\n    pub token_uri: String,\n    pub auth_provider_x509_cert_url: String,\n    pub client_secret: String,\n    pub redirect_uris: Vec<String>\n}\n\nThen we implement a method to the above struct to build the url for step 1.\n\npub fn auth_url(&self, scope: &str) -> String {\n    let params: HashMap<_,_> = HashMap::from([\n        (\"response_type\", \"code\"),\n        (\"access_type\", \"offline\"), // set 'offline' to obtain 'refresh_token'\n        (\"prompt\", \"consent\"),\n        (\"client_id\", &self.client_id),\n        (\"redirect_uri\", &self.redirect_uris[0]),\n        (\"scope\", &scope),\n        (\"state\", &self.client_id)\n    ]);\n\n    let url = reqwest::Url::parse_with_params(\n        &self.auth_uri, params\n    ).expect(\"Failed to parse auth url.\").to_string();\n    \n    return url;\n}\n\nNow we need to print out the above url and set up a http server to listen from Google’s response with the authorisation code to finish step 3.\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct AuthCode {\n    pub code: String,\n    pub scope: String\n}\n\npub fn auth_code(&self, scope: &str, port: u32) -> Result<AuthCode, std::io::Error> {\n    let auth_url: String = self.auth_url(scope);\n    println!(\"Please visit this URL to authorize this application: {}\", auth_url);\n\n    let listener: TcpListener = \n        TcpListener::bind(format!(\"localhost:{}\", port))\n            .expect(\"Failed to bind to port\");\n    \n    let (mut stream, _) = listener.accept().unwrap();\n    let mut buf = [0;2048];\n    stream.read(&mut buf).unwrap();\n\n    let buf_str: String = String::from_utf8_lossy(&buf[..]).to_string();\n    let buf_vec: Vec<&str> = buf_str\n        .split(\" \")\n        .collect::<Vec<&str>>();\n\n    let args: String = buf_vec[1].to_string();\n    let callback_url: Url = Url::parse(\n        (format!(\"http://localhost:{}\", port) + &args).as_str()\n    ).expect(\"Failed to parse callback URL\");\n    let query: HashMap<_,_> = callback_url.query_pairs().into_owned().collect();\n    let output = AuthCode {\n        code: query.get(\"code\").unwrap().to_string(),\n        scope: query.get(\"scope\").unwrap().to_string()\n    };\n    return Ok(output);\n}\n\nFor step 4, the following function will prepare a POST request to Google to exchange the authorisation code for the access_token (and refresh_token).\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct ClientSecretTokenResponse {\n    pub access_token: String,\n    pub expires_in: i64,\n    pub refresh_token: String,\n    pub scope: String,\n    pub token_type: String\n}\n\npub async fn auth_token(&self, code: &str) -> Result<ClientSecretTokenResponse, reqwest::Error> {\n    let body: Value = serde_json::json!({\n        \"client_id\": self.client_id,\n        \"client_secret\": self.client_secret,\n        \"code\": code,\n        \"grant_type\": \"authorization_code\",\n        \"redirect_uri\": self.redirect_uris[0]\n    });\n\n    let response = reqwest::Client::new()\n        .post(self.token_uri.as_str())\n        .json(&body)\n        .send()\n        .await?;\n\n    let content: ClientSecretTokenResponse = response.json()\n        .await.expect(\"Failed to parse http response\");\n\n    return Ok(content);\n}\n\nAuthorise Client Application (Refresh Token)\nWhen we obtained the refresh_token from the above, we can further request a new access_token when the previous one is expired. To do do, we first define a struct and implement an auth function to it.\n\npub const OAUTH_TOKEN_URL: &str = \"https://oauth2.googleapis.com/token\";\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct Token {\n    pub access_token: String,\n    pub expires_in: i64\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct UserSecret {\n    pub client_id: String,\n    pub client_secret: String,\n    pub refresh_token: String\n}\n\npub async fn auth(&self) -> Result<Token, reqwest::Error> {\n    // Prepare auth body\n    let mut body: Value = serde_json::to_value(&self)\n        .expect(\"Could not convert UserSecret to Value\");\n    body[\"grant_type\"] = Value::String(\"refresh_token\".to_string());\n\n    // Auth request\n    let response: reqwest::Response = reqwest::Client::new()\n        .post(OAUTH_TOKEN_URL)\n        .json(&body)\n        .send()\n        .await?;\n\n    // Parse response to output\n    let content: Token = response.json().await?;\n\n    return Ok(content)\n}\n\nAuthorise Service Account\nFor authorising a service account (see this figure), we need to\nPrepare a JWT token. The token is separated into 3 parts:\nHeader: consist of the algorithm name and the privated_key_id (from the secret JSON).\nClaim: consist of client_email, scope, aud, iat and exp.\nKey: the private_key from the secret JSON.\n\nUse the JWT token to exchange the access_token.\n\n\n\nFigure 2: Authorise service account\n\n\n\nBelow shows the implementation.\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct ServiceSecret {\n    pub client_email: String,\n    pub private_key_id: String,\n    pub private_key: String\n}\n\npub async fn auth(&self, scope: &str) -> Result<Token, reqwest::Error> {\n    // Auth Service Account\n    // https://developers.google.com/identity/protocols/oauth2/service-account\n\n    // Prepare JWT claim\n    let claim: serde_json::Value = serde_json::json!({\n        \"iss\": self.client_email.to_string(),\n        \"scope\": scope.to_string(),\n        \"aud\": \"https://oauth2.googleapis.com/token\".to_string(),\n        \"iat\": chrono::offset::Utc::now().timestamp(),\n        \"exp\": chrono::offset::Utc::now().timestamp() + 3600\n    });\n\n    // Prepare JWT header\n    let header: Header = Header{\n        alg: Algorithm::RS256,\n        kid: Some(self.private_key_id.to_string()),\n        ..Default::default()\n    };\n\n    // Prepare JWT key\n    let key: EncodingKey = EncodingKey::from_rsa_pem(\n        &self.private_key\n            .to_string()\n            .replace(\"\\\\n\", \"\\n\").as_bytes()\n    ).expect(\"Cannot build `EncodingKey`.\");\n\n    // Generate JWT\n    let token: String = encode(\n        &header, &claim, &key\n    ).expect(\"Cannot encode `token`.\");\n\n    // Auth JWT\n    let response: Response = reqwest::Client::new()\n        .post(OAUTH_TOKEN_URL)\n        .json(&serde_json::json!({\n            \"grant_type\": \"urn:ietf:params:oauth:grant-type:jwt-bearer\",\n            \"assertion\": token\n        }))\n        .send()\n        .await?;\n    \n    // Prepare output\n    let content: Token = match response.status() {\n        StatusCode::OK => response.json().await.expect(\"Unable to parse HTTP response JSON.\"),\n        StatusCode::UNAUTHORIZED => {\n            println!(\"{}\", response.text().await.unwrap());\n            panic!(\"HTTP request failed: Unauthorized.\");\n        },\n        _ => {\n            println!(\"{}\", response.text().await.unwrap());\n            panic!(\"HTTP request failed.\");\n        }\n    };\n\n    return Ok(content);\n}\n\nExamples\nA main.rs was also written in the Github repo to provide accessibility from command prompt.\n\ncargo run \n\n# Usage: gapi-oauth <SERVICE> <JSON_PATH> [SCOPE] [PORT]\n# \n# SERVICE: `user`, `service`, or `consent`\n# JSON_PATH: The path to the JSON file containing the credentials.\n# SCOPE: Only required for `service` and `consent`\n# PORT: Only required for `consent`\n\n\ncargo run user /path/to/client_token.json\n\n# {\n#   \"access_token\": \"...\",\n#   \"expires_in\": 3599\n# }\n\n\ncargo run user /path/to/service_acc.json 'https://www.googleapis.com/auth/drive'\n\n# {\n#   \"access_token\": \"...\",\n#   \"expires_in\": 3599\n# }\n\n\ncargo run consent /path/to/client_secret.json 'https://www.googleapis.com/auth/drive' 8088\n\n# Please visit this URL to authorize this application: \n# https://accounts.google.com/o/oauth2/auth?client_id=&prompt=consent&...\n# \n# {\n#   \"access_token\": \"...\",\n#   \"refresh_token\": \"...\",\n#   \"scopes\": [\n#     \"https://www.googleapis.com/auth/drive\"\n#   ],\n#   \"expiry\": \"2023-07-30T17:51:13.123456Z\",\n#   \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n#   \"token_uri\": \"https://oauth2.googleapis.com/token\",\n#   \"client_id\": \"...\",\n#   \"client_secret\": \"...\"\n# }\n\nIt can also be used as crate. After constructing the UserSecret or ServiceSecret, simply use the corresponding auth method to return the access_token.\n\nuse crate::auth_users::UserSecret;\nuse crate::auth_service::ServiceSecret;\n\n#[tokio::test]\nasync fn test_auth_user() {\n    let client_id = std::env::var(\"USER_CLIENT_ID\")\n        .expect(\"No USER_CLIENT_ID in env var\")\n        .as_str().to_string();\n    let client_secret = std::env::var(\"USER_CLIENT_SECRET\")\n        .expect(\"No USER_CLIENT_SECRET in env var\")\n        .as_str().to_string();\n    let refresh_token = std::env::var(\"USER_REFRESH_TOKEN\")\n        .expect(\"No USER_REFRESH_TOKEN in env var\")\n\n    // Construct UserSecret\n    let client_token = UserSecret {\n        client_id: client_id,\n        client_secret: client_secret,\n        refresh_token: refresh_token,\n    };\n\n    // Auth to Token, will panic if failed.\n    let _token = client_token.auth().await\n        .expect(\"Unable to authenticate\");\n}\n\n#[tokio::test]\nasync fn test_auth_service() {\n    let client_email = std::env::var(\"SERVICE_CLIENT_EMAIL\")\n        .expect(\"No SERVICE_CLIENT_EMAIL in env var\")\n        .as_str().to_string();\n    let private_key = std::env::var(\"SERVICE_PRIVATE_KEY\")\n        .expect(\"No SERVICE_PRIVATE_KEY in env var\")\n        .as_str().to_string();\n    let private_key_id = std::env::var(\"SERVICE_PRIVATE_KEY_ID\")\n        .expect(\"No SERVICE_PRIVATE_KEY_ID in env var\")\n        .as_str().to_string();\n\n    let service_secret = ServiceSecret {\n        client_email: client_email,\n        private_key: private_key,\n        private_key_id: private_key_id,\n    };\n\n    let scopes: Vec<String> = vec![\n        \"https://www.googleapis.com/auth/drive\".to_string(),\n        \"https://www.googleapis.com/auth/youtube\".to_string()\n    ];\n\n    let scope = scopes.join(\" \");\n\n    let _token = service_secret.auth(&scope).await\n        .expect(\"Unable to authenticate\");\n}\n\n\n\n\n",
    "preview": "posts/2023-07-29-rust-gapi-oauth2/img/auth_client_id.png",
    "last_modified": "2023-08-04T09:24:09+00:00",
    "input_file": "rust-gapi-oauth2.knit.md"
  }
]
