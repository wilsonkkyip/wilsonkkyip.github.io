[
  {
    "path": "posts/2023-08-16-text-mining-tfidf/",
    "title": "Text Mining - Identifying Similar YouTube Videos",
    "description": "Introduction to text mining with word2vec and tf-idf along with an example\nto match YouTube videos based on input query.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-16",
    "categories": [
      "text-mining",
      "vord2vec",
      "cbow",
      "skip-gram",
      "tf-idf",
      "soft-cosine",
      "gensim"
    ],
    "contents": "\n\nContents\nIntroduction\nPreprocessing\nSegmentation and Tokenisation\nStop words and Punctuation\nStemming\nLemmatisation\n\nword2vec\nCBOW\nSkip-gram\n\nTF-IDF\nSoft Cosine Similarity\nMatching YouTube Videos\nDataset\nPreprocessing\nword2vec Model Training\nTF-IDF Calculation\nSimilarity Matrix\nMatching Videos\n\n\nIntroduction\nIn previous article, we have collected\nmore than 400,000 rows of video data from YouTube API. We have also introduced\nhow soft cosine similarity score works in matching between vectors\n(see here). In this article, we will first\nintroduction some text mining technique including word2vec and tf-idf.\nAt last, we will show an example on how to generate a matching model using the\nsoft cosine similarity score to find the best matching videos from some string\nqueries. Below shows an example on using the model.\n\nq = \"ukraine war, conflict, clash, confrontation, russia\"\nresult = match_videos(q)\nresult\n\n#                       channel_id     video_id                                  title     score\n# 225877  UCfdNM3NAhaBOXCafH7krzrA  6Awx8MvT7qk     What Will Ukraine Do After The ...  0.613843\n# 253822  UCeY0bbntWzzVIaj2z3QigXg  qtA-zamctYA              One year of #Russia's ...  0.613843\n# 266182  UCeY0bbntWzzVIaj2z3QigXg  kV90vvVmAfY  What Is The Background To Conflic ...  0.603664\n# 225744  UCfdNM3NAhaBOXCafH7krzrA  WXzdTxYrOSo  What Russia Will Do After the War ...  0.600225\n# 89191   UCef1-8eOpJgud7szVPlZQAQ  HAb1pFRsh-4  Russia Vs Ukraine War Update LIVE ...  0.585112\n# 225448  UCfdNM3NAhaBOXCafH7krzrA  TXTS2R04Uu8            How Putin Almost Won th ...  0.576693\n# 105578  UCef1-8eOpJgud7szVPlZQAQ  Hb-MjE8DtQ4  Russia Ukraine War | Russia Attac ...  0.575848\n# 97898   UCef1-8eOpJgud7szVPlZQAQ  1E7vTp7tezg  Russia Ukraine War | Russian Parl ...  0.569067\n# 95272   UCef1-8eOpJgud7szVPlZQAQ  N5fJwSp7N6Y  Russia Ukraine War | Wagner Will  ...  0.557974\n# 225341  UCfdNM3NAhaBOXCafH7krzrA  GVLGPXuWoGs  Russia's Invasion of Ukraine is a ...  0.556573\n\nPreprocessing\nBefore analysing textual data, a series of preprocessing stages are required to\nconvert the text into a more structured format. These stages include sentence\nsegmentation, word tokenisation, lowercasing, stop word removal, stemming or\nlemmatisation, etc.\nSegmentation and Tokenisation\nSegmentation is a process to break up the text into corresponding sentences,\nwhile tokenisation is a process to further break up the sentences into words.\nStop words and Punctuation\nStop words are words that do no have much meaning in a sentence, but are\ncommonly used. For example, in English, “the”, “a”, “and”, etc. We will remove\nthese words along with punctuations.\nStemming\nStemming is a technique used to truncate a word into its word stem. Its\nalgorithm is based on the commonly used prefixes and suffixes to cut off the\nbeginning or the end of the word. Below shows an example for stemming.\n\nfrom gensim.parsing.porter import PorterStemmer\np = PorterStemmer()\n\nexample_words = [\"writter\", \"writting\", \"written\", \"wrote\", \"writtings\"]\n[p.stem(x) for x in example_words]\n\n# ['writter', 'writ', 'written', 'wrote', 'writ']\n\nLemmatisation\nLemmatisation is another method to root a word. It does not simply truncate a\nword like stemming. It involves standardising all inflections of a given word\ninto it’s base dictionary form. Below shows the same example as above but using\nlemmatisation.\n\nfrom nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()\n\nexample_words = [\"writter\", \"writting\", \"written\", \"wrote\", \"writtings\"]\n[wnl.lemmatize(x, pos=\"v\") for x in example_words]\n\n# ['writter', 'writting', 'write', 'write', 'writtings']\n\nword2vec\nSince computers use (binary) numbers for processing, it will be very desiring\nif we can convert words into numbers. Word embedding is an approach with which\nwe can represent a word, and hence a document, in a real valued vector.\nWord2Vec is a method to construct these embedding vectors. There are two similar\nmodels: Continuous Bag Of Words (CBOW) and Skip-gram. Both models origin from\nthe same idea. We first assign a number for the window size (\\(\\omega\\)) and our\ngoal is to learn the meaning of a word from the previous \\(\\omega\\) words and the\ncoming \\(\\omega\\) words.\nSuppose we want to learn the embeddings of the following words:\n\na quantitative methodological approach to understanding meaning in observed language\n\nWe first assign a number, say 2, as the window size. Then for learning the word\nmethodological, we will look at the 2 words before and after it. In this case,\nthey are [a, quantitative, approach, to]. We call the word methodological\ntarget word and the 4 surrounding words context words.\nThe CBOW model aims to train a neural network with 1 hidden layer to take the 4\ncontext words as input and predict the target word. Along the process, the\nhidden layer is expected to contains the embedded information of the target\nword. The goal of this model is to extract the hidden layer as the embedding\nvectors of words.\nThe skip-gram model uses similar approach to find the embedding vectors. Yet,\ninstead of predicting the target word by the context words, the skip-gram model\nuse the target word to maximise the optimisation to find the context words.\n\n\n\nFigure 1: CBOW and skip-gram models\n\n\n\nCBOW\nLet \\(D\\) (dictionary) be the set of all available vocabularies with cardinality\n\\(|D| = d\\), \\(\\mathbf{p}_t \\in D\\) be a word in the dictionary encoded using\n1-of-\\(D\\) coding. Also let \\(s\\) be the number of dimensions for the word\nembeddings.\nSuppose we want to find out the word embeddings for \\(\\mathbf{p}_t\\). The CBOW\nmodel takes words surrounding \\(\\mathbf{p}_t\\) with a window of size \\(\\omega\\) and\ntrain a neural network with one hidden layer and an output layer. The output\nlayer is the vector of \\(\\mathbf{p}_t\\), meaning the neural network aims to\npredict the word \\(\\mathbf{p}_t\\) from its surrounding words\n\\(\\mathbf{p}_{t - \\omega}, \\dots, \\mathbf{p}_{t - 1}, \\mathbf{p}_{t + 1}, \\dots, \\mathbf{p}_{t + \\omega}\\).\nThen the hidden layers must contain some meaningful information about the word\n\\(\\mathbf{p}_t\\) in the form of vector. We take this hidden layer as the word\nembedding vector \\(\\mathbf{e}_t\\) of \\(\\mathbf{p}_t\\). Hence, the hidden layer is an\n\\(s\\)-dimensional vector.\nNotice that the weight matrix \\(W\\) between the input layer is an \\(s \\times d\\)\nmatrix and\n\\[\\mathbf{e}_t = \\frac{1}{2\\omega} \\sum_{i = 1}^\\omega W(\\mathbf{p}_{t - i} + \\mathbf{p}_{t _ i}),\\]\nwhich is the average value of the projected vectors from the surrounding words.\nFinally, when the model is trained. the word embedding vector \\(\\mathbf{e}_j\\) for\n\\(\\mathbf{p}_j\\) is given by\n\\[\\mathbf{e}_j = W\\mathbf{p}_j,\\]\nor the \\(j\\)-th column of \\(W\\) since \\(\\mathbf{p}_j\\)’s entries are all \\(0\\) except\n\\(1\\) at the \\(j\\)-th entry.\nSkip-gram\nThe skip-gram model is similar to the CBOW model, but instead of predicting the\ntarget word \\(\\mathbf{p}_t\\), it takes the target word to predict the context\nwords \\(\\mathbf{p}_{t - \\omega}, \\dots, \\mathbf{p}_{t - 1}, \\mathbf{p}_{t + 1}, \\dots \\mathbf{p}_{t + \\omega}\\).\nThe first weight matrix \\(W\\) is an \\(s \\times d\\) matrix transforming the target\nword \\(\\mathbf{p}_t\\) to the embedding vector \\(\\mathbf{e}_j\\). The second weight\nmatrix \\(W'\\) is a \\(d \\times s\\) matrix transforming the embedding vector to one\nof the context word \\(\\mathbf{p}_{t + i}\\), where \\(i \\in {-\\omega, \\dots, -1, 1, \\dots, \\omega}\\).\nAll context words within the window share the same weight matrix \\(W'\\), meaning\nthat the model is required to through \\(2 \\omega\\) times for each target word.\nWhen the training is over, the word embedding vector for each words are given by\n\\[\\mathbf{e}_j = W\\mathbf{p}_j,\\]\nor the \\(j\\)-th column of the weight matrix \\(W\\).\nTF-IDF\nTerm Frequency–Inverse Document Frequency (TF-IDF) is used to measure the\nimportance of a token (or word) to the document it belongs relative to the\nimportance of the same token to the whole collection of corpus.\nIn general, the value is the product of two terms:\nTerm frequency \\(\\text{tf}(t, d)\\): a measure of importance of the term to the document.\nInverse document frequency \\(\\text{idf}(t, D)\\): a measure of importance of the term to the corpus.\nThat is,\n\\[\\text{tfidf}(t, d, D) = \\text{tf}(t, d) \\cdot \\text{idf}(t, D),\\]\nwhere \\(t\\) is a particular token (or word or term), \\(d\\) is a document and\n\\(D\\) is the collection of corpus.\nThere are different ways to calculate \\(\\text{tf}(t, d)\\) and \\(\\text{idf}(t, D)\\).\nBelow shows some variants of \\(\\text{tf}(t, d)\\) and \\(\\text{idf}(t, D)\\).\n\n\n\n\n\\(\\text{tf}\\) variants\n\n\n\n\n\\(\\text{idf}\\) variants\n\n\n\n\n\n\nraw count\n\n\n\\(f_{t, d}\\)\n\n\n\n\ninverse document frequency\n\n\n\\(\\log_b \\dfrac{N}{n_t}\\)\n\n\n\n\n\n\nterm frequency\n\n\n\\(f_{t, d}\\left/ \\sum\\limits_{t' \\in d} f_{t', d} \\right.\\)\n\n\n\n\ninverse document frequency smooth\n\n\n\\(\\log_b \\left( \\dfrac{N}{1 + n_t} \\right) + 1\\)\n\n\n\n\n\n\nlog normalisation\n\n\n\\(\\log_b(1 + f_{t, d})\\)\n\n\n\n\nprobabilistic inverse document frequency\n\n\n\\(\\log_b \\dfrac{N - n_t}{n_t}\\)\n\n\n\n\nwhere \\(f_{t, d}\\) is the frequency of token \\(t\\) in document \\(d\\), \\(b\\) is the base\nnumber for logarithm, \\(N = |D|\\) is the total number of documents, and\n\\(n_t = |\\{d \\in D | t \\in d\\}|\\) is the number of documents contain token \\(t\\).\nSoft Cosine Similarity\nIn previous article, we have discussed the\nsoft cosine similarity. It modified the generic cosine similarity by putting a\ncorrelation matrix between the dot product to define a new inner product and\nhence a new metric for measuring the difference or similarity between two\nvectors. Such calculation no longer assume each dimension is completely\nindependent to each other but having some correlation.\nFor any two \\(n\\)-dimensional vectors \\(\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^n\\),\nthe soft cosine similarity between these two vectors with respect to the\ncorrelation matrix \\(S\\) is given by\n\\[\n\\text{soft-cosine} =  \\frac{\\mathbf{a}^T S\\, \\mathbf{b}}{\\sqrt{\\mathbf{a}^T S \\, \\mathbf{a}} \\cdot \\sqrt{\\mathbf{b}^T S \\, \\mathbf{b}}}.\n\\]\nMatching YouTube Videos\nIn this section, we will first preprocess the text from the title and\ndescription of the videos. Then we will train a word2vec word embeddings from\nthis corpus. With these word embeddings, we can calculate the similarity\nmatrix between each pair of words. After that, we will calculate the importance\nof each word in a document using the tfidf scores. With any two word importance\nvectors along with the words similarity matrix, we can calculate the soft\ncosine similarity between these two vectors.\nDataset\nIn previous article, we have shown how to\ndownload data of YouTube videos. In this article, we will make use of the data\ncollected from this method to match similar videos based on their titles.\nWe have downloaded more than 400,000 videos’ data from the API. We first have a\nlook on the dataset.\n\nimport pandas as pd \n\ndf = pd.read_csv(\"./data.csv\")\ndf\n\n#                       channel_id     video_id                                              title\n# 0       UCSZbXT5TLLW_i-5W8FZpFsg  DFZnYR-5vnk  HIGHLIGHTS: Colorado Rapids vs. Seattle Sounde...\n# 1       UC9k-yiEpRHMNVOnOi_aQK8w  tilUNQJ5_WE  True Story Behind Festival Featured in 'Midsom...\n# 2       UCSHLoG-bXj1aVA2T5y8t84A  _zZdcckm1vk             Ankahee - Official Full Song - Lootera\n# 3       UCvQECJukTDE2i6aCoMnS-Vg  jn70KsBit3c    Sylvester James Gates, Jr.: Scientific Literacy\n# 4       UC8-Th83bH_thdKZDJCrn88g  7f_0UpRijm0  Queen Latifah's Dad Is One Tough Dude (Late Ni...\n# ...                          ...          ...                                                ...\n# 457036  UCQD3awTLw9i8Xzh85FKsuJA  mCWQ90YdtLo             Random 'Dead by Daylight' Bullshittery\n# 457037  UCeY0bbntWzzVIaj2z3QigXg  zCNZgK7OXPY  Wisconsin Senate Debate: Johnson, Barnes Are A...\n# 457038  UCvQECJukTDE2i6aCoMnS-Vg  J-jhR84F_9o      Mohammad's Message Dalia Mogahed  | Big Think\n# 457039  UC9k-yiEpRHMNVOnOi_aQK8w  mj1Qs4z_AzA         Farm Boy Has Trouble Closing Gates #shorts\n# 457040  UCwiTOchWeKjrJZw7S1H__1g  nojaKlA7MoU  Johnnie's Iconic Italian Beef Is A Delicious M...\n\nPreprocessing\n\nimport re\nfrom nltk.corpus import stopwords\nfrom urllib.request import urlopen\nfrom gensim.parsing.porter import PorterStemmer\n\nstop_words = stopwords.words('english')\nstopwords_url = \"https://raw.githubusercontent.com/Alir3z4/stop-words/master/english.txt\"\nfull_stopwords = urlopen(stopwords_url).read().decode(\"utf-8\").split()\n\nstop_words = stop_words + full_stopwords\nstop_words = list(set([x.replace(\"'\", \"\") for x in stop_words] + stop_words))\n\np = PorterStemmer()\ndef remove_stopwords(x):\n    x = p.stem_sentence(x)\n    return \" \".join([w for w in x.lower().split() if w not in stop_words])\n\ndf[\"title_processed\"] = df[\"title\"].apply(remove_stopwords)\ndf[\"title_processed\"] = df[\"title_processed\"]\\\n    .str.replace(\"[^\\\\w\\\\s]\", \"\", regex=True)\\\n        .str.replace('\\\\s+', ' ', regex=True)\\\n        .apply(lambda x: x.split())\n\ndf[\"description_processed\"] = df[\"description\"].apply(remove_stopwords)\ndf[\"description_processed\"] = df[\"description_processed\"]\\\n    .str.replace(\"[^\\\\w\\\\s]\", \"\", regex=True)\\\n        .str.replace('\\\\s+', ' ', regex=True)\\\n        .apply(lambda x: x.split())\n\ndf[[\"title_processed\", \"description_processed\"]]\n\n#                                           title_processed                              description_processed\n# 0       [highlights, colorado, rapid, vs, seattl, soun...  [colorado, rapid, continu, hot, streak, thirds...\n# 1          [true, stori, festiv, featur, midsommar, film]  [film, scari, twist, swedish, tradition, midso...\n# 2                         [ankahe, offici, song, lootera]  [here, romant, track, ankahee, upcom, movi, lo...\n# 3          [sylvest, jame, gates, jr, scientif, literaci]  [watch, video, think, httpsbigthinknewvideo, e...\n# 4       [queen, latifah, dad, tough, dude, late, night...  [queen, latifah, jimmi, live, ha, hurt, winter...\n# ...                                                   ...                                                ...\n# 457036             [random, dead, daylight, bullshitteri]  [random, moment, bullshit, fuck, clan, stream,...\n# 457037  [wisconsin, senat, debate, johnson, barn, oppo...  [conclus, wisconsin, us, senat, debate, ron, j...\n# 457038                   [mohammad, messag, dalia, mogah]  [mohammad, messag, video, daily, httpsbigthink...\n# 457039        [farm, boi, ha, troubl, close, gate, short]  [5yearold, boi, kentucki, troubl, close, gate,...\n# 457040  [johnnie, icon, italian, beef, delici, mess, b...  [ubiquit, italian, beef, sandwich, chicago, it...\n\nword2vec Model Training\n\nfrom gensim.models import Word2Vec\n\ndocuments = list(df[\"title_processed\"])\nmodel_documents = documents + list(df[\"description_processed\"])\nmodel_documents = pd.Series([\" \".join(x) for x in model_documents])\nmodel_documents = model_documents.drop_duplicates().apply(lambda x: x.split())\nmodel_documents = list(model_documents)\n\nmodel = Word2Vec(\n    sentences=model_documents,\n    vector_size=200,\n    window=5,\n    min_count=3,\n    workers=4,\n    epochs=3\n)\n\n# Or we can simply download some pretrained models\n# import gensim.downloader as api\n# model = api.load('word2vec-google-news-300')\n\nmodel.save(\"./youtube.model\")\nmodel = Word2Vec.load(\"youtube.model\")\n\nTF-IDF Calculation\n\nfrom gensim.models import TfidfModel\nfrom gensim.corpora import Dictionary\n\ndictionary = Dictionary(documents)\ndf[\"bows\"] = [dictionary.doc2bow(x) for x in documents]\ntfidf = TfidfModel(list(df[\"bows\"]))\n\nSimilarity Matrix\nCorrelation matrix from word2vec model ordered by tfidf index\n\nfrom gensim.similarities import SparseTermSimilarityMatrix, WordEmbeddingSimilarityIndex\n\ntermsim_index = WordEmbeddingSimilarityIndex(model.wv)\ntermsim_matrix = SparseTermSimilarityMatrix(termsim_index, dictionary, tfidf)\n\nMatching Videos\n\ndef process_q(q: str):\n    processed_q = remove_stopwords(q)\n    processed_q = re.sub(\"[^\\\\w\\\\s]\", \"\", processed_q)\n    processed_q = re.sub(\"\\\\s+\", \" \", processed_q).split()\n    return processed_q\n\ndef match_videos(q):\n    q_list = process_q(q)\n    q_bow = dictionary.doc2bow(q_list)\n\n    result = pd.Series([termsim_matrix.inner_product(\n        tfidf[q_bow],\n        tfidf[x], \n        normalized=(True, True)\n    ) for x in df[\"bows\"]])\n\n    result_top50 = result.sort_values(ascending=False)[:50].copy()\n    idx = result_top50.index\n\n    output = df.iloc[df.index[idx],:].copy()\n    output[\"score\"] = result_top50\n\n    return output[[\"channel_id\", \"video_id\", \"title\", \"score\"]]\n\nq = \"ukraine war, conflict, clash, confrontation, russia\"\nresult = match_videos(q)\nresult\n\n#                       channel_id     video_id                                              title     score\n# 225877  UCfdNM3NAhaBOXCafH7krzrA  6Awx8MvT7qk     What Will Ukraine Do After The War With Russia  0.613843\n# 253822  UCeY0bbntWzzVIaj2z3QigXg  qtA-zamctYA              One year of #Russia's war in #Ukraine  0.613843\n# 266182  UCeY0bbntWzzVIaj2z3QigXg  kV90vvVmAfY  What Is The Background To Conflict Between Rus...  0.603664\n# 225744  UCfdNM3NAhaBOXCafH7krzrA  WXzdTxYrOSo  What Russia Will Do After the War in Ukraine E...  0.600225\n# 89191   UCef1-8eOpJgud7szVPlZQAQ  HAb1pFRsh-4  Russia Vs Ukraine War Update LIVE | Is Russia ...  0.585112\n# 225448  UCfdNM3NAhaBOXCafH7krzrA  TXTS2R04Uu8            How Putin Almost Won the War in Ukraine  0.576693\n# 105578  UCef1-8eOpJgud7szVPlZQAQ  Hb-MjE8DtQ4  Russia Ukraine War | Russia Attacks Ukraine Ye...  0.575848\n# 97898   UCef1-8eOpJgud7szVPlZQAQ  1E7vTp7tezg  Russia Ukraine War | Russian Parliament Backs ...  0.569067\n# 95272   UCef1-8eOpJgud7szVPlZQAQ  N5fJwSp7N6Y  Russia Ukraine War | Wagner Will Not Fight In ...  0.557974\n# 225341  UCfdNM3NAhaBOXCafH7krzrA  GVLGPXuWoGs  Russia's Invasion of Ukraine is a Disaster for...  0.556573\n\n\n\n\n",
    "preview": "posts/2023-08-16-text-mining-tfidf/img/preview.png",
    "last_modified": "2023-08-20T12:33:00+00:00",
    "input_file": "text-mining-tfidf.knit.md"
  },
  {
    "path": "posts/2023-08-15-youtube-data-api-v3/",
    "title": "YouTube Data API v3",
    "description": "An introduction to YouTube Data API and Etags.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-15",
    "categories": [
      "google-api",
      "youtube-api",
      "etag",
      "if-none-match"
    ],
    "contents": "\n\nContents\nIntroduction\nYouTube Data API\nBefore Requests\nList Channel\nList PlaylistItems\nList Videos\nEtag\n\n\nIntroduction\nThis article will showcase what information we can obtain from YouTube Data API.\nTo use the API, one will need to create a Google Cloud Project and create a\ncredentials. It can be an API key, a service account, an OAuth client id. Please\nrefer to the previous article for further\ninformation about the authentication and authorisation for Google APIs.\nIn another previous article\nwe have sample a few hundreds of channel_id from the main page of YouTube.\nThis article will start from these channel_id and retrieve their videos’\ninformation.\nThis article will also talk about the etag and the if-none-match http\nheader. One can utilise this header to save time and bandwidth by caching\nprevious response.\n\n\n\nYouTube Data API\nThere is a full documentation\nprovided by YouTube about this API. In short, one can utilise this API for\nthings that can be normally executed on the YouTube website. For example one can\nview statistics about videos and channels, view comments of videos, explore\nitems in playlists, perform search query, etc. This API also allows us to insert\ncomments, captions, etc, as long as an authorised token is provided.\nAs described previously, this article will start from some channel_id’s to\nvideos published by them. It will cover 3 different endpoints for this purpose:\nList Channels\nList PlaylistItems\nList Videos\nWe will obtain, from the List Channels endpoint, a playlist_id that contains\nall the videos published by the channel. Then we will find all the items\n(videos) inside this playlist from the List PlaylistItems endpoint to obtain\nthe video_id of the videos. Finally we can get all the information about the\nvideos from the List Videos endpoint.\nPlease be aware that there is a quota limit for each of the Google Cloud Project\non each day. Each project will have 10,000 units of quota one each day and\ndifferent methods require different number of units.\nBefore Requests\nWhen you have your access token ready (see previous article),\nthe following script gives an example request to the API.\n\nimport os \nimport requests\n\n# Headers for GCP OAuth2 Token\nheaders = {\"Authorization\": f'Bearer {os.getenv(\"GCP_OAUTH_TOKEN\")}'}\n\n# Headers for GCP Service Account\nheaders = {\"Authorization\": f'Bearer {os.getenv(\"GCP_SERVICE_ACC_TOKEN\")}'}\n\n# Headers for GCP API Key\nheaders = {\"x-goog-api-key\": os.getenv(\"GCP_API_KEY\")}\n\n# Request\nresponse = requests.request(\n    method=method,     # GET or POST or other HTTP methods\n    url=url,           # Endpoint of service\n    params=params,     # Query or parameters (if needed)\n    json=body,         # Request body        (if needed)\n    headers=headers    # The headers specified above\n)\n\nList Channel\nIn this section, we will talk about the List Channel endpoint. From the\nprevious article,\nwe obtained hundreds of channel_id’s.\n\nchannel_ids = [\n    \"UCx1m6AboILQKMNXbsLjjI4Q\",\n    \"UC-iICi3q1AF_9WI--DVacTQ\"\n]\n\nWe will pass these channel ids to the endpoint\n\nGET https://www.googleapis.com/youtube/v3/channels\n\nWe are also required which part(s) of the channel resource we want to request.\n\nparts = [\n    \"brandingSettings\", \"contentDetails\", \"contentOwnerDetails\", \"id\", \n    \"localizations\", \"snippet\", \"statistics\", \"status\", \"topicDetails\"\n]\n\nNow we are ready to make the request. Notice that you can only request for 50\nchannels’ information in 1 request.\n\nparams = {\n    \"part\": \",\".join(parts),\n    \"id\": \",\".join(channel_ids) # Can put 50 channel ids in 1 request\n}\n\nresponse = requests.get(\n    url=\"https://www.googleapis.com/youtube/v3/channels\",\n    params=params,\n    headers=headers    # See \"Before Requests\" session\n)\n\nresponse.json()\n\n# {\n#     \"kind\": \"youtube#channelListResponse\",\n#     \"etag\": \"UVHnd3n70mtZnwVUmK2mRePbVCo\",\n#     \"pageInfo\": {\n#         \"totalResults\": 2,\n#         \"resultsPerPage\": 5\n#     },\n#     \"items\": [\n#         {\n#             \"kind\": \"youtube#channel\",\n#             \"etag\": \"KGqojbKzM5T5vGED4lXuDv1kPr4\",\n#             \"id\": \"UCU0zC0L4o0qOos83NLhK5ug\",\n#             \"snippet\": {\n#                 \"title\": \"Ginger Cat\",\n#                 \"description\": \"Welcome to Ginger Cat, where for everyone who loves animal and care about their wellbeing. Our goal is to make caring about animals a viral cause. I hope you guys have a good time in here!\\n\\n\",\n#                 \"customUrl\": \"@gingercatofficial\",\n#                 \"publishedAt\": \"2016-11-27T04:02:23Z\",\n#                 \"thumbnails\": {...},\n#                 \"localized\": {...},\n#                 \"country\": \"US\"\n#             },\n#             \"contentDetails\": {\n#                 \"relatedPlaylists\": {\n#                     \"likes\": \"\",\n#                     \"uploads\": \"UUU0zC0L4o0qOos83NLhK5ug\"\n#                 }\n#             },\n#             \"statistics\": {\n#                 \"viewCount\": \"155315923\",\n#                 \"subscriberCount\": \"347000\",\n#                 \"hiddenSubscriberCount\": false,\n#                 \"videoCount\": \"516\"\n#             },\n#             \"topicDetails\": {...},\n#             \"status\": {...},\n#             \"brandingSettings\": {...},\n#             \"contentOwnerDetails\": {}\n#         },\n#         {\n#             \"kind\": \"youtube#channel\",\n#             \"etag\": \"4_wvn4zWGtuxuWYN4ddla40T5S8\",\n#             \"id\": \"UCx1m6AboILQKMNXbsLjjI4Q\",\n#             \"snippet\": {\n#                 \"title\": \"Proper DIY\",\n#                 \"description\": \"After many years of 'Doing It Myself' I now invite you the viewer into my world of DIY projects around the house and garden. I hope to inspire everyone to give It a go and to use your new knowledge and skill to save money while having fun and a sense of achievement.\\n\\nThe channel was launched on 1st Jan 2021 and there will be weekly videos posted throughout 2021 on everything from changing a plug to building and electrical works. Please subscribe to ensure you follow me through the Youtube journey!\\n\",\n#                 \"customUrl\": \"@properdiy\",\n#                 \"publishedAt\": \"2018-12-03T07:05:09Z\",\n#                 \"thumbnails\": {...},\n#                 \"localized\": {...},\n#                 \"country\": \"GB\"\n#             },\n#             \"contentDetails\": {\n#                 \"relatedPlaylists\": {\n#                     \"likes\": \"\",\n#                     \"uploads\": \"UUx1m6AboILQKMNXbsLjjI4Q\"\n#                 }\n#             },\n#             \"statistics\": {\n#                 \"viewCount\": \"22997422\",\n#                 \"subscriberCount\": \"246000\",\n#                 \"hiddenSubscriberCount\": false,\n#                 \"videoCount\": \"155\"\n#             },\n#             \"topicDetails\": {...},\n#             \"status\": {...},\n#             \"brandingSettings\": {...},\n#             \"contentOwnerDetails\": {}\n#         }\n#     ]\n# }\n\nList PlaylistItems\nFrom the above step, we can obtain the playlists containing all the videos\npublished by the channels.\n\nplaylist_ids = [\n    x[\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"] for x in response.json()[\"items\"]\n]\n\nplaylist_ids\n\n# ['UUU0zC0L4o0qOos83NLhK5ug', 'UUx1m6AboILQKMNXbsLjjI4Q]\n\nNow we can list all items inside a playlist. Notice that we can only list the\nitems in a single playlist in each request. We also need to set the number of\nmaximum results to be returned by the service. The maximum maxResult is 50.\nYet, there are always more than 50 videos in a channel. We need to loop through\nthe pages via the nextPageToken key.\nThe function below will loop through all the videos in a playlist.\n\ndef fetch_all_videos_in_playlist(playlist_id):\n    parts = [\n        \"contentDetails\", \"id\", \"snippet\", \"status\"\n    ]\n\n    page_token = \"\"\n    items = []\n\n    while page_token is not None:\n        params = {\n            \"part\": \",\".join(parts),\n            \"playlistId\": playlist_id,\n            \"maxResults\": 50,\n            \"pageToken\": page_token or None\n        }\n\n        res = requests.get(\n            url=\"https://www.googleapis.com/youtube/v3/playlistItems\",\n            params=params,\n            headers=headers\n        )\n\n        items += res.json()[\"items\"]\n        page_token = res.json().get(\"nextPageToken\")\n    \n    return items\n\nWe now try out the function.\n\n# Fetch all the videos in the first channel\nplaylist_items = fetch_all_videos_in_playlist(playlist_ids[0])\n\nplaylist_items[:2]\n\n# [\n#     {\n#         \"kind\": \"youtube#playlistItem\",\n#         \"etag\": \"PbvtsMi9EMMPh7IvYd-Rln4rp-A\",\n#         \"id\": \"VVVVMHpDMEw0bzBxT29zODNOTGhLNXVnLkUwVXBKZkJ6X2Ew\",\n#         \"snippet\": {\n#             \"publishedAt\": \"2023-08-14T19:00:05Z\",\n#             \"channelId\": \"UCU0zC0L4o0qOos83NLhK5ug\",\n#             \"title\": \"Unforgettable Comedy Duo: Funniest Dog and Human Video Ever! \\ud83d\\ude31\",\n#             \"description\": \"Unforgettable Comedy Duo: Funniest Dog and Human Video Ever! \\ud83d\\ude31\\nGet ready to experience the unbreakable bond between humans and their furry best friends, as their mischievous antics and genuine love bring out the purest emotions. Don't miss these videos that will brighten your day!\\n----------------------------------------------------------------------------\\nBest of the WEEK! Funniest Cats And Dogs Videos \\ud83d\\ude02\\nhttps://www.youtube.com/watch?v=knkv4Habj9s&list=PLDae3oPkWscWqZZtu8gqj20Ug9ijrr7Ae&index=1&t=2s&pp=iAQB \\n----------------------------------------------------------------------------\\n\\n#dogs #pets #animal #dog #dogandhuman #animal2023\",\n#             \"thumbnails\": {...},\n#             \"channelTitle\": \"Ginger Cat\",\n#             \"playlistId\": \"UUU0zC0L4o0qOos83NLhK5ug\",\n#             \"position\": 0,\n#             \"resourceId\": {\n#                 \"kind\": \"youtube#video\",\n#                 \"videoId\": \"E0UpJfBz_a0\"\n#             },\n#             \"videoOwnerChannelTitle\": \"Ginger Cat\",\n#             \"videoOwnerChannelId\": \"UCU0zC0L4o0qOos83NLhK5ug\"\n#         },\n#         \"contentDetails\": {\n#             \"videoId\": \"E0UpJfBz_a0\",\n#             \"videoPublishedAt\": \"2023-08-14T19:00:05Z\"\n#         },\n#         \"status\": {\n#             \"privacyStatus\": \"public\"\n#         }\n#     },\n#     {\n#         \"kind\": \"youtube#playlistItem\",\n#         \"etag\": \"EwGgaznGyPJpoc6T-ERkuKE1nt8\",\n#         \"id\": \"VVVVMHpDMEw0bzBxT29zODNOTGhLNXVnLmtua3Y0SGFiajlz\",\n#         \"snippet\": {\n#             \"publishedAt\": \"2023-08-12T19:00:16Z\",\n#             \"channelId\": \"UCU0zC0L4o0qOos83NLhK5ug\",\n#             \"title\": \"Best of the WEEK! Funniest Cats And Dogs Videos \\ud83d\\ude02\",\n#             \"description\": \"Best of the WEEK!  Funniest Cats And Dogs Videos \\ud83d\\ude02\\nBeing an animal lover, these were cute!\\n----------------------------------------------------------------------------\\nDogs can talk, dogs can understand. Who knew?\\ud83d\\ude31 \\nhttps://www.youtube.com/watch?v=D6aecAswvyE&list=PLDae3oPkWscWqZZtu8gqj20Ug9ijrr7Ae&index=1&pp=iAQB  \\n----------------------------------------------------------------------------\\n\\n#dogs #pets #animal #dog #animal #animal2023\",\n#             \"thumbnails\": {...},\n#             \"channelTitle\": \"Ginger Cat\",\n#             \"playlistId\": \"UUU0zC0L4o0qOos83NLhK5ug\",\n#             \"position\": 1,\n#             \"resourceId\": {\n#                 \"kind\": \"youtube#video\",\n#                 \"videoId\": \"knkv4Habj9s\"\n#             },\n#             \"videoOwnerChannelTitle\": \"Ginger Cat\",\n#             \"videoOwnerChannelId\": \"UCU0zC0L4o0qOos83NLhK5ug\"\n#         },\n#         \"contentDetails\": {\n#             \"videoId\": \"knkv4Habj9s\",\n#             \"videoPublishedAt\": \"2023-08-12T19:00:16Z\"\n#         },\n#         \"status\": {\n#             \"privacyStatus\": \"public\"\n#         }\n#     }\n# ]\n\nNotice that the List PlaylistItems method can only return 20,000 items in\none single playlist if you are using API key.\nList Videos\nNow we have a list of videos published by the channel UUU0zC0L4o0qOos83NLhK5ug.\n\nvideo_ids = [x[\"snippet\"][\"resourceId\"][\"videoId\"] for x in playlist_items]\n\nvideo_ids[:10]\n\n# [\n#     'E0UpJfBz_a0', 'knkv4Habj9s', 'D6aecAswvyE', 'uGdDiiro8yg', 'pwNqOkkdry4', \n#     'ylp36nsXpfE', 'VtOY_2-YBSM', '27VRjJfW3A8', '3BGQGzpeOkU', '8qaV3s8kzcU'\n# ]\n\nWe can use the List Videos method to extract information about the videos\n(likeCount, viewCount, commentCount, title, description, etc.). Like\nthe List Channels method, you can only request for 50 videos in 1 request.\nThe function below handles this issue.\n\nfrom math import ceil\n\ndef fetch_videos(video_ids):\n    n = 50\n    batches = ceil(len(video_ids) / n)\n    ids = [\n        video_ids[(n * i):(n * (i + 1))] for i in range(batches)\n    ]\n\n    parts = [\n        \"contentDetails\", \"id\", \"liveStreamingDetails\",\n        \"localizations\", \"player\", \"recordingDetails\",\n        \"snippet\", \"statistics\", \"status\", \"topicDetails\"\n    ]\n\n    items = []\n    for id in ids:\n        params = {\n            \"part\": \",\".join(parts),\n            \"id\": \",\".join(id)\n        }\n        response = requests.get(\n            url=\"https://www.googleapis.com/youtube/v3/videos\",\n            params=params,\n            headers=headers\n        )\n        items += response.json()[\"items\"]\n    \n    return items\n\nNow we are ready for the API request.\n\nvideos = fetch_videos(video_ids)\n\nvideos[:2]\n\n# [\n#     {\n#         \"kind\": \"youtube#video\",\n#         \"etag\": \"h7TfUpEP8ts-XNzYvActIph1mg4\",\n#         \"id\": \"E0UpJfBz_a0\",\n#         \"snippet\": {\n#             \"publishedAt\": \"2023-08-14T19:00:05Z\",\n#             \"channelId\": \"UCU0zC0L4o0qOos83NLhK5ug\",\n#             \"title\": \"Unforgettable Comedy Duo: Funniest Dog and Human Video Ever! \\ud83d\\ude31\",\n#             \"description\": \"Unforgettable Comedy Duo: Funniest Dog and Human Video Ever! \\ud83d\\ude31\\nGet ready to experience the unbreakable bond between humans and their furry best friends, as their mischievous antics and genuine love bring out the purest emotions. Don't miss these videos that will brighten your day!\\n----------------------------------------------------------------------------\\nBest of the WEEK! Funniest Cats And Dogs Videos \\ud83d\\ude02\\nhttps://www.youtube.com/watch?v=knkv4Habj9s&list=PLDae3oPkWscWqZZtu8gqj20Ug9ijrr7Ae&index=1&t=2s&pp=iAQB \\n----------------------------------------------------------------------------\\n\\n#dogs #pets #animal #dog #dogandhuman #animal2023\",\n#             \"thumbnails\": {...},\n#             \"channelTitle\": \"Ginger Cat\",\n#             \"tags\": [...],\n#             \"categoryId\": \"24\",\n#             \"liveBroadcastContent\": \"none\",\n#             \"defaultLanguage\": \"en\",\n#             \"localized\": {...},\n#             \"defaultAudioLanguage\": \"en\"\n#         },\n#         \"contentDetails\": {\n#             \"duration\": \"PT8M56S\",\n#             \"dimension\": \"2d\",\n#             \"definition\": \"hd\",\n#             \"caption\": \"false\",\n#             \"licensedContent\": true,\n#             \"contentRating\": {},\n#             \"projection\": \"rectangular\"\n#         },\n#         \"status\": {\n#             \"uploadStatus\": \"processed\",\n#             \"privacyStatus\": \"public\",\n#             \"license\": \"youtube\",\n#             \"embeddable\": true,\n#             \"publicStatsViewable\": true,\n#             \"madeForKids\": false\n#         },\n#         \"statistics\": {\n#             \"viewCount\": \"34275\",\n#             \"likeCount\": \"749\",\n#             \"favoriteCount\": \"0\",\n#             \"commentCount\": \"28\"\n#         },\n#         \"player\": {...},\n#         \"topicDetails\": {...},\n#         \"recordingDetails\": {},\n#         \"localizations\": {...}\n#     },\n#     {\n#         \"kind\": \"youtube#video\",\n#         \"etag\": \"hFSA6Fj9y8Hx-zAKzavsM9vR8Kk\",\n#         \"id\": \"knkv4Habj9s\",\n#         \"snippet\": {\n#             \"publishedAt\": \"2023-08-12T19:00:16Z\",\n#             \"channelId\": \"UCU0zC0L4o0qOos83NLhK5ug\",\n#             \"title\": \"Best of the WEEK! Funniest Cats And Dogs Videos \\ud83d\\ude02\",\n#             \"description\": \"Best of the WEEK!  Funniest Cats And Dogs Videos \\ud83d\\ude02\\nBeing an animal lover, these were cute!\\n----------------------------------------------------------------------------\\nDogs can talk, dogs can understand. Who knew?\\ud83d\\ude31 \\nhttps://www.youtube.com/watch?v=D6aecAswvyE&list=PLDae3oPkWscWqZZtu8gqj20Ug9ijrr7Ae&index=1&pp=iAQB  \\n----------------------------------------------------------------------------\\n\\n#dogs #pets #animal #dog #animal #animal2023\",\n#             \"thumbnails\": {...},\n#             \"channelTitle\": \"Ginger Cat\",\n#             \"tags\": [...],\n#             \"categoryId\": \"24\",\n#             \"liveBroadcastContent\": \"none\",\n#             \"defaultLanguage\": \"en\",\n#             \"localized\": {...},\n#             \"defaultAudioLanguage\": \"en\"\n#         },\n#         \"contentDetails\": {\n#             \"duration\": \"PT10M19S\",\n#             \"dimension\": \"2d\",\n#             \"definition\": \"hd\",\n#             \"caption\": \"false\",\n#             \"licensedContent\": true,\n#             \"contentRating\": {},\n#             \"projection\": \"rectangular\"\n#         },\n#         \"status\": {\n#             \"uploadStatus\": \"processed\",\n#             \"privacyStatus\": \"public\",\n#             \"license\": \"youtube\",\n#             \"embeddable\": true,\n#             \"publicStatsViewable\": true,\n#             \"madeForKids\": false\n#         },\n#         \"statistics\": {\n#             \"viewCount\": \"37920\",\n#             \"likeCount\": \"588\",\n#             \"favoriteCount\": \"0\",\n#             \"commentCount\": \"25\"\n#         },\n#         \"player\": {...},\n#         \"topicDetails\": {...},\n#         \"recordingDetails\": {},\n#         \"localizations\": {...}\n#     }\n# ]\n\nEtag\nEvery time you request a resource from these methods, an etag will be attached\nwithin the response. One of the main use of this tag is to keep track of the\nresource. If the resource has not changed since your last request, the etag\nwill stay the same.\nOne can pass the etag along with the request in the if-none-match header\nand if the resource’s etag match with the one you provided, the server will\nreturn HTTP status 304, indicates that you can use the previous response.\nOtherwise, it will return the updated resources.\n\nchannel_id = \"UCSXwxpWZQ7XZ1WL3wqevChA\"\n\nresponse = requests.get(\n    url=\"https://www.googleapis.com/youtube/v3/channels\",\n    params={\"id\":channel_id},\n    headers=headers\n)\n\nheaders[\"if-none-match\"] = response.json()[\"etag\"]\n\nresponse = requests.get(\n    url=\"https://www.googleapis.com/youtube/v3/channels\",\n    params={\"id\":channel_id},\n    headers=headers\n)\n\nresponse.status_code\n\n# 304\n\n\n\n\n",
    "preview": "posts/2023-08-15-youtube-data-api-v3/img/preview.png",
    "last_modified": "2023-08-20T12:32:43+00:00",
    "input_file": "youtube-data-api-v3.knit.md"
  },
  {
    "path": "posts/2023-08-14-soft-cosine-measure/",
    "title": "Mathematical reasoning behind soft cosine measure and reduce complexity from O(N²) to O(N)",
    "description": "Mathmatical proofs of validity of soft cosine measure and reduce complexity from O(N²) to O(N) in deployment.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-14",
    "categories": [
      "math",
      "linear-algebra",
      "soft-cosine",
      "time-complexity"
    ],
    "contents": "\n\nContents\nIntroduction\nProperties of Positive Definite [Semidefinite] Matrix\nSoft Cosine Similarity\n\n\n\\[\n\\newcommand\\norm[1]{\\lVert#1\\rVert}\n\\newcommand\\fnorm[1]{\\left\\lVert#1\\right\\rVert}\n\\newcommand\\inner[2]{\\left\\langle#1,#2\\right\\rangle}\n\\newcommand\\rank{\\text{rank}}\n\\newcommand\\diag{\\text{diag}}\n\\]\n\nIntroduction\nSoft cosine measure is widely used in text mining to retrieve the move relevant documents. It is a modification to the well known cosine similarity score, where the similarity between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is given by\n\\[\n\\text{cosine-score} = \\frac{\\mathbf{x}^T \\mathbf{y}}{\\sqrt{\\mathbf{x}^T \\mathbf{x}} \\cdot \\sqrt{\\mathbf{y}^T \\mathbf{y}}}.\n\\]\nSuch calculation assumes every dimension is completely independent of each other. In reality, this may not be the case. In text mining, words like money and wealth are very closely related but this relationship cannot be captured in the generic cosine similarity score.\nIn 2014, the soft cosine measure was introduced in this paper. It adjusts the cosine similarity score by putting a similarity matrix between the dot product.\n\\[\n\\text{soft-cosine} =  \\frac{\\mathbf{a}^T S\\, \\mathbf{b}}{\\sqrt{\\mathbf{a}^T S \\, \\mathbf{a}} \\cdot \\sqrt{\\mathbf{b}^T S \\, \\mathbf{b}}}.\n\\]\nThis article will dive deep into the mathematical ground to understand this calculation.\nProperties of Positive Definite [Semidefinite] Matrix\n\n\nDefinition 1  (Inner Product) Let \\(\\mathsf{V}\\) be a vector space over \\(\\mathbb{R}\\). An inner product on \\(\\mathsf{V}\\) is a function \\(\\langle \\cdot \\; , \\; \\cdot \\rangle : \\mathsf{V} \\times \\mathsf{V} \\rightarrow \\mathbb{R}\\) such that for all \\(\\mathbf{x}, \\mathbf{y}, \\mathbf{z} \\in \\mathsf{V}\\) and all \\(c \\in \\mathbb{R}\\), the following conditions hold:\n\n\\(\\langle \\mathbf{x} + \\mathbf{z}, \\mathbf{y} \\rangle = \\langle \\mathbf{x}, \\mathbf{y} \\rangle + \\langle \\mathbf{z}, \\mathbf{y} \\rangle\\).\n\n\n\\(\\langle c \\mathbf{x}, \\mathbf{y} \\rangle = c \\langle \\mathbf{x}, \\mathbf{y} \\rangle\\).\n\n\n\\(\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\langle \\mathbf{y}, \\mathbf{x} \\rangle\\).\n\n\n\\(\\langle \\mathbf{x}, \\mathbf{x} \\rangle > 0\\) if \\(\\mathbf{x} \\neq \\mathbf{0}\\).\n\n\n\n\n\nDefinition 2  (Norm) Let \\(\\mathsf{V}\\) be an inner product space. For \\(\\mathbf{x} \\in \\mathsf{V}\\), define the norm or length of \\(\\mathbf{x}\\) by \\(\\norm{\\mathbf{x}} = \\sqrt{\\inner{\\mathbf{x}}{\\mathbf{x}}}\\).\n\n\n\n\nDefinition 3  (Transpose) Let \\(A \\in \\mathsf{M}_{m \\times n}(\\mathbb{R})\\). Define the transpose of \\(A\\) to be \\(A^T \\in \\mathsf{M}_{n \\times n}(\\mathbb{R})\\) such that \\((A^T)_{ij} = A_{ji}\\) for all \\(i, j\\).\n\n\n\n\nDefinition 4  (Symmetric) Let \\(A \\in \\mathsf{M}_n(\\mathbb{R})\\). A is symmetric if \\(A = A^T\\).\n\n\n\n\nDefinition 5  (Orthogonal) Let \\(\\mathsf{V}\\) be in inner product space. Vectors \\(\\mathbf{x}, \\mathbf{y} \\in \\mathsf{V}\\) are orthogonal (perpendicular) if \\(\\langle \\mathbf{x}, \\mathbf{y} \\rangle = 0\\). A subset \\(S\\) of \\(\\mathsf{V}\\) is orthogonal if any two distinct vectors in \\(S\\) are orthogonal. A vector \\(\\mathbf{x} \\in \\mathsf{V}\\) is a unit vector if \\(\\lVert \\mathbf{x} \\rVert = 1\\). Finally, a subset \\(S\\) of \\(\\mathsf{V}\\) is orthonormal if \\(S\\) is orthogonal and consists entirely of unit vectors.\n\n\n\n\nDefinition 6  (Orthogonal Matrix) Let \\(A \\in \\mathsf{M}_n(\\mathbb{R})\\). \\(A\\) is called an orthogonal matrix if \\(A^TA = AA^T = I\\).\n\n\n\n\nTheorem 1  Let \\(A \\in \\mathsf{M}_n(\\mathbb{R})\\). Then \\(A\\) is symmetric if and only if \\(A = Q^TDQ\\), where \\(D\\) is a diagonal matrix consists of eigenvalues of \\(A\\) and \\(Q\\) is an orthogonal matrix whose columns consists of eigenvectors of \\(A\\).\n\n\n\n\nTheorem 2  Let \\(A \\in \\mathsf{M}_n(\\mathbb{R})\\). \\(A\\) is symmetric if and only if for all \\(\\mathbf{x} \\in \\mathbb{R}^n\\)\n\\[\\mathbf{x} = \\sum_{i = 1}^n a_i \\mathbf{v}_i,\\]\nwhere \\(\\mathbf{v}_i\\)’s are the orthonormal eigenvectors of \\(A\\) and \\(a_i \\in \\mathbb{R}\\) for \\(i = 1, 2, \\dots, n\\).\n\n\n\n\nDefinition 7  (Positive Definite) Let \\(A \\in \\mathsf{M}_n(\\mathbb{R})\\). \\(A\\) is called positive definite [positive semidefinite] if \\(A\\) is symmetric and \\(\\mathbf{x}^T A \\mathbf{x} > 0\\) \\(\\left[ \\mathbf{x}^T A \\mathbf{x} \\geq 0 \\right]\\) for all \\(\\mathbf{0} \\neq \\mathbf{x} \\in \\mathbb{R}^n\\).\n\n\n\n\nTheorem 3  Let \\(A \\in \\mathsf{M}_n(\\mathbb{R})\\). Then \\(A\\) is positive definite [semidefinite] if and only if all its eigenvalues are positive [nonnegative].\n\n\n\n\nProof. Since \\(A\\) is symmetric, by Theorem 2, we know that for all \\(\\mathbf{x} \\in \\mathbb{R}^n\\),\n\\[\\mathbf{x} = \\sum_{i = 1}^n a_i \\mathbf{v}_i,\\]\nwhere \\(\\mathbf{v}_i\\)’s are the orthonormal eigenvectors of \\(A\\) and \\(a_i \\in \\mathbb{R}\\) for \\(i = 1, 2, \\dots, n\\). Then\n\\[A\\mathbf{x} = \\sum_{i = 1}^n a_i A\\mathbf{v}_i = \\sum_{i = 1}^n a_i \\lambda_i \\mathbf{v}_i.\\]\nHence\n\\[\n\\begin{align*}\n\\mathbf{x}^T A \\mathbf{x} &= \\left( \\sum_{i = 1}^n a_i \\mathbf{v}_i \\right) \\left( \\sum_{j = 1}^n a_j \\lambda_j \\mathbf{v}_j \\right) \\\\[5pt]\n&= \\sum_{i = 1}^n \\sum_{j = 1}^n a_i a_j \\lambda_j \\mathbf{v}_i \\mathbf{v}_j \\\\[5pt]\n&= \\sum_{i = 1}^n a_i^2 \\lambda_i.\n\\end{align*}\n\\]\nHence, the result holds.\n\n\n\n\nTheorem 4  Let \\(A \\in \\mathsf{M}_n(\\mathbb{R})\\). if \\(A\\) is positive definite [semidefinite], there exists an unique positive definite [semidefinite] matrix \\(B \\in \\mathsf{M}_n(\\mathbb{R})\\) such that \\(A = B^T B\\).\n\n\n\n\nProof. Suppose \\(A\\) is positive definite [semidefinite]. By Theorem 1, we know that\n\\[A = Q^TDQ,\\]\nwhere \\(Q\\) consists of orthogonal eigenvectors of \\(A\\) and \\(D = \\diag(\\lambda_1, \\lambda_2, \\dots, \\lambda_n)\\) are the eigenvalues of \\(A\\) and hence are all positive [nonnegative] by Theorem 3.\nDefine the matrix\n\\[B = Q^T D^{1/2} Q,\\]\nwhere \\(D^{1/2} = \\diag(\\sqrt{\\lambda_1}, \\sqrt{\\lambda_2}, \\dots, \\sqrt{\\lambda_n})\\). Since \\(\\sqrt{\\lambda_i} > 0\\) \\(\\left[\\sqrt{\\lambda_i} \\geq 0\\right]\\) for \\(i = 1, 2, \\dots, n\\), we know that \\(B\\) is also positive definite [semidefinite]. Also,\n\\[B^2 = Q^T D^{1/2} Q Q^T D^{1/2} Q = Q^T D^{1/2} D^{1/2} Q = Q^T D Q = A.\\]\nFor the uniqueness part, suppose \\(C\\) is another positive definite [semidefinite] roots of \\(A\\). Let\n\\[C = P^T H P,\\]\nwhere \\(H = \\diag(\\mu_1, \\mu_2, \\dots, \\mu-n)\\) and \\(P\\) is orthogonal. Since \\(C^2 = P^T H^2 P = A\\), WLOG, we may assume \\(\\mu_i^2 = \\lambda_i\\) for \\(i = 1, 2, \\dots, n\\). Thus, we have\n\\[C = P^T D^{1/2} P.\\]\nAnd since \\(B^2 = A = C^2\\), we have\n\\[\n\\begin{align*}\n(Q^T D^{1/2} Q)^2 &= (P^T D^{1/2} P)^2 \\\\[5pt]\nQ^T D Q &= P^T D P \\\\[5pt]\n(P Q^T) D &= D (P Q^T).\n\\end{align*}\n\\]\nLet \\(W = PQ^T\\),\n\\[\nD = \\begin{pmatrix}\n\\lambda_{1} I_{k_1} & 0 & \\cdots & 0 \\\\\n0 & \\lambda_{2} I_{k_2} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_{r} I_{k_r} \\end{pmatrix}\n\\]\nand\n\\[\nW = \\begin{pmatrix}\nW_{11} & W_{12} & \\cdots & W_{1,k_r} \\\\\nW_{21} & W_{22} & \\cdots & W_{2,k_r} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\nW_{k_1,1} & W_{k_2,2} & \\cdots & W_{k_r,k_r}\n\\end{pmatrix}\n\\]\nSince \\(WD = DW\\), we know that \\(W_{ij} = O\\) for \\(i \\neq j\\). Now consider\n\\[\n\\begin{align*}\nWD^{1/2} &= \\diag(W_{11}, \\dots, W_{k_r, k_r}) \\cdot \\diag(\\sqrt{\\lambda_{k_1}}I_{k_1}, \\dots, \\sqrt{\\lambda_{k_r}}I_{k_r}) \\\\[5pt]\n&= \\diag(\\sqrt{\\lambda_{k_1}}W_{11}, \\dots, \\sqrt{\\lambda_{k_r}}W_{k_r, k_r}) \\\\[5pt]\n&= \\diag(\\sqrt{\\lambda_{k_1}}I_{k_1}, \\dots, \\sqrt{\\lambda_{k_r}}I_{k_r}) \\cdot \\diag(W_{11}, \\dots, W_{k_r, k_r}) \\\\[5pt]\n&= D^{1/2}W.\n\\end{align*}\n\\]\nHence,\n\\[\n\\begin{align*}\nPQ^TD^{1/2} &= D^{1/2} PQ^T \\\\[5pt]\nQ^T D^{1/2} Q &= P^T D^{1/2} P \\\\[5pt]\nB &= C,\n\\end{align*}\n\\]\nwhich shows the uniqueness of \\(B\\).\n\n\n\n\nTheorem 5  If \\(A \\in \\mathsf{M}_n(\\mathbb{R})\\) is positive definite, then \\(\\mathbf{x}^T A \\mathbf{y}\\) defines an inner product for all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\).\n\n\n\n\nProof. We just need to confirm the 4 conditions listed in Definition 1. For all \\(\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\), let \\(\\inner{\\mathbf{x}}{\\mathbf{y}} = \\mathbf{x}^T A \\mathbf{y}\\). Then\n\\[\n\\begin{align*}\n\\inner{\\mathbf{x}}{\\mathbf{y}} &= (\\inner{\\mathbf{x}}{\\mathbf{y}})^T \\\\[5pt]\n&= (\\mathbf{x}^T A \\mathbf{y})^T \\\\[5pt]\n&= \\mathbf{y}^T A \\mathbf{x} \\qquad  \\text{($A$ is symmetric)} \\\\[5pt]\n&= \\inner{\\mathbf{y}}{\\mathbf{x}}.\n\\end{align*}\n\\]\nThe other three conditions are trivial.\n\n\nSoft Cosine Similarity\nRecall that Cosine Similarity between two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) is given by\n\\[\n\\begin{align*}\n\\text{cosine}(\\mathbf{x}, \\mathbf{y}) &= \\frac{\\mathbf{x}^T \\mathbf{y}}{\\sqrt{\\mathbf{x}^T \\mathbf{x}} \\cdot \\sqrt{\\mathbf{y}^T \\mathbf{y}}} \\\\[5pt]\n&= \\left( \\frac{\\mathbf{x}}{\\sqrt{\\mathbf{x}^T \\mathbf{x}}} \\right)^T \\left( \\frac{\\mathbf{y}}{\\sqrt{\\mathbf{y}^T \\mathbf{y}}} \\right) \\\\[5pt]\n&= \\hat{\\mathbf{x}}^T \\hat{\\mathbf{y}} \\\\[5pt]\n&= \\sum_{i = 1}^n \\hat{x}_i \\hat{y}_i\n\\end{align*}\n\\]\nwhere \\(n\\) is the dimension of the vectors and \\(\\hat{\\mathbf{x}}\\) is the unit vector of \\(\\mathbf{x}\\). The time and space complexity of the calculation is \\(O(n)\\).\nThe problem of the generic cosine similarity score is that it assumes all the dimension are completely independent to each other. In mathematical terms, they are mutually orthogonal. The soft-cosine measure introduced in this paper solve this problem by introducing a correlation matrix to the middle of the dot product when calculating the cosine score. Below shows the matrix form of the calculation.\n\\[\n\\text{soft-cosine} =  \\frac{\\mathbf{a}^T S\\, \\mathbf{b}}{\\sqrt{\\mathbf{a}^T S \\, \\mathbf{a}} \\cdot \\sqrt{\\mathbf{b}^T S \\, \\mathbf{b}}}\n\\]\nIf we replace \\(S\\) by \\(I\\) (the identity matrix), it reduces to the generic cosine score immediately. Theorem 5 also shows that this calculation is indeed another inner product. This means it is another valid way to measure the distance between two vectors. For instance, by Definition 2, the distance between \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) with respect to the correlation matrix \\(S\\) is given by\n\\[\n\\begin{align*}\n\\norm{\\mathbf{a} - \\mathbf{b}}_S &= \\sqrt{(\\mathbf{a} - \\mathbf{b})^T S (\\mathbf{a} - \\mathbf{b})}.\n\\end{align*}\n\\]\nBesides, by Theorem 4, there exists a unique positive definite matrix \\(S^{1/2}\\) such that \\(S = S^{1/2} S^{1/2}\\). With some manipulation, the soft cosine measure between \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) with respect to the correlation matrix \\(S\\) can reduce to a simple dot product between the unit vectors \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\), where \\(\\alpha = S^{1/2} \\mathbf{a}\\) and \\(\\beta = S^{1/2} \\mathbf{b}\\).\n\\[\n\\begin{align*}\n\\text{soft-cosine} &=  \\frac{\\mathbf{a}^T S\\, \\mathbf{b}}{\\sqrt{\\mathbf{a}^T S \\, \\mathbf{a}} \\cdot \\sqrt{\\mathbf{b}^T S \\, \\mathbf{b}}}\\\\[5pt]\n&= \\frac{(S^{1/2}\\mathbf{a})^T \\cdot (S^{1/2}\\mathbf{b})}{\\sqrt{(S^{1/2}\\mathbf{a})^T(S^{1/2}\\mathbf{a})} \\cdot \\sqrt{(S^{1/2}\\mathbf{b})^T(S^{1/2}\\mathbf{b})}} \\\\[5pt]\n&= \\frac{\\alpha^T \\cdot \\beta}{\\sqrt{\\alpha^T \\cdot \\alpha} \\cdot \\sqrt{\\beta^T \\cdot \\beta}}\\\\[5pt]\n&= \\hat{\\alpha} \\cdot \\hat{\\beta}.\n\\end{align*}\n\\]\nThis can significantly reduce the time and space complexity from \\(O(n^2)\\) to \\(O(n)\\) if we store the transformed vectors in database and perform the soft cosine measure by calculating the dot product when needed.\n\n\n\n",
    "preview": "posts/2023-08-14-soft-cosine-measure/img/preview.png",
    "last_modified": "2023-08-20T12:32:21+00:00",
    "input_file": "soft-cosine-measure.knit.md"
  },
  {
    "path": "posts/2023-08-10-writing-airflow-dags/",
    "title": "Writing Airflow DAGs",
    "description": "Introduction to how to write a DAG on Airflow. Including writing custom operators, XComs, branching operators, triggers and variables.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-10",
    "categories": [
      "python",
      "airflow"
    ],
    "contents": "\n\nContents\nIntroduction\nDAG Location\nBasic DAG File\nCustom Operator\nAirflow Context\nTaskflow\nXComs\nBranching\nTriggers\nDatasets\nTriggerDAGRunOperator\n\nVariables\nSet Up Variables\nCalling Variables\nEncrypt Values\nMasking Values\n\nAppendix\n\nIntroduction\nThis article aims to introduce how to write an airflow DAG. We will go through the basic BashOperator and PythonOperator, using Airflow TaskFlow decorators, Airflow context, passing information between tasks using XComs, branching tasks based on conditions, and more.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDAG Location\nThis section will introduce how to write a Directed Acyclic Graph (DAG) in Airflow. Within the Docker image’s main folder, you should find a directory named dags. Create one if you do not. This directory should link to the containers as it is specified in the docker-compose.yaml.\n\n\nx-airflow-common:\n  &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.6.3}\n  environment:\n    ...\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n    - ${AIRFLOW_PROJ_DIR:-.}/dag-inputs:/opt/airflow/dag-inputs\n    - ${AIRFLOW_PROJ_DIR:-.}/dag-outputs:/opt/airflow/dag-outputs\n\n\nInside the dags directory, create a file named trial_dag.py with the following content. It will create a DAG in Airflow and you can find it in the main page of Airflow (default: localhost:8080).\nBasic DAG File\nBelow shows an example of a DAG.\n\nfrom airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"retries\": 5,\n    \"retry_delay\": timedelta(minutes=2)\n}\n\ndef _task2(w):\n    print(w)\n\nwith DAG(\n    dag_id=\"trial_dag\",\n    description=\"A trial dag\",\n    default_args=default_args,\n    start_date=datetime(2023,8,1),\n    # end_date=datetime(2024,7,31),\n    schedule=\"@once\"  # Use `schedule_interval` if using Airflow 2.3 or below\n) as dag: \n    task1 = BashOperator(\n        task_id=\"task1\",\n        bash_command=\" echo 'hello'\"\n    )\n    \n    task2 = PythonOperator(\n        task_id=\"task2\",\n        python_callable=_task2,\n        op_kwargs={\"w\": \"world\"}\n    )\n    \n    task1 >> task2  # The order of task being executed.\n\nThe snippet above shows a simple DAG definition file. It first imports the required modules from airflow and datetime. Then define the default_args for the dag (see more available key-value pairs here).\nThere are multiple ways to initialise a dag. This time we used with DAG() as dag: with dag_id, description, schedule, etc. We will see some other methods to define a dag in later sections.\nA task in a dag is an operator object. There are two basic operators: BashOperator and PythonOperator. The BashOperator is used to execute bash command and the PythonOperator is used to execute Python functions. The name of the Python function is passed to the python_callable parameter and the arguments of the primitive function can be passed through the operator using the op_kwargs and op_args arguments.\nFinally we need to specify the order of the tasks by task1 >> task2.\nCustom Operator\nWe can also create custom operators inside the plugins directory which is located at the main Docker image directory. Please make sure the directory is created and specified in docker-compose.yaml. Please also create an empty __init__.py in the plugins directory. Now, we can create a .py file inside the plugins directory to create operators like below.\n\n# plugins/hello_operator.py\n\nfrom airflow.models import BaseOperator\n\nclass HelloOperator(BaseOperator):\n    def __init__(self, name, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.name = name\n\n    def execute(self, context):\n        self.log.info(\"Input `name`: {self.name}.\") # Logging\n        message = f\"Hello {self.name}\"\n        print(message)\n\nTo use the custom operators, simply import the operators from a module as usual.\n\nfrom hello_operator import HelloOperator\n\nwith Dag() as dag:\n    task1 = HelloOperator(task_id=\"task1\", name=\"Bob\")\n    \n    task1\n\nAirflow Context\nLike Gibhub context (see this article), Airflow also provide a way to access information about the running dag and task. As seen in the custom operator, the execute method in the operator class takes in a context argument.\nBelow shows two examples on how to use these context information.\n\n# plugins/hello_operator.py\n\nfrom airflow.models import BaseOperator\n\nclass HelloOperator(BaseOperator):\n    template_fields = ['name']\n    \n    def __init__(self, name, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.name = name\n\n    def execute(self, context):   # context is a TypedDict. See below section\n        message = f\"The name of this DAG is {self.name}\"\n        print(message)\n        \n        ts = context[\"ts_nodash\"] # context is a TypedDict. See below section\n        print(f\"This DAG started at {ts}\")\n\nThe above snippet describe how to use the context variable inside a custom operator. One can also use jinja syntax to specify the information like below. Just remember to specify which argument will be using jinja syntax in the custom operator class (see above template_fields = ['name']).\n\n# dags/hello_dag.py\n\nfrom hello_operator import HelloOperator\n\nwith Dag() as dag:\n    task1 = HelloOperator(task_id=\"task1\", name=\"{{ dag.dag_id }}\")\n    \n    task1\n\nA detailed list of available values from context can be found in [appendix][#appendix].\nTaskflow\nThis section describe how to write a DAG with the TaskFlow API paradigm. The paradigm converts functions into Airflow DAGs or tasks with the dag and task decorators. Below shows a simple DAG written in this paradigm.\n\nfrom airflow.decorators import task, dag\nfrom airflow.operators.python import get_current_context\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"retries\": 0,\n    \"retry_delay\": timedelta(minutes=2)\n}\n\n@dag(\n    dag_id=\"trial_dag\",\n    schedule=\"@once\",\n    start_date=datetime(2023,8,1),\n    default_args=default_args\n)\ndef trial_dag():\n    @task(task_id=\"task1\")\n    def task1():\n        print(\"hello\")\n    \n    @task(task_id=\"task2\")\n    def task2(name):\n        context = get_current_context()    # Use context within TaskFlow API\n        print(f\"The name of the DAG is {name}\")\n        dag_loc = context[\"dag\"].fileloc\n        print(f\"The location of the DAG is {dag_loc}\")\n    \n    task1() >> task2(\"{{ dag.dag_id }}\")   # Set the order of tasks\n\ntrial_dag()    # Initialise the DAG\n\nXComs\nOften times, we would like to pass some information from the upstream task to the downstream task. In Airflow, we can achieve this via the XComs. Please note that XComs is not designed to transfer large data. If you are using MySQL as the Airflow database, you can only transfer information within 64kb. You can define your own XComs backend if needed. Yet, this is out of the scope of this article.\nThere are two ways to send information in one task.\nSimply return the value (see ModelTrainingOperator)\nUsing ti.xcom_push (see ETLOperator)\nThe pushed values are stored in the Airflow context in the form of key-value pairs. That means, there is a specific key to each returned or pushed value. The key for the first method is always return_value but you can specify the key you want in the second method.\nAs the pushed values are stored in the Airflow context, it can be accessed via the context argument (see ModelSelectionOperator) or via the jinja template (see model_training_task1). Yet, the values pulled from jinja template are always in text form.\nThe script below mimics a ML model training pipeline, which utilises the XComs to transfer information between tasks.\n\nfrom airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.models import BaseOperator\nfrom random import randint\nimport numpy as np\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"retries\": 0,\n    \"retry_delay\": timedelta(minutes=2)\n}\n\nclass ETLOperator(BaseOperator):\n    template_fields = ['name']\n    def __init__(self, name, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.name = name\n\n    def execute(self, context):\n        self.log.info(f\"Input parameter: {self.name}\")\n        context[\"ti\"].xcom_push(key=\"dag_id\", value=self.name)\n\nclass ModelTrainingOperator(BaseOperator):\n    template_fields = ['name']\n    def __init__(self, name, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.name = name\n    \n    def execute(self, context):\n        self.log.info(f\"Previous output: {self.name}\")\n        task_id = context[\"ti\"].task_id\n        return {\"task_id\": task_id, \"score\": randint(1, 10)}\n\nclass ModelSelectionOperator(BaseOperator):\n    def __init__(self, models, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.models = models\n    \n    def execute(self, context):\n        models = self.models.split(\",\")\n        result = []\n        for model in models:\n            result.append(context[\"ti\"].xcom_pull(model))\n        \n        self.log.info(result)\n        best_model = result[np.argmax([x[\"score\"] for x in result])]\n        self.log.info(f\"Best model: {best_model}\")\n\nwith DAG(\n    dag_id=\"trial_dag\",\n    default_args=default_args,\n    start_date=datetime(2023,8,1),\n    schedule=\"@once\",\n    description=\"A trial dag\"\n) as dag: \n    etl_task = ETLOperator(\n        task_id=\"etl_task\",\n        name=\"{{ dag.dag_id }}\"\n    )\n    \n    model_training_task1 = ModelTrainingOperator(\n        task_id=\"model_training_task1\",\n        name=\"{{ ti.xcom_pull(task_ids=['etl_task'], key='dag_id') }}\"\n    )\n    \n    model_training_task2 = ModelTrainingOperator(\n        task_id=\"model_training_task2\",\n        name=\"{{ ti.xcom_pull(task_ids=['etl_task'], key='dag_id') }}\"\n    )\n    \n    model_training_task3 = ModelTrainingOperator(\n        task_id=\"model_training_task3\",\n        name=\"{{ ti.xcom_pull(task_ids=['etl_task'], key='dag_id') }}\"\n    )\n\n    model_selection_task = ModelSelectionOperator(\n        task_id=\"model_selection_task\",\n        models=\"model_training_task1,model_training_task2,model_training_task3\"\n    )\n    \n    etl_task >> \\\n    [model_training_task1, model_training_task2, model_training_task3] >> \\\n    model_selection_task\n\n\n\n\nBranching\nThe previous example will run all three model_training tasks after the etl_task has finished. This section shows how to do branching, which means to execute a particular task based on some conditions.\n\nfrom airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.models import BaseOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.operators.branch import BaseBranchOperator\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.edgemodifier import Label\nfrom random import randint, choice\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"retries\": 0,\n    \"retry_delay\": timedelta(minutes=2)\n}\n\nclass CriteriaBranchingOperator(BaseBranchOperator):\n    def __init__(self, methods, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.methods = methods\n    \n    def choose_branch(self, context):\n        return choice(self.methods)\n\nclass ModelTrainingOperator(BaseOperator):\n    def execute(self, context):\n        task_id = context[\"ti\"].task_id\n        return {\"task_id\": task_id, \"score\": randint(1, 10)}\n\nwith DAG(\n    dag_id=\"trial_dag\",\n    default_args=default_args,\n    start_date=datetime(2023,8,1),\n    schedule=\"@once\",\n    description=\"A trial dag\"\n) as dag: \n    etl_task = EmptyOperator(\n        task_id=\"etl_task\"\n    )\n\n    methods = [\"method_A\", \"method_B\", \"method_C\", \"method_D\"]\n\n    criteria_selection = CriteriaBranchingOperator(\n        task_id=\"criteria_selection\",\n        methods=methods\n    )\n\n    etl_task >> criteria_selection\n\n    deploy_model = EmptyOperator(\n        task_id=\"deploy_model\",\n        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS\n    )\n\n    for method in methods:\n        method_task = ModelTrainingOperator(\n            task_id=method\n        )\n\n        criteria_selection >> Label(method) >> method_task >> deploy_model\n\n\n\n\nTriggers\nThis section introduce triggers to start a DAG from another DAG. We will go through 2 different methods:\nDatasets\nTriggerDAGRunOperator\nDatasets\nTo trigger a DAG using Datasets, the Dataset class must be imported first. Then define a dataset with an uri. In one of the task in the trigger_dag, set the outlets to a list of the defined dataset. Finally, in the target_dag, set the schedule to be a list of datasets that will trigger the DAG to run. See an example below.\n\nfrom airflow import Dataset, DAG \nfrom airflow.models import BaseOperator\nfrom datetime import datetime\n\ndataset = Dataset(\"/opt/airflow/dag-outputs/test_data.json\")\n\nclass TriggerOperator(BaseOperator):\n    def execute(self, context):\n        outlets = context[\"outlets\"]\n        ts = context[\"ts_nodash\"]\n        for outlet in outlets:\n            with open(outlet.uri, \"w\") as f:\n                f.write(ts)\n        \n        self.log.info(context[\"outlets\"])\n        # [2023-08-13, 21:34:06 UTC] {trial_dag.py:15} INFO - \n        #  [Dataset(uri='/opt/airflow/dag-outputs/test_data.json', extra=None)]\n\n        self.log.info(\"Triggering Dataset\")\n        # [2023-08-13, 21:34:06 UTC] {trial_dag.py:16} INFO - Triggering Dataset\n\nclass ActionOperator(BaseOperator):\n    def execute(self, context):\n        self.log.info(context[\"dag\"].schedule_interval)\n        # [2023-08-13, 21:34:06 UTC] {trial_dag.py:20} INFO - Dataset\n        \n        self.log.info(context[\"dag\"].dataset_triggers)\n        # [2023-08-13, 21:34:06 UTC] {trial_dag.py:21} INFO - \n        #  [Dataset(uri='/opt/airflow/dag-outputs/test_data.json', extra=None)]\n\nwith DAG(\n    dag_id=\"trigger_dag\",\n    schedule=\"@once\",\n    start_date=datetime(2023,8,1)\n) as dag:\n    trigger_task = TriggerOperator(\n        task_id=\"trigger_task\",\n        outlets=[dataset]\n    )\n\n    trigger_task\n\nwith DAG(\n    dag_id=\"target_dag\",\n    schedule=[dataset],\n    start_date=datetime(2023,8,1)\n) as dag:\n    action_task = ActionOperator(\n        task_id=\"action_task\"\n    )\n\n    action_task\n\nTriggerDAGRunOperator\nAnother method to trigger a DAG to run is to use the TriggerDAGRunOperator. In the trigger_dag, create a task with TriggerDAGRunOperator, set the trigger_dag_id to the DAG you wish to trigger. You can also pass some information to the target_dag using the conf argument as shown below.\nIn the target_dag, you can retrieve the conf by accessing context[\"dag_run\"].conf.\n\nfrom airflow import DAG \nfrom airflow.models import BaseOperator\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\nfrom datetime import datetime\n\nclass ActionOperator(BaseOperator):\n    def execute(self, context):\n        # Retrieving conf from previous dag in TriggerDagRunOperator\n        self.log.info(context[\"dag_run\"].conf)\n        # {\"upstream_dag_id\": \"trigger_dag\", \"last_update\": \"...\"}\n\nwith DAG(\n    dag_id=\"trigger_dag\",\n    start_date=datetime(2023,8,1),\n    schedule=\"@once\"\n) as dag:\n    trigger_task = TriggerDagRunOperator(\n        task_id=\"trigger_task\",\n        trigger_dag_id=\"target_dag\",\n        conf = {\n            \"upstream_dag_id\": \"{{ dag.dag_id }}\",\n            \"last_update\": \"{{ ts_nodash }}\"\n        }\n    )\n\n    trigger_task\n\nwith DAG(\n    dag_id=\"target_dag\",\n    start_date=datetime(2023,8,1),\n    schedule=\"@once\"\n) as dag:\n    action_task = ActionOperator(\n        task_id=\"action_task\"\n    )\n\n    action_task\n\nVariables\nThis section will cover how to define and use variables in Airflow context.\nSet Up Variables\nThere are two ways to set up Airflow variables.\nOn Airflow main page, go to Admin –> Variables and set up there.\nSet up environment variables with prefix AIRFLOW_VAR_<NAME_OF_VAR>.\nCalling Variables\n\nfrom airflow import DAG \nfrom airflow.models import BaseOperator\nfrom datetime import datetime\nfrom airflow.models import Variable\n\nclass TrialOperator(BaseOperator):\n    template_fields = [\"json_var\", \"value_var\"]\n    def __init__(self, json_var, value_var, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.json_var = json_var\n        self.value_var = value_var\n\n    def execute(self, context):\n        # Access from jinja template with type dict\n        self.log.info(self.json_var)\n\n        # Access from Variable with type dict\n        self.log.info(Variable.get(\"my_var\", deserialize_json=True))\n\n        # Access from jinja template with type str\n        self.log.info(self.value_var)\n\n        # Access from Variable with type str\n        self.log.info(Variable.get(\"my_var\", deserialize_json=False))\n\n        # The following will fail\n        self.log.info(context[\"var\"][\"json\"][\"my_var\"])\n\n        # The following will fail\n        self.log.info(context[\"var\"][\"value\"][\"my_var\"])\n\n\nwith DAG(\n    dag_id=\"trial_dag\",\n    start_date=datetime(2023,8,1),\n    schedule=\"@once\"\n) as dag:\n    task1 = TrialOperator(\n        task_id=\"task1\",\n        json_var=\"{{  var.json.my_var  }}\",\n        value_var=\"{{  var.value.my_var  }}\"\n    )\n\n    task1\n\n# [2023-08-13, 18:12:03 UTC] {trial_dag.py:16} INFO - {'name': 'Bob', 'age': 21}\n# [2023-08-13, 18:12:03 UTC] {trial_dag.py:19} INFO - {'name': 'Bob', 'age': 21}\n# [2023-08-13, 18:12:03 UTC] {trial_dag.py:22} INFO - {\"name\": \"Bob\", \"age\": 21}\n# [2023-08-13, 18:12:03 UTC] {trial_dag.py:25} INFO - {\"name\": \"Bob\", \"age\": 21}\n\nEncrypt Values\nTo encrypt the value of variables, first generate a fernet_key from the following.\n\nfrom cryptography.fernet import Fernet\nFernet.generate_key().decode()\n\n# 'bbg62_v203tply8_kXf8rD4WJ93IDYw6EVdQ7u2G-Bk='\n\nThen change the following line in airflow.cfg under [core] section.\n\n[core]\n...\nfernet_key = bbg62_v203tply8_kXf8rD4WJ93IDYw6EVdQ7u2G-Bk=\n\nFinally rebuild the Docker containers. Now all the variables created in Admin –> Variables are encrypted. One can check the variable table from the Postgres container.\n\nSELECT * FROM variable;\n\n--  id |   key    |          val          | description | is_encrypted \n-- ----+----------+-----------------------+-------------+--------------\n--   1 | my_var   | gAAAAABk2...wiuSFTRw= |             | t\n--   2 | my_token | gAAAAABk2...goZh6tg== |             | t\n\nMasking Values\nBy default, Airflow will hide the variable value if its key contains access_token, api_key, apikey,authorization, passphrase, passwd, password, private_key, secret or token. To extend the list, you can specify them in the airflow.cfg.\n\n[core]\nsensitive_var_conn_names = comma,separated,sensitive,names\n\n\nfrom airflow import DAG \nfrom airflow.models import BaseOperator\nfrom datetime import datetime\nfrom airflow.models import Variable\n\nclass TrialOperator(BaseOperator):\n    def execute(self, context):\n        my_token = Variable.get(\"my_token\", deserialize_json=False)\n        self.log.info(my_token)\n        self.log.info(my_token == \"123456\")\n\nwith DAG(\n    dag_id=\"trial_dag\",\n    start_date=datetime(2023,8,1),\n    schedule=\"@once\"\n) as dag:\n    task1 = TrialOperator(\n        task_id=\"task1\"\n    )\n\n    task1\n\n# [2023-08-13, 18:16:04 UTC] {trial_dag.py:9} INFO - ***\n# [2023-08-13, 18:16:04 UTC] {trial_dag.py:10} INFO - True\n\nAppendix\n\n# Context dictionary\n{\n    'conf': <***.configuration.AirflowConfigParser object at 0xffff82ce0c10>, # Not useful\n    'dag': <DAG: trial_dag>, \n    'dag_run': <DagRun trial_dag @ 2023-08-11 11:51:11.303634+00:00: manual__2023-08-11T11:51:11.303634+00:00, state:running, queued_at: 2023-08-11 11:51:11.312548+00:00. externally triggered: True>, \n    'data_interval_end': DateTime(2023, 8, 11, 11, 51, 11, 303634, tzinfo=Timezone('UTC')), \n    'data_interval_start': DateTime(2023, 8, 11, 11, 51, 11, 303634, tzinfo=Timezone('UTC')), \n    'ds': '2023-08-11', \n    'ds_nodash': '20230811', \n    'execution_date': DateTime(2023, 8, 11, 11, 51, 11, 303634, tzinfo=Timezone('UTC')), \n    'expanded_ti_count': None, \n    'inlets': [], \n    'logical_date': DateTime(2023, 8, 11, 11, 51, 11, 303634, tzinfo=Timezone('UTC')), \n    'macros': <module '***.macros' from '/home/***/.local/lib/python3.7/site-packages/***/macros/__init__.py'>, \n    'next_ds': '2023-08-11', \n    'next_ds_nodash': '20230811', \n    'next_execution_date': DateTime(2023, 8, 11, 11, 51, 11, 303634, tzinfo=Timezone('UTC')), \n    'outlets': [], \n    'params': {}, \n    'prev_data_interval_start_success': DateTime(2023, 8, 11, 11, 44, 10, 670770, tzinfo=Timezone('UTC')), \n    'prev_data_interval_end_success': DateTime(2023, 8, 11, 11, 44, 10, 670770, tzinfo=Timezone('UTC')), \n    'prev_ds': '2023-08-11', \n    'prev_ds_nodash': '20230811', \n    'prev_execution_date': DateTime(2023, 8, 11, 11, 51, 11, 303634, tzinfo=Timezone('UTC')), \n    'prev_execution_date_success': DateTime(2023, 8, 11, 11, 44, 10, 670770, tzinfo=Timezone('UTC')), \n    'prev_start_date_success': DateTime(2023, 8, 11, 11, 44, 11, 512290, tzinfo=Timezone('UTC')), \n    'run_id': 'manual__2023-08-11T11:51:11.303634+00:00', \n    'task': <Task(HelloOperator): task2>, \n    'task_instance': <TaskInstance: trial_dag.task2 manual__2023-08-11T11:51:11.303634+00:00 [running]>, \n    'task_instance_key_str': 'trial_dag__task2__20230811', \n    'test_mode': False, \n    'ti': <TaskInstance: trial_dag.task2 manual__2023-08-11T11:51:11.303634+00:00 [running]>, \n    'tomorrow_ds': '2023-08-12', \n    'tomorrow_ds_nodash': '20230812', \n    'triggering_dataset_events': <Proxy at 0xffff7ac07230 with factory <function TaskInstance.get_template_context.<locals>.get_triggering_events at 0xffff7abe78c0>>, \n    'ts': '2023-08-11T11:51:11.303634+00:00', \n    'ts_nodash': '20230811T115111', \n    'ts_nodash_with_tz': '20230811T115111.303634+0000', \n    'var': {'json': None, 'value': None}, \n    'conn': None, \n    'yesterday_ds': '2023-08-10', \n    'yesterday_ds_nodash': '20230810'\n}\n\n\n# context[\"dag\"]\n{\n    'access_control': None, \n    'add_task': <bound method DAG.add_task of <DAG: trial_dag>>, \n    'add_tasks': <bound method DAG.add_tasks of <DAG: trial_dag>>, \n    'allow_future_exec_dates': False, \n    'auto_register': True, \n    'bulk_sync_to_db': <bound method DAG.bulk_sync_to_db of <class '***.models.dag.DAG'>>, \n    'bulk_write_to_db': <bound method DAG.bulk_write_to_db of <class '***.models.dag.DAG'>>, \n    'catchup': True, \n    'clear': <bound method DAG.clear of <DAG: trial_dag>>, \n    'clear_dags': <bound method DAG.clear_dags of <class '***.models.dag.DAG'>>, \n    'cli': <bound method DAG.cli of <DAG: trial_dag>>, \n    'concurrency': 16, \n    'concurrency_reached': False, \n    'create_dagrun': <bound method DAG.create_dagrun of <DAG: trial_dag>>, \n    'dag_id': 'trial_dag', \n    'dagrun_timeout': None, \n    'dataset_triggers': [], \n    'date_range': <bound method DAG.date_range of <DAG: trial_dag>>, \n    'deactivate_stale_dags': <function DAG.deactivate_stale_dags at 0xffff7ed833b0>, \n    'deactivate_unknown_dags': <function DAG.deactivate_unknown_dags at 0xffff7ed83290>, \n    'default_args': {'owner': '***', 'retries': 0, 'retry_delay': datetime.timedelta(seconds=120)}, \n    'default_view': 'grid', \n    'description': 'A trial dag', \n    'doc_md': None, \n    'edge_info': {}, \n    'end_date': None, \n    'fileloc': '/opt/***/dags/trial_dag.py', \n    'filepath': 'trial_dag.py', \n    'folder': '/opt/***/dags', \n    'following_schedule': <bound method DAG.following_schedule of <DAG: trial_dag>>, \n    'full_filepath': '/opt/***/dags/trial_dag.py', \n    'get_active_runs': <bound method DAG.get_active_runs of <DAG: trial_dag>>, \n    'get_concurrency_reached': <bound method DAG.get_concurrency_reached of <DAG: trial_dag>>, \n    'get_dagrun': <bound method DAG.get_dagrun of <DAG: trial_dag>>, \n    'get_dagruns_between': <bound method DAG.get_dagruns_between of <DAG: trial_dag>>, \n    'get_default_view': <bound method DAG.get_default_view of <DAG: trial_dag>>, \n    'get_doc_md': <bound method DAG.get_doc_md of <DAG: trial_dag>>, \n    'get_edge_info': <bound method DAG.get_edge_info of <DAG: trial_dag>>, \n    'get_is_active': <bound method DAG.get_is_active of <DAG: trial_dag>>, \n    'get_is_paused': <bound method DAG.get_is_paused of <DAG: trial_dag>>, \n    'get_last_dagrun': <bound method DAG.get_last_dagrun of <DAG: trial_dag>>, \n    'get_latest_execution_date': <bound method DAG.get_latest_execution_date of <DAG: trial_dag>>, \n    'get_next_data_interval': <bound method DAG.get_next_data_interval of <DAG: trial_dag>>, \n    'get_num_active_runs': <bound method DAG.get_num_active_runs of <DAG: trial_dag>>, \n    'get_num_task_instances': <function DAG.get_num_task_instances at 0xffff7ed834d0>, \n    'get_run_data_interval': <bound method DAG.get_run_data_interval of <DAG: trial_dag>>, \n    'get_run_dates': <bound method DAG.get_run_dates of <DAG: trial_dag>>, \n    'get_serialized_fields': <bound method DAG.get_serialized_fields of <class '***.models.dag.DAG'>>, \n    'get_task': <bound method DAG.get_task of <DAG: trial_dag>>, \n    'get_task_instances': <bound method DAG.get_task_instances of <DAG: trial_dag>>, \n    'get_task_instances_before': <bound method DAG.get_task_instances_before of <DAG: trial_dag>>, \n    'get_template_env': <bound method DAG.get_template_env of <DAG: trial_dag>>, \n    'handle_callback': <bound method DAG.handle_callback of <DAG: trial_dag>>, \n    'has_dag_runs': <bound method DAG.has_dag_runs of <DAG: trial_dag>>, \n    'has_on_failure_callback': False, \n    'has_on_success_callback': False, \n    'has_task': <bound method DAG.has_task of <DAG: trial_dag>>, \n    'has_task_group': <bound method DAG.has_task_group of <DAG: trial_dag>>, \n    'infer_automated_data_interval': <bound method DAG.infer_automated_data_interval of <DAG: trial_dag>>, \n    'is_fixed_time_schedule': <bound method DAG.is_fixed_time_schedule of <DAG: trial_dag>>, \n    'is_paused': False, \n    'is_paused_upon_creation': None, \n    'is_subdag': False, \n    'iter_dagrun_infos_between': <bound method DAG.iter_dagrun_infos_between of <DAG: trial_dag>>, \n    'iter_invalid_owner_links': <bound method DAG.iter_invalid_owner_links of <DAG: trial_dag>>, \n    'jinja_environment_kwargs': None, \n    'last_loaded': datetime.datetime(2023, 8, 11, 12, 7, 12, 429629, tzinfo=Timezone('UTC')), \n    'latest_execution_date': datetime.datetime(2023, 8, 11, 12, 7, 10, 904583, tzinfo=Timezone('UTC')), \n    'leaves': [<Task(HelloOperator): task2>], \n    'log': <Logger ***.models.dag.DAG (INFO)>, \n    'logger': <bound method LoggingMixin.logger of <class '***.models.dag.DAG'>>, \n    'max_active_runs': 16, \n    'max_active_tasks': 16, \n    'next_dagrun_after_date': <bound method DAG.next_dagrun_after_date of <DAG: trial_dag>>, \n    'next_dagrun_info': <bound method DAG.next_dagrun_info of <DAG: trial_dag>>, \n    'normalize_schedule': <bound method DAG.normalize_schedule of <DAG: trial_dag>>, \n    'normalized_schedule_interval': None, \n    'on_failure_callback': None, \n    'on_success_callback': None, \n    'orientation': 'LR', \n    'owner': '***', \n    'owner_links': {}, \n    'param': <bound method DAG.param of <DAG: trial_dag>>, \n    'params': {}, \n    'parent_dag': None, \n    'partial': False, \n    'partial_subset': <bound method DAG.partial_subset of <DAG: trial_dag>>, \n    'pickle': <bound method DAG.pickle of <DAG: trial_dag>>, \n    'pickle_id': None, \n    'pickle_info': <bound method DAG.pickle_info of <DAG: trial_dag>>, \n    'previous_schedule': <bound method DAG.previous_schedule of <DAG: trial_dag>>, \n    'relative_fileloc': PosixPath('trial_dag.py'), \n    'render_template_as_native_obj': False, \n    'resolve_template_files': <bound method DAG.resolve_template_files of <DAG: trial_dag>>, \n    'roots': [<Task(BashOperator): task1>], \n    'run': <bound method DAG.run of <DAG: trial_dag>>, \n    'safe_dag_id': 'trial_dag', \n    'schedule_interval': '@once', \n    'set_dag_runs_state': <bound method DAG.set_dag_runs_state of <DAG: trial_dag>>, \n    'set_dependency': <bound method DAG.set_dependency of <DAG: trial_dag>>,\n    'set_edge_info': <bound method DAG.set_edge_info of <DAG: trial_dag>>, \n    'set_task_instance_state': <bound method DAG.set_task_instance_state of <DAG: trial_dag>>, \n    'sla_miss_callback': None, \n    'start_date': DateTime(2023, 8, 1, 0, 0, 0, tzinfo=Timezone('UTC')), \n    'sub_dag': <bound method DAG.sub_dag of <DAG: trial_dag>>, \n    'subdags': [], \n    'sync_to_db': <bound method DAG.sync_to_db of <DAG: trial_dag>>, \n    'tags': [], 'task': functools.partial(<***.decorators.TaskDecoratorCollection object at 0xffff7aafde10>, dag=<DAG: trial_dag>), \n    'task_count': 2, \n    'task_dict': {'task1': <Task(BashOperator): task1>, 'task2': <Task(HelloOperator): task2>}, \n    'task_group': <***.utils.task_group.TaskGroup object at 0xffff7be957d0>, \n    'task_group_dict': {}, \n    'task_ids': ['task1', 'task2'], \n    'tasks': [<Task(BashOperator): task1>, <Task(HelloOperator): task2>], \n    'template_searchpath': None, \n    'template_undefined': <class 'jinja2.runtime.StrictUndefined'>, \n    'test': <bound method DAG.test of <DAG: trial_dag>>, \n    'timetable': <***.timetables.simple.OnceTimetable object at 0xffff7bed0410>, \n    'timezone': Timezone('UTC'), \n    'topological_sort': <bound method DAG.topological_sort of <DAG: trial_dag>>, \n    'tree_view': <bound method DAG.tree_view of <DAG: trial_dag>>, \n    'user_defined_filters': None, \n    'user_defined_macros': None, \n    'validate': <bound method DAG.validate of <DAG: trial_dag>>, \n    'validate_schedule_and_params': <bound method DAG.validate_schedule_and_params of <DAG: trial_dag>>\n}\n\n\n# context[\"ti\"]\n{\n    'are_dependencies_met': <bound method TaskInstance.are_dependencies_met of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'are_dependents_done': <bound method TaskInstance.are_dependents_done of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'check_and_change_state_before_execution': <bound method TaskInstance.check_and_change_state_before_execution of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'clear_db_references': <bound method TaskInstance.clear_db_references of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'clear_next_method_args': <bound method TaskInstance.clear_next_method_args of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'clear_xcom_data': <bound method TaskInstance.clear_xcom_data of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'command_as_list': <bound method TaskInstance.command_as_list of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'current_state': <bound method TaskInstance.current_state of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'dag_id': 'trial_dag', \n    'dag_run': <DagRun trial_dag @ 2023-08-11 12:45:13.996317+00:00: manual__2023-08-11T12:45:13.996317+00:00, state:running, queued_at: 2023-08-11 12:45:14.009070+00:00. externally triggered: True>, \n    'dry_run': <bound method TaskInstance.dry_run of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'duration': None, \n    'email_alert': <bound method TaskInstance.email_alert of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'end_date': None, \n    'error': <bound method TaskInstance.error of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'execution_date': datetime.datetime(2023, 8, 11, 12, 45, 13, 996317, tzinfo=Timezone('UTC')), \n    'executor_config': {}, \n    'external_executor_id': None, \n    'filter_for_tis': <function TaskInstance.filter_for_tis at 0xffff7ed49050>, \n    'generate_command': <function TaskInstance.generate_command at 0xffff7ed3f200>, \n    'get_dagrun': <bound method TaskInstance.get_dagrun of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_email_subject_content': <bound method TaskInstance.get_email_subject_content of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_failed_dep_statuses': <bound method TaskInstance.get_failed_dep_statuses of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_num_running_task_instances': <bound method TaskInstance.get_num_running_task_instances of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_previous_dagrun': <bound method TaskInstance.get_previous_dagrun of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_previous_execution_date': <bound method TaskInstance.get_previous_execution_date of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_previous_start_date': <bound method TaskInstance.get_previous_start_date of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_previous_ti': <bound method TaskInstance.get_previous_ti of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_relevant_upstream_map_indexes': <bound method TaskInstance.get_relevant_upstream_map_indexes of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_rendered_k8s_spec': <bound method TaskInstance.get_rendered_k8s_spec of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_rendered_template_fields': <bound method TaskInstance.get_rendered_template_fields of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_template_context': <bound method TaskInstance.get_template_context of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_truncated_error_traceback': <function TaskInstance.get_truncated_error_traceback at 0xffff7ed46320>, \n    'handle_failure': <bound method TaskInstance.handle_failure of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'hostname': '7a02942b92a8', \n    'init_on_load': <bound method TaskInstance.init_on_load of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'init_run_context': <bound method TaskInstance.init_run_context of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'insert_mapping': <function TaskInstance.insert_mapping at 0xffff7ed37dd0>, \n    'is_eligible_to_retry': <bound method TaskInstance.is_eligible_to_retry of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'is_premature': False, \n    'is_trigger_log_context': False, \n    'job_id': '86', \n    'key': TaskInstanceKey(dag_id='trial_dag', task_id='task2', run_id='manual__2023-08-11T12:45:13.996317+00:00', try_number=1, map_index=-1), \n    'log': <Logger ***.task (INFO)>, \n    'log_url': 'http://localhost:8080/log?execution_date=2023-08-11T12%3A45%3A13.996317%2B00%3A00&task_id=task2&dag_id=trial_dag&map_index=-1', \n    'logger': <bound method LoggingMixin.logger of <class '***.models.taskinstance.TaskInstance'>>, \n    'map_index': -1, \n    'mark_success_url': 'http://localhost:8080/confirm?task_id=task2&dag_id=trial_dag&dag_run_id=manual__2023-08-11T12%3A45%3A13.996317%2B00%3A00&upstream=false&downstream=false&state=success', \n    'max_tries': 0, \n    'metadata': MetaData(), \n    'next_kwargs': None, \n    'next_method': None, \n    'next_retry_datetime': <bound method TaskInstance.next_retry_datetime of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'next_try_number': 2, \n    'operator': 'HelloOperator', \n    'overwrite_params_with_dag_run_conf': <bound method TaskInstance.overwrite_params_with_dag_run_conf of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'pid': 10659, \n    'pool': 'default_pool', \n    'pool_slots': 1, \n    'prev_attempted_tries': 1, \n    'previous_start_date_success': DateTime(2023, 8, 11, 12, 37, 0, 737891, tzinfo=Timezone('UTC')), \n    'previous_ti': None, \n    'previous_ti_success': <TaskInstance: trial_dag.task2 manual__2023-08-11T12:36:58.934316+00:00 [success]>, \n    'priority_weight': 1, \n    'queue': 'default', \n    'queued_by_job_id': 8, \n    'queued_dttm': datetime.datetime(2023, 8, 11, 12, 45, 15, 464793, tzinfo=Timezone('UTC')), \n    'raw': True, \n    'ready_for_retry': <bound method TaskInstance.ready_for_retry of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'refresh_from_db': <bound method TaskInstance.refresh_from_db of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'refresh_from_task': <bound method TaskInstance.refresh_from_task of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'registry': <sqlalchemy.orm.decl_api.registry object at 0xffff7ff9e650>, \n    'render_k8s_pod_yaml': <bound method TaskInstance.render_k8s_pod_yaml of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'render_templates': <bound method TaskInstance.render_templates of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'rendered_task_instance_fields': None, \n    'run': <bound method TaskInstance.run of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'run_as_user': None, \n    'run_id': 'manual__2023-08-11T12:45:13.996317+00:00', \n    'schedule_downstream_tasks': <bound method TaskInstance.schedule_downstream_tasks of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'set_duration': <bound method TaskInstance.set_duration of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'set_state': <bound method TaskInstance.set_state of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'start_date': datetime.datetime(2023, 8, 11, 12, 45, 15, 753572, tzinfo=Timezone('UTC')), \n    'state': 'running', \n    'task': <Task(HelloOperator): task2>, \n    'task_id': 'task2', \n    'test_mode': False, \n    'ti_selector_condition': <bound method TaskInstance.ti_selector_condition of <class '***.models.taskinstance.TaskInstance'>>, \n    'trigger_id': None, \n    'trigger_timeout': None, \n    'try_number': 1, \n    'unixname': '***', \n    'updated_at': datetime.datetime(2023, 8, 11, 12, 45, 15, 766080, tzinfo=Timezone('UTC')), \n    'xcom_pull': <bound method TaskInstance.xcom_pull of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'xcom_push': <bound method TaskInstance.xcom_push of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>\n}\n\n\n\n\n",
    "preview": "posts/2023-08-10-writing-airflow-dags/img/preview.png",
    "last_modified": "2023-08-20T12:31:06+00:00",
    "input_file": "writing-airflow-dags.knit.md"
  },
  {
    "path": "posts/2023-08-09-sample-youtube-channels-id-with-async-functions/",
    "title": "Sampling YouTube Channels' Id with Async Functions",
    "description": "A quick way to collect YouTube channels' id from YouTube home page asynchronously.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-09",
    "categories": [
      "python",
      "async",
      "web-scraping"
    ],
    "contents": "\n\nContents\nIntroduction\nAsyncio\nCoroutines\nTasks\nEvent loops\nSemaphore\n\nScraping YouTube asynchronously\n\nIntroduction\nWhen exchanging information between nodes (like making http request, or querying a database, etc.), the client side usually is required to wait for the response from the server. In synchronous programming, commands / statements are executed line by line, meaning that if a statement is required to wait for a particular amount of time, every other statements below cannot be executed because they are blocked by this long waiting statement to complete.\nFor example if we want to query two tables from two different databases and process them afterwords, in synchronous programming, we will need to wait for the first query to complete in order to start the second query. If the response time of the both queries are 10 seconds, we need to spend 20 seconds just for waiting. Asynchronous programming allows the processor to execute other tasks when a particular command is required to wait.\nThis article will first introduce the asyncio library in Python. Then we will use this method to sample channel ids from YouTube for later use.\nAsyncio\nCoroutines\nThe first thing in asynchronous programming is to create an async function. To do so in Python, we simply add the keyword async before the def keyword. Yet, you cannot simply execute an async function. When execute an async function, it always returns a coroutine object. A coroutine is an awaitable and therefore can be awaited from other coroutines. Notice that the await keyword can only be used in an async function.\nBelow shows the awaitability of synchronous (time.sleep) and asynchronous (foo) functions.\n\nimport time\nimport asyncio\n\nasync def foo1():\n    time.sleep(1)   # time.sleep is not an async function and cannot be awaited\n    print(\"Hello world!\")\n\ntype(foo1())\n\n# <stdin>:1: RuntimeWarning: coroutine 'foo1' was never awaited\n# RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n# <class 'coroutine'>\n\nasync def foo2():\n    await foo1()    # foo1 is an async function and can be awaited\n\nTo execute a coroutine object, asyncio.run is required.\n\nasyncio.run(foo1())\n\n# Hello world!\n\ntype(asyncio.run(foo1()))\n\n# Hello world!\n# <class 'NoneType'>\n\nTasks\nTasks are used to run coroutines in event loops. In other words, a coroutine is required to be wrapped into a task in order to pass to an event loop for execution. A task can be created via asyncio.create_task or loop.create_task methods. Whenever a task is created via these methods, they will execute whenever the running theread is available. Besides, if we do not await a task to be finished, the task will be dropped when the last line of the async function is executed.\n\nimport asyncio\n\nasync def foo1():\n    for i in range(10):\n        print(i)\n        await asyncio.sleep(0.5)\n\nasync def foo2():\n    await asyncio.sleep(2)\n    print(\"Hi from foo2\")\n\nasync def main():\n    task1 = asyncio.create_task(foo1())\n    task2 = asyncio.create_task(foo2())\n    \n    # The above tasks will not be executed before this line\n    # Because the running thread is not available before this\n    print(\"The thread is busy before this\")  \n    \n    await task2    # this task will be awaited. But not task1\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n# The thread is busy before this\n# 0\n# 1\n# 2\n# 3\n# Hi from foo2\n\nEvent loops\nAn event loop is an object to manage a list of tasks. It identifies if the running thread is available for execute other tasks. The method asyncio.run simply acquires an event loop, convert the coroutine into a task, execute it and closing the threadpool. Below shows the same example as above but better illustrate the event loop.\n\nimport asyncio\n\nasync def foo1():\n    for i in range(10):\n        print(i)\n        await asyncio.sleep(0.5)\n\nasync def foo2():\n    await asyncio.sleep(2)\n    print(\"Hi from foo2\")\n\nasync def main():\n    task1 = asyncio.create_task(foo1())\n    task2 = asyncio.create_task(foo2())\n\n    print(\"The thread is busy before this\")\n    \n    await task2\n    print(type(asyncio.get_event_loop()))\n\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()  # Acquire an event loop\n    loop.run_until_complete(main())  # Execute the tasks\n    loop.close()                     # Close the loop\n\nSemaphore\nBy default, an event loop will put all the tasks to execute whenever there are resources. But sometimes we want to limit the number of concurrent tasks. In this case, asyncio.Semaphore may come in handy.\n\nimport asyncio\n\nasync def async_func(task_no, sem):\n    async with sem:\n        print(f'{task_no} :Hello ...')\n        await asyncio.sleep(1)\n        print(f'{task_no}... world!')\n\nasync def main(sem):\n    tasks = [\n        asyncio.create_task(async_func(x, sem)) for x in [\"t1\", \"t2\", \"t3\"]\n    ]\n    await asyncio.gather(*tasks)\n\nif __name__ == \"__main__\":\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    sem = asyncio.Semaphore(2)\n    result = asyncio.run(main(sem))\n    loop.close()\n\n# t1 :Hello ...\n# t2 :Hello ...\n# t1... world!\n# t2... world!\n# t3 :Hello ...\n# t3... world!\n\nScraping YouTube asynchronously\nIn this section, the code snippet below sample channel ids from YouTube main page using async functions. I am using httpx instead of requests because the later library does not provide async http client. I also set the cookie \"CONSENT\": \"YES+yt.463627267.en-GB+FX+553\" to consent YouTube using cookies tracking. Yet, I remove all cookies except this one every time I make the request to avoid YouTube gives me the same channels. Lastly, I use regex to extract all the channel ids from the html.\nThe whole script only used 7.7 seconds to complete 100 requests, which is much faster to use the synchronous client.\n\nimport re\nimport httpx\nimport asyncio\nfrom asyncio import Semaphore, create_task, gather\nfrom httpx import AsyncClient\nfrom time import time\nfrom itertools import chain\n\n\nasync def async_request_youtube(sem: Semaphore, client: AsyncClient):\n    async with sem:\n        client.cookies = {\"CONSENT\": \"YES+yt.463627267.en-GB+FX+553\"} \n        client.headers = {\n            'accept': '*/*', \n            'accept-encoding': 'gzip, deflate', \n            'connection': 'keep-alive', \n            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.5400.117 Safari/537.36'\n        }\n\n        response = await client.get(\"https://www.youtube.com\")\n        return response\n\nasync def sample_youtube_channels(n: int, sem: Semaphore):\n    async with httpx.AsyncClient() as client:\n        client.timeout = 10\n        tasks = [\n            create_task(async_request_youtube(sem, client)) for i in range(n)\n        ]\n        result = await gather(*tasks)\n    \n    return result\n\ndef request_youtube(n: int, concurrency: int = 50):\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    sem = asyncio.Semaphore(concurrency)\n    result = asyncio.run(sample_youtube_channels(n, sem))\n    loop.close()\n    return result\n\nif __name__ == \"__main__\":\n    start_ts = time()\n    result = request_youtube(100, 50)\n    channel_id_re = re.compile('\"(UC[A-z0-9_-]{22})\"')\n    channel_ids = [\n        channel_id_re.findall(res.text) for res in result\n    ]\n    channel_ids = list(set(list(chain(*channel_ids))))\n    with open(\"./youtube_channel_ids.txt\", \"w\") as f:\n        for cid in channel_ids:\n            _ = f.write(f\"{cid}\\n\")\n    end_ts = time()\n    print(f\"Time used: {end_ts - start_ts} seconds\")\n\n# Time used: 7.707166910171509 seconds\n\n\n\n\n",
    "preview": "posts/2023-08-09-sample-youtube-channels-id-with-async-functions/img/preview.png",
    "last_modified": "2023-08-20T12:32:00+00:00",
    "input_file": "sample-youtube-channels-id-with-async-functions.knit.md"
  },
  {
    "path": "posts/2023-08-08-recursive-programming-querying-nested-json-in-python/",
    "title": "Recursive Programming - Querying nested JSON in Python",
    "description": "Querying nested JSON in Python with recursive programming.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-08",
    "categories": [
      "python",
      "recursive-programming"
    ],
    "contents": "\n\nContents\nIntroduction\nRecursive function\nQuerying nested JSON dictionary\nExamples\nExample 1\nExample 2\nExample 3\n\n\nIntroduction\nAs we are requesting REST API heavily nowadays, we need to deal with JSON frequently. JSON objects can be transfer into Python dictionaries very easily by the json library. Yet, it is quite tedious to query nested JSON if it contains many layers.\n\nd = {\n    \"layer1_item1\": {\"layer2_item1\": {\"layer3_item1\": \"some_info\", \"layer3_item2\": \"another_info\"}},\n    \"layer1_item2\": {\"layer2_item1\": {\"layer3_item1\": \"some_info\", \"layer3_item2\": \"another_info\"}}\n}\n\nFor example if we want to query layer3_item2 from the above JSON,\n\nd.get(\"layer1_item1\").get(\"layer2_item1\").get(\"layer3_item2\")\n'another_info'\n\nIt involves so much brackets and quotes. I have written a recursive function to query these nested JSON dictionary in Python. But we will first take a look what a recursive functions is.\nRecursive function\nA recursive function means the function execute itself within its own definition. A simple but endless definition is shown below.\n\ndef foo():\n    foo()\n\nThe function foo calls itself within its own definition. It simply creates a loop (but an endless one in the above example).\nWe now take a look on a practical factorial function. Recall the factorial of \\(n\\) is given by\n\\[n! = n \\cdot (n - 1) \\cdot (n - 2) \\cdots 3 \\cdot 2 \\cdot 1.\\]\nIn Python, we can illustrate this by\n\ndef factorial(n):\n    if n == 1:\n        return n\n    return n * factorial(n - 1)\n\n\nfactorial(5)\n120\n\nQuerying nested JSON dictionary\nThe following function can query nested dictonary with syntax like key1.key2.key3 instead of multiple get, brackets and quotes. See some examples in the coming section.\n\nfrom typing import Any, Optional\n\ndef pjq(\n    json_dict: 'list | dict', \n    query: 'list[str] | str', \n    default: Optional[Any] = None, \n    sep: str = \".\", \n    idx_sep: str = \",\", \n    trim: bool = True, \n    prev_q: Optional[str] = None\n) -> Any:\n    query = query.split(sep) if isinstance(query, str) else query\n    # Cannot pop query index otherwise affecting the for loop in list\n    q: str = query[0]\n    query: list[str] = query[1:]\n    if json_dict == default:\n        # If `default` is set to a list of dict, it cannot go through this\n        return default\n\n    elif isinstance(json_dict, dict):\n        json_dict = json_dict.get(q, default)\n        \n        if query:\n            return pjq(json_dict, query, default, sep, idx_sep, trim=trim, prev_q=q)\n        return json_dict\n\n    elif isinstance(json_dict, list):\n        if q:\n            try:\n                idx: list[int] = [int(i) for i in q.split(idx_sep)]\n                json_dict = [json_dict[i] for i in idx]\n            except Exception:\n                return default\n        if query:\n            json_dict = [pjq(jd, query, default, sep, idx_sep, trim=trim, prev_q=q) for jd in json_dict]\n\n        if trim:\n            json_dict = json_dict[0] if len(json_dict) == 1 else json_dict\n\n        return json_dict\n\n    else:\n        return None \n\nExamples\nExample 1\n\nd = {\n    \"a\": {\n        \"b\": {\"b1\": 1, \"b2\": 2},\n        \"c\": {\"c1\": 3, \"c2\": 4}\n    }\n}\n\n\npjq(d, \"a.b.b2\")\n2\n\nExample 2\n\nd = {\n    \"a\": [\n        {\"b1\": 1, \"b2\": 2},\n        {\"b1\": 3, \"b2\": 4},\n        {\"b1\": 5, \"b2\": 6}\n    ]\n}\n\n\npjq(d, \"a.1,2.b2\")\n[4, 6]\n\n\npjq(d, \"a..b2\")\n[2, 4, 6]\n\nExample 3\n\nd = [\n    {\"a\": {\"b\": {\"x\": 1}, \"c\": {\"y\": 3}}},\n    {\"a\": {\"b\": {\"x\": 2}, \"c\": {\"y\": 4}}}\n]\n\n\npjq(d, \".a.c.y\")\n[3, 4]\n\n\n\n\n",
    "preview": "posts/2023-08-08-recursive-programming-querying-nested-json-in-python/img/preview.png",
    "last_modified": "2023-08-20T12:31:44+00:00",
    "input_file": "recursive-programming-querying-nested-json-in-python.knit.md"
  },
  {
    "path": "posts/2023-08-05-introduction-to-docker-with-airflow-and-postgres-stack/",
    "title": "Introduction to Docker with Airflow and Postgres stack",
    "description": "Deploying Docker stacks of Airflow and Postgres database with docker-compose.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-05",
    "categories": [
      "docker",
      "airflow",
      "postgres"
    ],
    "contents": "\n\nContents\nIntroduction\nDocker Compose YAML File\nService Name\nImage Name\nPorts\nEnvironment\nVolumes\nAnchor\nStarting containers\n\nAirflow and Postgres stack\nObtaining docker compose YAML template\nCustomise the YAML file (optional)\nCreate initial databases and tables (optional)\nStart the stack\n\n\nIntroduction\nDocker is a software hosting containers. A container is an isolated place on top of the running operating system (OS) where an application can run without affecting other applications on the OS. Developers have been creating images to run on Docker. An image is a template of an application, like Postgres, that are ready to run in a container. It allows users to take the image and run it in seconds in a container and save tremendous times to install the application from scratch.\nIn this document, we will install a stack of containers to run Apache Airflow, a workflow management tool, and Postgres database. In later articles, we will be writing ETL workflows on Airflow using this stack. You can find the code source from my Github repo. If you have not installed Docker and Docker Compose, please follow the documentation below:\nDocker: https://www.docker.com/products/docker-desktop/\nDocker Compose: https://docs.docker.com/compose/install/\nWe will first describe how to tell Docker what images we want to use in a docker-compose.yaml specification file. Then we will setup Airflow and Postgres as an example.\nDocker Compose YAML File\nIn this section, we will briefly introduce how to specify what images to use in a docker-compose.yaml. We first take a look on a sample of the YAML file for creating a Postgres container.\n\nversion: '3.8'           # Version of the specification file \n\nservices: \n  mypostgres:            # Name of the local container\n    image: postgres:13   # image_name:version\n    ports: \n      - \"5431:5432\"      # [local port]:[cotainer port]\n    environment:         # The key-value pairs depends on image\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}\n      POSTGRES_DB: airflow\n    volumes:\n      - /path/in/local:/path/in/container\n      - /another/path:/another/path\n    \n  myairflow:             # Another container\n    ...\n\nA full documentation on the sections can be found here. Below we will only introduce some of them.\nService Name\nFrom the above example, we have specified 2 containers, one is mypostgres and the other is myairflow. These are the local names of the containers, where you can see them from\n\ndocker container ls\n\nImage Name\nUnder the block mypostgres, we have specified to use the image postgres:13. We can find published images from https://hub.docker.com/ and put the image’s name in docker-compose.yaml. We can also specify the version of the image. In this example, we are using version 13.\n\n\n\nPorts\nWe have also put 5431:5432 as the port configuration, meaning that we can use the local port 5431 to connect to the port 5432 of the container. In other words, we can connect the database via the following command\n\npsql postgres://username:password@localhost:5431\n\nEnvironment\nThe environment section stores the environment variables to be passed to the container. In this example, we are passing POSTGRES_USER, POSTGRES_PASSWORD and POSTGRES_DB to the container. The image will pick up these environment variables to build the Postgres database in the container.\nWhen we are passing sensitive information like the password in this example, we can create and store key-value pairs in .env file besides the docker-compose.yaml. Then we can use the keys defind in the .env file in the specification file. The expression ${POSTGRES_PASSWORD:-password} means it will look for the key-value pair inside .env and password will be use instead if the key-value pair cannot be found. Below shows and example of the .env file.\n\nPOSTGRES_PASSWORD=\"AnotherPassword\"\n\nVolumes\nWe can share files and directories under the volumes session with the below format.\n\n/path_to_local_file:/path_to_container_file\n\nAnchor\nWhen working with a stack of containers, it is common to repeated use some configuration. For example to put the same environment variables into different containers. From the example below, we can both the services airflow-scheduler and airflow-website share the same set of configuration on image and environment from the airflow-common configuration.\n\nx-airflow-common:\n  &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.6.3}\n  environment:\n    AIRFLOW__CORE__EXECUTOR: LocalExecutor\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n  ...\n  \nservices:\n  airflow-scheduler:\n    <<: *airflow-common\n    command: scheduler\n    ...\n    \n  airflow-webserver:\n    <<: *airflow-common\n    command: webserver\n    ...\n\nStarting containers\nWe can start a single service from the stack with the following command:\n\ndocker-compose up <service-name>\n\nOr start the whole stack with the following command:\n\ndocker-compose up --build -d\n\nThe option -d lets the stack to run in background. We can check the status of each container with the following command:\n\ndocker container ls\n\n# CONTAINER ID   IMAGE                  ...   PORTS                    NAMES\n# 4c26970df34a   postgres:13            ...   0.0.0.0:5431->5432/tcp   postgres-mypostgres-1\n# 05365378f0f0   apache/airflow:2.6.3   ...   8080/tcp                 yt-docker-stack-airflow-triggerer-1\n# cf43be7e9755   apache/airflow:2.6.3   ...   8080/tcp                 yt-docker-stack-airflow-scheduler-1\n# 12fdbf84054e   apache/airflow:2.6.3   ...   0.0.0.0:8081->8080/tcp   yt-docker-stack-airflow-webserver-1\n# 73cfeea83789   postgres:13            ...   0.0.0.0:5432->5432/tcp   yt-docker-stack-postgres-1\n\nWe can also go into the bash environment of a container with the following command:\n\ndocker exec -it <container_id | container_name> bash\n\nFinally, if we want to close the whole stack of containers, we can use the following command:\n\ndocker-compose down -v\n\nThe option -v removes all the volumes used by the stack.\nAirflow and Postgres stack\nIn this section, we will show an example to build a stack of containers with Apache Airflow and Postgres database.\nObtaining docker compose YAML template\nWe need to first obtain a docker-compose.yaml template file from here. I was using Airflow version 2.6.3 at the moment. Hence the YAML file is\nhttps://airflow.apache.org/docs/apache-airflow/2.6.3/docker-compose.yaml\nCustomise the YAML file (optional)\nIn this example, I have customised the YAML file to fit my purposes. You can find my specification file from Github.\nI will be using the LocalExecutor instead of the CeleryExecutor. Hence I have changed the environment variable AIRFLOW__CORE__EXECUTOR from CeleryyExecutor to LocalExecutor under the airflow-common section, and removed the following items:\n\n# THE FOLLOWING SECTIONS ARE REMOVED\nx-airflow-common:\n  environment:\n    AIRFLOW__CELERY__RESULT_BACKEND: ...\n    AIRFLOW__CELERY__BROKER_URL: ...\n  depends_on:\n    redis:\n      ...\n\nservices:\n  redis:\n    ...\n\n  airflow-worker:\n    ...\n  \n  flower:\n    ...\n\nI have also changed / added the followings:\n\n# THE FOLLOWING VALUES ARE CHANGED / ADDED\nx-airflow-common:\n  environment:\n    AIRFLOW__CORE__EXECUTOR: LocalExecutor\n    AIRFLOW_INPUT_DIR: '/opt/airflow/dag-inputs'\n    POSTGRES_USER: airflow\n    POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n    GAPI_KEY: ${GAPI_KEY}\n    \nservices:\n  postgres:\n    ports:\n      - \"5432:5432\"\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: airflow\n    volumes:\n      - postgres-db-volume:/var/lib/postgresql/data\n      - ./init/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql\n      - ./init/postgres:/mnt/sources/init\n  \n  airflow-init:\n    environment:\n      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_PASSWORD}\n\nCreate initial databases and tables (optional)\nThe Postgres docker image will execute the files in /docker-entrypoint-initdb.d during initialisation. One can put some queries in this folder for initial execution, which is why I share some volumes in the above section.\n\nCREATE DATABASE youtube;\n\\c youtube;\n\\i /mnt/sources/init/enum_iso3166.sql;\nCREATE TABLE channel (\n    uuid UUID default gen_random_uuid() NOT NULL,\n    etag CHAR(27) NOT NULL,\n    id CHAR(24) NOT NULL,\n    title VARCHAR(100) NOT NULL,\n    \"description\" VARCHAR(5000),\n    custom_url VARCHAR(31),\n    published_at Timestamp,\n    country country_alpha2,\n    uploads CHAR(24),\n    view_count INTEGER,\n    subscriber INTEGER,\n    video_count INTEGER,\n    topic_category TEXT[],\n    updated_at TIMESTAMP NOT NULL default CURRENT_TIMESTAMP\n);\n\nStart the stack\nAs described in previous sections, I now start the stack with the following cammands:\n\ndocker-compose up airflow-init\n\ndocker-compose up --build -d\n\nAnd you can find Airflow is running on http://localhost:8080 or via docker container ls.\n\n\n\n\n\n\n",
    "preview": "posts/2023-08-05-introduction-to-docker-with-airflow-and-postgres-stack/img/preview.png",
    "last_modified": "2023-08-20T12:31:27+00:00",
    "input_file": "introduction-to-docker-with-airflow-and-postgres-stack.knit.md"
  },
  {
    "path": "posts/2023-08-03-github-actions-with-example/",
    "title": "Github Actions with Example",
    "description": "An introduction to Github Actions with an example to write a post whenever a new blog post is merged to the main branch.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-03",
    "categories": [
      "continuous-delivery",
      "cicd",
      "github-actions",
      "github",
      "linkedin-api",
      "oauth2"
    ],
    "contents": "\n\nContents\nIntroduction\nGithub Actions\nWorkflows\nEvents\nJobs\nActions\nRunners\nSecrets\nGithub Context\n\nLinkedIn API\nOAuth2\nCalling API\nIdentify User Id\nWrite Post\n\n\nAuto Posting Workflow\nThe Workflow\nThe Python Script\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\nIn modern software development, an engineer’s job does not end when a product is developed. Numerous times are spent on testing and deploying the product, no matter if the product is a website or a programming library or anything. Usually these tasks are repetitive and boring because these products are required to be maintained and updated. The same testing and deploying process will need to be rerun again throughout the life-cycle of the product.\nThe same problem happens on data scientists and machine learning engineers as well, where the models they have developed are also required to be tested and deployed (and updated and tested and deployed again and again). The concept of continuous integration and delivery came to automate these repetitive tasks and saves our precious time.\nThis article describes these concepts through an example – write a LinkedIn post whenever a new blog post is created in this blog. We will first briefly go through what Github Actions is, then we will talk about how to write a post on LinkedIn through its API. Finally we will create a workflow to check if there is a new blog post and write a LinkedIn post if there is.\nGithub Actions\nGithub Actions is a platform for continuous integration / continuous delivery (CI/CD). One can write workflows to automate build, testing, and deployment pipelines. Each workflow is triggered by one or more events and can be run by different runners. We will describe these concepts more below.\nEach workflow must be defined in the folder of .github/workflows in a repo and it must be specified in a YAML file like below. We will go through each section of the file.\n\n\n# Workflow Name\nname: Release Process\n\non:\n  # Events\n  push:                                   # One event\n    branches:\n      - main\n\n  workflow_run:                           # Another event\n    workflows: [pages-build-deployment]\n    types: \n      - completed\n\njobs:\n  # Job\n  generate-release:                 # Job id\n    name: Create GitHub Release     # Job name\n    runs-on: ubuntu-latest          # Runner\n    steps:\n    - name: Checkout Repository     # Step1\n      uses: actions/checkout@v2     # Actions\n      \n    - name: Run release code        # Step2\n      run: |\n        cd /target/directory\n        ./run-release-code\n  \n  # Another Job\n  another-job:                      # Job id\n    name: Another Job               # Job name\n    needs: [generate-release]       # Requires the job to complete successfully\n    runs-on: ubuntu-latest          # Runner\n    steps:\n    - name: Checkout Repository     # Step1\n      uses: actions/checkout@v2     # Actions\n      \n    - name: do other stuffs         # Step2\n      run: echo $CUSTOM_VAR\n      env: \n        CUSTOM_VAR: \"${{ secrets.CUSTOM_VAR }}\" # Secret value\n\n\nWorkflows\nThe entire YAML file specified in this code chunk is a workflow. There can be multiple workflows in different YAML files stored inside .github/workflows directory. Each workflow can be triggered by one or more events, or they can be triggered manually, or at a defined schedule. Each workflow can also contains one or more jobs.\nEvents\nAn event is an activity within the repository. For example, an event can be a pull / push request. It can also be the completion of another workflow or scheduled by cron syntax.\nThe above workflow will be triggered whenever one of the two specified events occurs. These two events are\nEvery time the main branch is pushed or merged from another branch, this workflow will be started.\nWhenever another workflow pages-build-deployment is completed, this workflow will be started.\nJobs\nA job is a series of steps that will be executed on the same runner. Each step is either a shell script or an action. The steps will be executed in order and dependent on each other. By default, each job will be run by a different runner and concurrently. One can specify the dependency of jobs by the key needs. The above example shows an implementation.\nAlso, one can also specify a strategy matrix to repeat the same job for different conditions. For example, the following job will be executed 6 times, namely\n{node-version: 10, os: ubuntu-22.04}\n{node-version: 10, os: ubuntu-20.04}\n{node-version: 12, os: ubuntu-22.04}\n{node-version: 12, os: ubuntu-20.04}\n{node-version: 14, os: ubuntu-22.04}\n{node-version: 14, os: ubuntu-20.04}\n\njobs:\n  example_matrix:\n    strategy:\n      matrix:\n        os: [ubuntu-22.04, ubuntu-20.04]\n        version: [10, 12, 14]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/setup-node@v3\n        with:\n          node-version: ${{ matrix.version }}\n\nActions\nActions are custom applications for GitHub Actions that perform complex but repetitive tasks. You can write an action from scratch or use an existing action available from the GitHub Marketplace in your workflow.\nRunners\nA runner is an OS on a virtual machine or container to execute a specific job. GitHub provides Ubuntu Linux, Microsoft Windows, and macOS runners to run the workflows. One can also host their own machine as runner.\nSecrets\nFor each step or job, one can specify an env session to define environment variables. But if we are dealing with credentials, this might not be a good choice. One can go to Settings of the repository, under Security, click Secrets and variables, then click Actions. Inside the page, one can define secrets for the repository and can access them within the env session inside a workflow as shown in the example.\nGithub Context\nContexts are a way to access information about workflow runs, variables, runner environments, jobs, and steps. For example the name of the working branch, the working directory of Github Actions, etc. The keyword secrets in the above section is also a context. See more from this page.\nLinkedIn API\nLinkedIn offers various API products for consumers to do various of things. One of which is to write posts on behalf of the users (see this documentation). To do that, we need to\nCreate a company on LinkedIn\nCreate an application on behalf of the company\nAuthenticate yourself and authorise the application to write posts on behalf of you\nThe process is similar to my previous blog post about OAuth2 for Google APIs. I will briefly describe the process here.\nOAuth2\nWe will first create a company on LinkedIn and the application.\nGo to https://developer.linkedin.com/ and click Create App (and login to your LinkedIn account)\nEnter the name of the application\nClick Create a new LinkedIn Page if you do not have a company on LinkedIn\nSelect Company\nEnter the name of the company, select the industry, company size, company type. Check the terms and click Create page\nGo back to the developer page and select the company just created\nUpload a logo for the application\nCheck the Legal agreement and click Create app\nClick Verify and follow the instruction\nClick Products, click Request access for both Share on LinkedIn and Sign in with LinkedIn\nClick Auth and copy the Client ID and Client Secret\nUnder OAuth 2.0 settings, enter the authorised redirect url\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we have the client_id, client_secret and redirect_uri ready, we can now authenticate ourselves and authorise the application. The following script will generate a url to login to your LinkedIn account. Then it will generate the access_token.\n\nimport os\nfrom urllib.parse import urlencode, urlparse\nfrom http.server import HTTPServer, BaseHTTPRequestHandler\nimport json\nimport requests\nimport webbrowser\n\nclient_id = os.getenv(\"CLIENT_ID\")\nclient_secret = os.getenv(\"CLIENT_SECRET\")\nredirect_uri = os.getenv(\"REDIRECT_URI\")\nscope = \"r_liteprofile w_member_social openid profile email\"\n\ndef parse_query(path):\n    parsed_url = urlparse(path)\n    query = parsed_url.query.split(\"&\")\n    query = [x.split(\"=\") for x in query]\n    query = {x[0]: x[1] for x in query}\n    return query\n\ndef auth_code(code, client_id, client_secret, redirect_uri):\n    params = {\n        \"grant_type\": \"authorization_code\",\n        \"code\": code,\n        \"redirect_uri\": redirect_uri,\n        \"client_id\": client_id,\n        \"client_secret\": client_secret\n    }\n    headers = {\n        \"content-type\": \"application/x-www-form-urlencoded\",\n        \"content-length\": \"0\"\n    }\n    url = \"https://www.linkedin.com/oauth/v2/accessToken\"\n    response = requests.post(url, params=params, headers=headers)\n    response.raise_for_status()\n    content = response.json()\n    return content\n\nclass NeuralHTTP(BaseHTTPRequestHandler):\n    def do_GET(self):\n        path = self.path\n        query = parse_query(path)\n\n        code = query.get(\"code\")\n        if code:\n            status_code = 200\n            content = auth_code(\n                code=query.get(\"code\"),\n                client_id=client_id,\n                client_secret=client_secret,\n                redirect_uri=redirect_uri\n            )\n            print(json.dumps(content, indent=4))\n        else:\n            status_code = 400\n            content = {\n                \"error\": \"code not found\"\n            }\n\n        self.send_response(status_code)\n        self.send_header(\"Content-Type\", \"application/json\")\n        self.end_headers()\n        self.wfile.write(bytes(json.dumps(content, indent=4), \"utf-8\"))\n    \n    def log_message(self, format, *args):\n        \"\"\"Silence log message. Can be ignored.\"\"\"\n        return\n\nif __name__ == \"__main__\":\n    with HTTPServer((\"127.0.0.1\", 8088), NeuralHTTP) as server:\n        auth_url = \"https://www.linkedin.com/oauth/v2/authorization\"\n        params = {\n            \"client_id\": client_id,\n            \"response_type\": \"code\",\n            \"redirect_uri\": redirect_uri,\n            \"scope\": scope,\n        }\n\n        url = f\"{auth_url}?{urlencode(params)}\"\n        webbrowser.open(url)\n        server.handle_request()\n\n\n# {\n#     \"access_token\": \"...\",\n#     \"expires_in\": 5183999,\n#     \"scope\": \"email,openid,profile,r_liteprofile,w_member_social\",\n#     \"token_type\": \"Bearer\",\n#     \"id_token\": \"...\"\n# }\n\nCalling API\nIdentify User Id\nTo write a post on LinkedIn, We need to first identify the author’s user_id. A GET request to https://api.linkedin.com/v2/userinfo with the access_token obtained from the above are needed.\n\nimport os\nimport requests \nimport json\n\nurl = \"https://api.linkedin.com/v2/userinfo\"\ntoken = os.getenv(\"LINKEDIN_ACCESS_TOKEN\")\n\nheaders = {\"Authorization\": f\"Bearer {token}\"}\n\nresponse = requests.get(url, headers=headers)\nresponse.raise_for_status()\ncontent = response.json()\nprint(json.dumps(content, indent=4))\n\n\n{\n    \"sub\": \"....\",\n    \"email_verified\": true,\n    \"name\": \"Wilson Yip\",\n    \"locale\": {\n        \"country\": \"US\",\n        \"language\": \"en\"\n    },\n    \"given_name\": \"Wilson\",\n    \"family_name\": \"Yip\",\n    \"email\": \"wilsonyip@elitemail.org\",\n    \"picture\": \"https://media.licdn.com/dms/image/C4E03AQGo1BKbUYmyBA/profile-displayphoto-shrink_100_100/0/1646639382257?e=1696464000&v=beta&t=6lhHrDK3vx6GOC01wIKkfVYAmCiSWoZtc8XpE0JoUmM\"\n}\n\nThe user_id is stored in the sub value.\nWrite Post\nWe will be calling the Share in LinkedIn endpoint to write a post in LinkedIn along with the specific request body to attach an article to the post. The following scripts shows an example.\n\nimport os\nimport requests \n\ndef build_post_body(\n    user_id, \n    post_content, \n    media_title, \n    media_description, \n    article_url\n):\n    body = {\n        \"author\": f\"urn:li:person:{user_id}\",\n        \"lifecycleState\": \"PUBLISHED\",\n        \"specificContent\": {\n            \"com.linkedin.ugc.ShareContent\": {\n            \"shareCommentary\": {\n                    \"text\": post_content\n                },\n                \"shareMediaCategory\": \"ARTICLE\",\n                \"media\": [\n                    {\n                        \"status\": \"READY\",\n                        \"description\": {\n                            \"text\": media_description\n                        },\n                        \"originalUrl\": article_url,\n                        \"title\": {\n                            \"text\": media_title\n                        }\n                    }\n                ]\n            }\n        },\n        \"visibility\": {\n            \"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"\n        }\n    }\n    return body\n\nif __name__ == \"__main__\":\n    linkedin_user_id = os.getenv(\"LINKEDIN_USER_ID\")    # user_id \n    linkedin_token = os.getenv(\"LINKEDIN_TOKEN\")        # access_token\n    linkedin_post_endpoint = \"https://api.linkedin.com/v2/ugcPosts\"\n\n    headers = {\n        \"X-Restli-Protocol-Version\": \"2.0.0\",\n        \"Authorization\": \"Bearer \" + linkedin_token \n    }\n\n    body = build_post_body(\n        user_id=linkedin_user_id,\n        post_content=\"Content of the LinkedIn post\",\n        media_title=\"The title of the article\",\n        media_description=\"The description of the article\",\n        article_url=\"https://www.link-to-article.com/article\"\n    )\n\n    response = requests.post(\n        url=linkedin_post_endpoint, \n        json=body, \n        headers=headers\n    )\n\nAuto Posting Workflow\nA workflow is created to write a post on LinkedIn whenever there is a new article merged to the main branch of a repository. The workflow is triggered every time after completion of the pages-build-deployment workflow, which is the workflow to build the website. Yet, there is a problem:\n\nWe need to keep tract which article was posted to LinkedIn already in order to define which article is new.\n\nFor simplicity, I have created a Google Sheet to store the article paths and the corresponding LinkedIn post_id. If an article’s path does not appear in the table, that is the new article and will further trigger the scripts.\nThe workflow is quite simple. It just runs a Python file. The Python file will check if there are any new article path, write a LinkedIn post if there is one, and update the log file.\nThe Workflow\n\nname: create-linkedin-post\n\non:\n  workflow_run:\n    workflows: [pages-build-deployment]\n    types: \n      - completed\n\njobs:\n  on-success:\n    runs-on: ubuntu-latest\n    if: ${{ github.event.workflow_run.conclusion == 'success' }}\n\n    steps:\n      - name: Chekcout\n        uses: actions/checkout@v3\n      \n      - name: Install python dependencies\n        run: pip install pyyaml\n      \n      - name: Wait for some seconds\n        run: sleep 30\n      \n      - name: Create Linkedin Post\n        run: python ./tools/cd/linkedin_post.py\n        env: \n          LINKEDIN_USER_ID: ${{ secrets.LINKEDIN_USER_ID }}\n          LINKEDIN_TOKEN: ${{ secrets.LINKEDIN_TOKEN }}\n          GCP_CLIENT_EMAIL: ${{ secrets.GCP_CLIENT_EMAIL }}\n          GCP_PRIVATE_KEY_ID: ${{ secrets.GCP_PRIVATE_KEY_ID }}\n          GCP_PRIVATE_KEY: ${{ secrets.GCP_PRIVATE_KEY }}\n          LINKEDIN_POSTS_LOG_SSID: ${{ secrets.LINKEDIN_POSTS_LOG_SSID }}\n          LINKEDIN_POSTS_LOG_RANGE: ${{ secrets.LINKEDIN_POSTS_LOG_RANGE }}\n  on-failure:\n    runs-on: ubuntu-latest\n    if: ${{ github.event.workflow_run.conclusion == 'failure' }}\n    steps: \n      - run: echo \"Fail to write LinkedIn Post.\"\n\nThe Python Script\n\n#!/usr/bin/python\n\nimport os\nimport requests \nimport json \nfrom time import time\nimport jwt\nimport yaml\n\ndef build_post_body(\n    user_id, \n    post_content, \n    media_title, \n    media_description, \n    article_url\n):\n    preview_url = f\"{article_url}/img/preview.png\".replace(\n        \"//img/\", \"/img/\"\n    )\n    body = {\n        \"author\": f\"urn:li:person:{user_id}\",\n        \"lifecycleState\": \"PUBLISHED\",\n        \"specificContent\": {\n            \"com.linkedin.ugc.ShareContent\": {\n            \"shareCommentary\": {\n                    \"text\": post_content\n                },\n                \"shareMediaCategory\": \"ARTICLE\",\n                \"media\": [\n                    {\n                        \"status\": \"READY\",\n                        \"description\": {\n                            \"text\": media_description\n                        },\n                        \"originalUrl\": article_url,\n                        \"title\": {\n                            \"text\": media_title\n                        },\n                        \"thumbnails\": [\n                            {\n                                \"url\": preview_url\n                            }\n                        ]\n                    }\n                ]\n            }\n        },\n        \"visibility\": {\n            \"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"\n        }\n    }\n    return body\n\ndef find_latest_missing_post(page_posts, linkedin_posts):\n    page_post_paths = [x.get(\"path\") for x in page_posts]\n    linkedin_post_paths = [x.get(\"path\") for x in linkedin_posts]\n    missing_idx = [\n        i for i, x in enumerate(page_post_paths) if x not in linkedin_post_paths\n    ]\n    \n    if missing_idx:\n        missing_paths = [page_post_paths[i] for i in missing_idx]\n        missing_post_dates = [page_posts[i].get(\"date\") for i in missing_idx]\n        latest_missing_post = missing_paths[missing_post_dates.index(max(missing_post_dates))]\n        latest_missing_post = page_posts[page_post_paths.index(latest_missing_post)]\n    else:\n        latest_missing_post = None\n\n    return latest_missing_post\n\ndef read_rmd_yml(path):\n    with open(path, \"r\") as f:\n        rmd_yml = f.readlines()\n    \n    yml_idx = [i for i, x in enumerate(rmd_yml) if x == \"---\\n\"]\n    return yaml.safe_load(\"\".join(rmd_yml[(yml_idx[0]+1):(yml_idx[1])]))\n\ndef auth_gapi_token(client_email, private_key_id, private_key):\n    payload: dict = {\n        \"iss\": client_email,\n        \"scope\": \"https://www.googleapis.com/auth/drive\",\n        \"aud\": \"https://oauth2.googleapis.com/token\",\n        \"iat\": int(time()),\n        \"exp\": int(time() + 3599)\n    }\n    headers: dict[str, str] = {'kid': private_key_id}\n\n    signed_jwt: bytes = jwt.encode(\n        payload=payload,\n        key=private_key.replace(\"\\\\n\", \"\\n\"),\n        algorithm=\"RS256\",\n        headers=headers\n    )\n\n    body: dict = {\n        \"grant_type\": \"urn:ietf:params:oauth:grant-type:jwt-bearer\",\n        \"assertion\": signed_jwt\n    }\n    response: requests.Response = requests.request(\n        \"POST\", \"https://oauth2.googleapis.com/token\", json=body\n    )\n\n    response.raise_for_status()\n\n    content = response.json()\n    return content.get('access_token')\n\ndef read_gsheet(ssid, ranges, token):\n    url = f\"https://sheets.googleapis.com/v4/spreadsheets/{ssid}/values/{ranges}\"\n    headers = {\n        \"Authorization\": f\"Bearer {token}\"\n    }\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n\ndef append_gsheet(ssid, ranges, data, token):\n    url = f\"https://sheets.googleapis.com/v4/spreadsheets/{ssid}/values/{ranges}:append\"\n\n    body = {\n        \"range\": ranges,\n        \"majorDimension\": \"ROWS\",\n        \"values\": data\n    }\n    headers = {\n        \"Authorization\": f\"Bearer {token}\"\n    }\n    response = requests.post(url, params={\"valueInputOption\": \"RAW\"}, headers=headers, json=body)\n    response.raise_for_status()\n\ndef create_linkedin_post(post):\n    linkedin_user_id = os.getenv(\"LINKEDIN_USER_ID\")\n    linkedin_token = os.getenv(\"LINKEDIN_TOKEN\")\n    linkedin_post_endpoint = \"https://api.linkedin.com/v2/ugcPosts\"\n\n    rmd_file = os.listdir(f\"./_{post['path']}\")\n    rmd_file = list(filter(lambda x: \".rmd\" in x.lower(), rmd_file))[0]\n\n    rmd_yml = read_rmd_yml(f\"./_{post['path']}/{rmd_file}\")\n    post_note = \"The post was created by Github Actions.\\nhttps://github.com/wilsonkkyip/wilsonkkyip.github.io\"\n    abstract = rmd_yml[\"abstract\"] + f\"\\n\\n{post_note}\"\n\n    body = build_post_body(\n        user_id=linkedin_user_id,\n        post_content=abstract,\n        media_title=rmd_yml[\"title\"],\n        media_description=rmd_yml[\"description\"],\n        article_url=f\"https://wilsonkkyip.github.io/{post['path']}\"\n    )\n\n    headers = {\n        \"X-Restli-Protocol-Version\": \"2.0.0\",\n        \"Authorization\": \"Bearer \" + linkedin_token \n    }\n\n    response = requests.post(\n        url=linkedin_post_endpoint, \n        json=body, \n        headers=headers\n    )\n    response.raise_for_status()\n\n    content = response.json()\n\n    return content\n\n\ndef main():\n    gcp_client_email = os.getenv(\"GCP_CLIENT_EMAIL\")\n    gcp_private_key_id = os.getenv(\"GCP_PRIVATE_KEY_ID\")\n    gcp_private_key = os.getenv(\"GCP_PRIVATE_KEY\")\n\n    log_ssid = os.getenv(\"LINKEDIN_POSTS_LOG_SSID\")\n    log_range = os.getenv(\"LINKEDIN_POSTS_LOG_RANGE\")\n\n    gcp_token = auth_gapi_token(\n        gcp_client_email, gcp_private_key_id, gcp_private_key\n    )\n\n    logs = read_gsheet(log_ssid, log_range, gcp_token)\n    linkedin_posts = [\n        {logs[\"values\"][0][0]: x[0], logs[\"values\"][0][1]: x[1]} for x in logs[\"values\"][1:]\n    ]\n\n    with open(\"./posts/posts.json\", \"r\") as file:\n        page_posts = json.loads(file.read())\n\n    missing_post = find_latest_missing_post(page_posts, linkedin_posts)\n\n    if missing_post:\n        response = create_linkedin_post(missing_post)\n        appending_data = [[missing_post[\"path\"], response.get(\"id\")]]\n        append_gsheet(log_ssid, log_range, appending_data, gcp_token)\n\nif __name__ == \"__main__\": \n    main()\n\n\n\n\n",
    "preview": "posts/2023-08-03-github-actions-with-example/img/github-actions.png",
    "last_modified": "2023-08-20T12:28:15+00:00",
    "input_file": "github-actions-with-example.knit.md"
  },
  {
    "path": "posts/2023-07-29-rust-gapi-oauth2/",
    "title": "Google OAuth2 Implementation on Rust Reqwest",
    "description": "An implementation of Google OAuth2 procedures on Rust reqwest for Server-side Web Apps and Service Accounts.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-07-29",
    "categories": [
      "rust",
      "oauth2",
      "google-oauth"
    ],
    "contents": "\n\nContents\nIntroduction\nOAuth2 Procedures\nCreate Google Cloud Project\nSelect Required Library\nCreate OAuth Client ID Secrets\nConfigure OAuth Consent Screen\nCreate OAuth Client ID for Users\n\nService Account\n\nRust Reqwest Implementation\nAuthorise Client Application (Client ID)\nAuthorise Client Application (Refresh Token)\nAuthorise Service Account\n\nExamples\n\n\n\n\nIntroduction\nIt comes to me on many occasions that Google APIs are required to complete my tasks. API keys may be an easy choice for those non-sensitive scopes (for example calling YouTube API for some public videos and channels). But when it comes to handling files in Google Drive, things become complicated as the service requires authentication and authorisation. This article aims to provide a solution on obtaining an authorised token to be put in http requests’ header for calling the Google APIs’ sensitive scopes in Rust environment.\nA Github repo was created for the purpose. It can be used in CLI environment and imported as rust crate as well. Below will first briefly describe the OAuth2 procedures, then walk through some important script, and finally will show some examples using of the crate.\nOAuth2 Procedures\nWhen I first encountered OAuth2, I was confused about what scopes and endpoints are because both scopes and endpoints are represented by url-like strings in Google APIs. In a nutshell, endpoints represent what services you want to use. For example there is a specific endpoint for reading the metadata of a file in Google Drive; there is another endpoint for you to update the file. On the other hand, scopes are the abilities of your authorised token. For example, is your token able to read the files from Google Drive? It depends on whether your token contains the specific scope.\nGoogle separates the authorisation method for server-to-server interactions and user-to-server interactions. We will use a service account for the prior situation and a client secret for the later one. Both can be represented by a JSON file. To obtain these JSON files, we first need to create a Google Cloud Project. Then within the project, we can create the secret JSON files.\n\n\n\n\n\n\nCreate Google Cloud Project\nGo to https://console.developers.google.com and click Select a project.\nClick New Project.\nEnter the Project name and click Create.\n\n\n\n\n\n\n\n\n\n\n\n\nSelect Required Library\nUnder APIs and service, click Library.\nSearch the API library(ies) you wish to use. In this example, we choose Google Drive API.\nClick the library you want.\nClick Enable.\n\n\n\n\n\n\n\n\n\n\n\n\nCreate OAuth Client ID Secrets\nNow we have created a project and picked the required libraries. This section will show how to obtain a client_secret of the application for users to authorise. In order to do so, we need to first configure an OAuth consent screen to inform users about the name of the application and which scopes will be used by the application when they do the authorisation. Then we will create the application secret (or client_secret) for this application.\nConfigure OAuth Consent Screen\nUnder APIs and services, Credentials, click Configure consent screen or OAuth consent screen.\nIf you are within an organisation, you can pick Internal or External as User Type. Otherwise, you can only pick External.\nFor internal apps, it is only available to users within the organisation. But the app is not required to have any privacy policy.\nFor external apps, you can add at most 100 test users for testing the application before published. But the refresh_token obtained from the authorisation and authentication process is only valid for 1 week only.\n\nEnter the App name and User support email.\nScroll down and enter the developer contact information and click Save and continue.\nClick Add or remove scopes.\nSelect the scopes you want to use.\nClick Save and continue.\n(For external apps only) Click Add users as test users for the application.\n(For external apps only) Enter the email address(es) for the test user(s). Then click Add.\nClick Save and continue.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate OAuth Client ID for Users\nUnder APIs and services, Credentials, click Create Credentials, then click OAuth client ID.\nSelect Web application as Application type and enter the name of the application.\nScroll down and enter the Authorised redirect URIs. Please put a slash (/) at the end of the uri. Then click Create.\nFinally click Download JSON.\n\n\n\n\n\n\n\n\n\n\n\n\nService Account\nThis section describe how to obtain a service account JSON. If you wish to handle users’ data, please follow this section.\nUnder APIs and services, Credentials, click Create Credentials, then click Service Account.\nInsert the name, account id, and description of the service account. Then click Done.\nClick the newly created service account email.\nClick Keys, then clickAdd key and Create new key.\nSelect JSON as key type and click Create to download the service account JSON.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRust Reqwest Implementation\nNow we have obtained the secret JSON (either a client_seceret or a service_account or both). Depends on which type of secret we have, the authorisation methods are different.\nAuthorise Client Application (Client ID)\nFor authorising a client application (see this figure), we need to\nBuild a url with the follow query parameters:\nclient_id (the identification of the client application)\nredirect_uri (those we specified in step 3 in this section, put 1 uri here only)\nscope (the scopes the application wants to use; space-delimited if more than one is used)\naccess_type (either online of offline. A refresh_token will be obtained in later step for acquiring updated access token without another consent from the users)\n\nYou can specified more parameters for different configuration. See more from here.\nSend a request to Google OAuth page using the above url. Google will also ask for user consent in this stage.\nGoogle returns an authorisation code to the redirect_uri we specified above.\nSend another request to Google with the authorisation code obtained from the last step to exchange an access_token (and refresh_token if specified in step 1).\nThis access_token can use used to access the authorised endpoints.\n\n\n\nFigure 1: Authorise client application\n\n\n\nAmong the JSON obtained from this section, we create the following struct for the key-value pairs.\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct ClientSecret {\n    pub client_id: String,\n    pub project_id: String,\n    pub auth_uri: String,\n    pub token_uri: String,\n    pub auth_provider_x509_cert_url: String,\n    pub client_secret: String,\n    pub redirect_uris: Vec<String>\n}\n\nThen we implement a method to the above struct to build the url for step 1.\n\npub fn auth_url(&self, scope: &str) -> String {\n    let params: HashMap<_,_> = HashMap::from([\n        (\"response_type\", \"code\"),\n        (\"access_type\", \"offline\"), // set 'offline' to obtain 'refresh_token'\n        (\"prompt\", \"consent\"),\n        (\"client_id\", &self.client_id),\n        (\"redirect_uri\", &self.redirect_uris[0]),\n        (\"scope\", &scope),\n        (\"state\", &self.client_id)\n    ]);\n\n    let url = reqwest::Url::parse_with_params(\n        &self.auth_uri, params\n    ).expect(\"Failed to parse auth url.\").to_string();\n    \n    return url;\n}\n\nNow we need to print out the above url and set up a http server to listen from Google’s response with the authorisation code to finish step 3.\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct AuthCode {\n    pub code: String,\n    pub scope: String\n}\n\npub fn auth_code(&self, scope: &str, port: u32) -> Result<AuthCode, std::io::Error> {\n    let auth_url: String = self.auth_url(scope);\n    println!(\"Please visit this URL to authorize this application: {}\", auth_url);\n\n    let listener: TcpListener = \n        TcpListener::bind(format!(\"localhost:{}\", port))\n            .expect(\"Failed to bind to port\");\n    \n    let (mut stream, _) = listener.accept().unwrap();\n    let mut buf = [0;2048];\n    stream.read(&mut buf).unwrap();\n\n    let buf_str: String = String::from_utf8_lossy(&buf[..]).to_string();\n    let buf_vec: Vec<&str> = buf_str\n        .split(\" \")\n        .collect::<Vec<&str>>();\n\n    let args: String = buf_vec[1].to_string();\n    let callback_url: Url = Url::parse(\n        (format!(\"http://localhost:{}\", port) + &args).as_str()\n    ).expect(\"Failed to parse callback URL\");\n    let query: HashMap<_,_> = callback_url.query_pairs().into_owned().collect();\n    let output = AuthCode {\n        code: query.get(\"code\").unwrap().to_string(),\n        scope: query.get(\"scope\").unwrap().to_string()\n    };\n    return Ok(output);\n}\n\nFor step 4, the following function will prepare a POST request to Google to exchange the authorisation code for the access_token (and refresh_token).\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct ClientSecretTokenResponse {\n    pub access_token: String,\n    pub expires_in: i64,\n    pub refresh_token: String,\n    pub scope: String,\n    pub token_type: String\n}\n\npub async fn auth_token(&self, code: &str) -> Result<ClientSecretTokenResponse, reqwest::Error> {\n    let body: Value = serde_json::json!({\n        \"client_id\": self.client_id,\n        \"client_secret\": self.client_secret,\n        \"code\": code,\n        \"grant_type\": \"authorization_code\",\n        \"redirect_uri\": self.redirect_uris[0]\n    });\n\n    let response = reqwest::Client::new()\n        .post(self.token_uri.as_str())\n        .json(&body)\n        .send()\n        .await?;\n\n    let content: ClientSecretTokenResponse = response.json()\n        .await.expect(\"Failed to parse http response\");\n\n    return Ok(content);\n}\n\nAuthorise Client Application (Refresh Token)\nWhen we obtained the refresh_token from the above, we can further request a new access_token when the previous one is expired. To do do, we first define a struct and implement an auth function to it.\n\npub const OAUTH_TOKEN_URL: &str = \"https://oauth2.googleapis.com/token\";\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct Token {\n    pub access_token: String,\n    pub expires_in: i64\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct UserSecret {\n    pub client_id: String,\n    pub client_secret: String,\n    pub refresh_token: String\n}\n\npub async fn auth(&self) -> Result<Token, reqwest::Error> {\n    // Prepare auth body\n    let mut body: Value = serde_json::to_value(&self)\n        .expect(\"Could not convert UserSecret to Value\");\n    body[\"grant_type\"] = Value::String(\"refresh_token\".to_string());\n\n    // Auth request\n    let response: reqwest::Response = reqwest::Client::new()\n        .post(OAUTH_TOKEN_URL)\n        .json(&body)\n        .send()\n        .await?;\n\n    // Parse response to output\n    let content: Token = response.json().await?;\n\n    return Ok(content)\n}\n\nAuthorise Service Account\nFor authorising a service account (see this figure), we need to\nPrepare a JWT token. The token is separated into 3 parts:\nHeader: consist of the algorithm name and the privated_key_id (from the secret JSON).\nClaim: consist of client_email, scope, aud, iat and exp.\nKey: the private_key from the secret JSON.\n\nUse the JWT token to exchange the access_token.\n\n\n\nFigure 2: Authorise service account\n\n\n\nBelow shows the implementation.\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct ServiceSecret {\n    pub client_email: String,\n    pub private_key_id: String,\n    pub private_key: String\n}\n\npub async fn auth(&self, scope: &str) -> Result<Token, reqwest::Error> {\n    // Auth Service Account\n    // https://developers.google.com/identity/protocols/oauth2/service-account\n\n    // Prepare JWT claim\n    let claim: serde_json::Value = serde_json::json!({\n        \"iss\": self.client_email.to_string(),\n        \"scope\": scope.to_string(),\n        \"aud\": \"https://oauth2.googleapis.com/token\".to_string(),\n        \"iat\": chrono::offset::Utc::now().timestamp(),\n        \"exp\": chrono::offset::Utc::now().timestamp() + 3600\n    });\n\n    // Prepare JWT header\n    let header: Header = Header{\n        alg: Algorithm::RS256,\n        kid: Some(self.private_key_id.to_string()),\n        ..Default::default()\n    };\n\n    // Prepare JWT key\n    let key: EncodingKey = EncodingKey::from_rsa_pem(\n        &self.private_key\n            .to_string()\n            .replace(\"\\\\n\", \"\\n\").as_bytes()\n    ).expect(\"Cannot build `EncodingKey`.\");\n\n    // Generate JWT\n    let token: String = encode(\n        &header, &claim, &key\n    ).expect(\"Cannot encode `token`.\");\n\n    // Auth JWT\n    let response: Response = reqwest::Client::new()\n        .post(OAUTH_TOKEN_URL)\n        .json(&serde_json::json!({\n            \"grant_type\": \"urn:ietf:params:oauth:grant-type:jwt-bearer\",\n            \"assertion\": token\n        }))\n        .send()\n        .await?;\n    \n    // Prepare output\n    let content: Token = match response.status() {\n        StatusCode::OK => response.json().await.expect(\"Unable to parse HTTP response JSON.\"),\n        StatusCode::UNAUTHORIZED => {\n            println!(\"{}\", response.text().await.unwrap());\n            panic!(\"HTTP request failed: Unauthorized.\");\n        },\n        _ => {\n            println!(\"{}\", response.text().await.unwrap());\n            panic!(\"HTTP request failed.\");\n        }\n    };\n\n    return Ok(content);\n}\n\nExamples\nA main.rs was also written in the Github repo to provide accessibility from command prompt.\n\ncargo run \n\n# Usage: gapi-oauth <SERVICE> <JSON_PATH> [SCOPE] [PORT]\n# \n# SERVICE: `user`, `service`, or `consent`\n# JSON_PATH: The path to the JSON file containing the credentials.\n# SCOPE: Only required for `service` and `consent`\n# PORT: Only required for `consent`\n\n\ncargo run user /path/to/client_token.json\n\n# {\n#   \"access_token\": \"...\",\n#   \"expires_in\": 3599\n# }\n\n\ncargo run user /path/to/service_acc.json 'https://www.googleapis.com/auth/drive'\n\n# {\n#   \"access_token\": \"...\",\n#   \"expires_in\": 3599\n# }\n\n\ncargo run consent /path/to/client_secret.json 'https://www.googleapis.com/auth/drive' 8088\n\n# Please visit this URL to authorize this application: \n# https://accounts.google.com/o/oauth2/auth?client_id=&prompt=consent&...\n# \n# {\n#   \"access_token\": \"...\",\n#   \"refresh_token\": \"...\",\n#   \"scopes\": [\n#     \"https://www.googleapis.com/auth/drive\"\n#   ],\n#   \"expiry\": \"2023-07-30T17:51:13.123456Z\",\n#   \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n#   \"token_uri\": \"https://oauth2.googleapis.com/token\",\n#   \"client_id\": \"...\",\n#   \"client_secret\": \"...\"\n# }\n\nIt can also be used as crate. After constructing the UserSecret or ServiceSecret, simply use the corresponding auth method to return the access_token.\n\nuse crate::auth_users::UserSecret;\nuse crate::auth_service::ServiceSecret;\n\n#[tokio::test]\nasync fn test_auth_user() {\n    let client_id = std::env::var(\"USER_CLIENT_ID\")\n        .expect(\"No USER_CLIENT_ID in env var\")\n        .as_str().to_string();\n    let client_secret = std::env::var(\"USER_CLIENT_SECRET\")\n        .expect(\"No USER_CLIENT_SECRET in env var\")\n        .as_str().to_string();\n    let refresh_token = std::env::var(\"USER_REFRESH_TOKEN\")\n        .expect(\"No USER_REFRESH_TOKEN in env var\")\n\n    // Construct UserSecret\n    let client_token = UserSecret {\n        client_id: client_id,\n        client_secret: client_secret,\n        refresh_token: refresh_token,\n    };\n\n    // Auth to Token, will panic if failed.\n    let _token = client_token.auth().await\n        .expect(\"Unable to authenticate\");\n}\n\n#[tokio::test]\nasync fn test_auth_service() {\n    let client_email = std::env::var(\"SERVICE_CLIENT_EMAIL\")\n        .expect(\"No SERVICE_CLIENT_EMAIL in env var\")\n        .as_str().to_string();\n    let private_key = std::env::var(\"SERVICE_PRIVATE_KEY\")\n        .expect(\"No SERVICE_PRIVATE_KEY in env var\")\n        .as_str().to_string();\n    let private_key_id = std::env::var(\"SERVICE_PRIVATE_KEY_ID\")\n        .expect(\"No SERVICE_PRIVATE_KEY_ID in env var\")\n        .as_str().to_string();\n\n    let service_secret = ServiceSecret {\n        client_email: client_email,\n        private_key: private_key,\n        private_key_id: private_key_id,\n    };\n\n    let scopes: Vec<String> = vec![\n        \"https://www.googleapis.com/auth/drive\".to_string(),\n        \"https://www.googleapis.com/auth/youtube\".to_string()\n    ];\n\n    let scope = scopes.join(\" \");\n\n    let _token = service_secret.auth(&scope).await\n        .expect(\"Unable to authenticate\");\n}\n\n\n\n\n",
    "preview": "posts/2023-07-29-rust-gapi-oauth2/img/auth_client_id.png",
    "last_modified": "2023-08-20T12:27:34+00:00",
    "input_file": "rust-gapi-oauth2.knit.md"
  }
]
