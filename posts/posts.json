[
  {
    "path": "posts/2023-08-10-writing-airflow-dags/",
    "title": "Writing Airflow DAGs",
    "description": "Introduction to how to write a DAG on Airflow. Including writing custom operators, XComs, branching operators, triggers and variables.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-10",
    "categories": [
      "python",
      "airflow"
    ],
    "contents": "\n\nContents\nIntroduction\nDAG Location\nBasic DAG File\nCustom Operator\nAirflow Context\nTaskflow\nXComs\nBranching\nTriggers\nDatasets\nTriggerDAGRunOperator\n\nVariables\nSet Up Variables\nCalling Variables\nEncrypt Values\nMasking Values\n\nAppendix\n\nIntroduction\nThis article aims to introduce how to write an airflow DAG. We will go through the basic BashOperator and PythonOperator, using Airflow TaskFlow decorators, Airflow context, passing information between tasks using XComs, branching tasks based on conditions, and more.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDAG Location\nThis section will introduce how to write a Directed Acyclic Graph (DAG) in Airflow. Within the Docker imageâ€™s main folder, you should find a directory named dags. Create one if you do not. This directory should link to the containers as it is specified in the docker-compose.yaml.\n\n\nx-airflow-common:\n  &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.6.3}\n  environment:\n    ...\n  volumes:\n    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags\n    - ${AIRFLOW_PROJ_DIR:-.}/logs:/opt/airflow/logs\n    - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config\n    - ${AIRFLOW_PROJ_DIR:-.}/plugins:/opt/airflow/plugins\n    - ${AIRFLOW_PROJ_DIR:-.}/dag-inputs:/opt/airflow/dag-inputs\n    - ${AIRFLOW_PROJ_DIR:-.}/dag-outputs:/opt/airflow/dag-outputs\n\n\nInside the dags directory, create a file named trial_dag.py with the following content. It will create a DAG in Airflow and you can find it in the main page of Airflow (default: localhost:8080).\nBasic DAG File\nBelow shows an example of a DAG.\n\nfrom airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.operators.bash import BashOperator\nfrom airflow.operators.python import PythonOperator\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"retries\": 5,\n    \"retry_delay\": timedelta(minutes=2)\n}\n\ndef _task2(w):\n    print(w)\n\nwith DAG(\n    dag_id=\"trial_dag\",\n    description=\"A trial dag\",\n    default_args=default_args,\n    start_date=datetime(2023,8,1),\n    # end_date=datetime(2024,7,31),\n    schedule=\"@once\"  # Use `schedule_interval` if using Airflow 2.3 or below\n) as dag: \n    task1 = BashOperator(\n        task_id=\"task1\",\n        bash_command=\" echo 'hello'\"\n    )\n    \n    task2 = PythonOperator(\n        task_id=\"task2\",\n        python_callable=_task2,\n        op_kwargs={\"w\": \"world\"}\n    )\n    \n    task1 >> task2  # The order of task being executed.\n\nThe snippet above shows a simple DAG definition file. It first imports the required modules from airflow and datetime. Then define the default_args for the dag (see more available key-value pairs here).\nThere are multiple ways to initialise a dag. This time we used with DAG() as dag: with dag_id, description, schedule, etc. We will see some other methods to define a dag in later sections.\nA task in a dag is an operator object. There are two basic operators: BashOperator and PythonOperator. The BashOperator is used to execute bash command and the PythonOperator is used to execute Python functions. The name of the Python function is passed to the python_callable parameter and the arguments of the primitive function can be passed through the operator using the op_kwargs and op_args arguments.\nFinally we need to specify the order of the tasks by task1 >> task2.\nCustom Operator\nWe can also create custom operators inside the plugins directory which is located at the main Docker image directory. Please make sure the directory is created and specified in docker-compose.yaml. Please also create an empty __init__.py in the plugins directory. Now, we can create a .py file inside the plugins directory to create operators like below.\n\n# plugins/hello_operator.py\n\nfrom airflow.models import BaseOperator\n\nclass HelloOperator(BaseOperator):\n    def __init__(self, name, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.name = name\n\n    def execute(self, context):\n        self.log.info(\"Input `name`: {self.name}.\") # Logging\n        message = f\"Hello {self.name}\"\n        print(message)\n\nTo use the custom operators, simply import the operators from a module as usual.\n\nfrom hello_operator import HelloOperator\n\nwith Dag() as dag:\n    task1 = HelloOperator(task_id=\"task1\", name=\"Bob\")\n    \n    task1\n\nAirflow Context\nLike Gibhub context (see this article), Airflow also provide a way to access information about the running dag and task. As seen in the custom operator, the execute method in the operator class takes in a context argument.\nBelow shows two examples on how to use these context information.\n\n# plugins/hello_operator.py\n\nfrom airflow.models import BaseOperator\n\nclass HelloOperator(BaseOperator):\n    template_fields = ['name']\n    \n    def __init__(self, name, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.name = name\n\n    def execute(self, context):   # context is a TypedDict. See below section\n        message = f\"The name of this DAG is {self.name}\"\n        print(message)\n        \n        ts = context[\"ts_nodash\"] # context is a TypedDict. See below section\n        print(f\"This DAG started at {ts}\")\n\nThe above snippet describe how to use the context variable inside a custom operator. One can also use jinja syntax to specify the information like below. Just remember to specify which argument will be using jinja syntax in the custom operator class (see above template_fields = ['name']).\n\n# dags/hello_dag.py\n\nfrom hello_operator import HelloOperator\n\nwith Dag() as dag:\n    task1 = HelloOperator(task_id=\"task1\", name=\"{{ dag.dag_id }}\")\n    \n    task1\n\nA detailed list of available values from context can be found in [appendix][#appendix].\nTaskflow\nThis section describe how to write a DAG with the TaskFlow API paradigm. The paradigm converts functions into Airflow DAGs or tasks with the dag and task decorators. Below shows a simple DAG written in this paradigm.\n\nfrom airflow.decorators import task, dag\nfrom airflow.operators.python import get_current_context\nfrom datetime import datetime, timedelta\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"retries\": 0,\n    \"retry_delay\": timedelta(minutes=2)\n}\n\n@dag(\n    dag_id=\"trial_dag\",\n    schedule=\"@once\",\n    start_date=datetime(2023,8,1),\n    default_args=default_args\n)\ndef trial_dag():\n    @task(task_id=\"task1\")\n    def task1():\n        print(\"hello\")\n    \n    @task(task_id=\"task2\")\n    def task2(name):\n        context = get_current_context()    # Use context within TaskFlow API\n        print(f\"The name of the DAG is {name}\")\n        dag_loc = context[\"dag\"].fileloc\n        print(f\"The location of the DAG is {dag_loc}\")\n    \n    task1() >> task2(\"{{ dag.dag_id }}\")   # Set the order of tasks\n\ntrial_dag()    # Initialise the DAG\n\nXComs\nOften times, we would like to pass some information from the upstream task to the downstream task. In Airflow, we can achieve this via the XComs. Please note that XComs is not designed to transfer large data. If you are using MySQL as the Airflow database, you can only transfer information within 64kb. You can define your own XComs backend if needed. Yet, this is out of the scope of this article.\nThere are two ways to send information in one task.\nSimply return the value (see ModelTrainingOperator)\nUsing ti.xcom_push (see ETLOperator)\nThe pushed values are stored in the Airflow context in the form of key-value pairs. That means, there is a specific key to each returned or pushed value. The key for the first method is always return_value but you can specify the key you want in the second method.\nAs the pushed values are stored in the Airflow context, it can be accessed via the context argument (see ModelSelectionOperator) or via the jinja template (see model_training_task1). Yet, the values pulled from jinja template are always in text form.\nThe script below mimics a ML model training pipeline, which utilises the XComs to transfer information between tasks.\n\nfrom airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.models import BaseOperator\nfrom random import randint\nimport numpy as np\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"retries\": 0,\n    \"retry_delay\": timedelta(minutes=2)\n}\n\nclass ETLOperator(BaseOperator):\n    template_fields = ['name']\n    def __init__(self, name, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.name = name\n\n    def execute(self, context):\n        self.log.info(f\"Input parameter: {self.name}\")\n        context[\"ti\"].xcom_push(key=\"dag_id\", value=self.name)\n\nclass ModelTrainingOperator(BaseOperator):\n    template_fields = ['name']\n    def __init__(self, name, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.name = name\n    \n    def execute(self, context):\n        self.log.info(f\"Previous output: {self.name}\")\n        task_id = context[\"ti\"].task_id\n        return {\"task_id\": task_id, \"score\": randint(1, 10)}\n\nclass ModelSelectionOperator(BaseOperator):\n    def __init__(self, models, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.models = models\n    \n    def execute(self, context):\n        models = self.models.split(\",\")\n        result = []\n        for model in models:\n            result.append(context[\"ti\"].xcom_pull(model))\n        \n        self.log.info(result)\n        best_model = result[np.argmax([x[\"score\"] for x in result])]\n        self.log.info(f\"Best model: {best_model}\")\n\nwith DAG(\n    dag_id=\"trial_dag\",\n    default_args=default_args,\n    start_date=datetime(2023,8,1),\n    schedule=\"@once\",\n    description=\"A trial dag\"\n) as dag: \n    etl_task = ETLOperator(\n        task_id=\"etl_task\",\n        name=\"{{ dag.dag_id }}\"\n    )\n    \n    model_training_task1 = ModelTrainingOperator(\n        task_id=\"model_training_task1\",\n        name=\"{{ ti.xcom_pull(task_ids=['etl_task'], key='dag_id') }}\"\n    )\n    \n    model_training_task2 = ModelTrainingOperator(\n        task_id=\"model_training_task2\",\n        name=\"{{ ti.xcom_pull(task_ids=['etl_task'], key='dag_id') }}\"\n    )\n    \n    model_training_task3 = ModelTrainingOperator(\n        task_id=\"model_training_task3\",\n        name=\"{{ ti.xcom_pull(task_ids=['etl_task'], key='dag_id') }}\"\n    )\n\n    model_selection_task = ModelSelectionOperator(\n        task_id=\"model_selection_task\",\n        models=\"model_training_task1,model_training_task2,model_training_task3\"\n    )\n    \n    etl_task >> \\\n    [model_training_task1, model_training_task2, model_training_task3] >> \\\n    model_selection_task\n\n\n\n\nBranching\nThe previous example will run all three model_training tasks after the etl_task has finished. This section shows how to do branching, which means to execute a particular task based on some conditions.\n\nfrom airflow import DAG\nfrom datetime import datetime, timedelta\nfrom airflow.models import BaseOperator\nfrom airflow.utils.trigger_rule import TriggerRule\nfrom airflow.operators.branch import BaseBranchOperator\nfrom airflow.operators.empty import EmptyOperator\nfrom airflow.utils.edgemodifier import Label\nfrom random import randint, choice\n\ndefault_args = {\n    \"owner\": \"airflow\",\n    \"retries\": 0,\n    \"retry_delay\": timedelta(minutes=2)\n}\n\nclass CriteriaBranchingOperator(BaseBranchOperator):\n    def __init__(self, methods, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.methods = methods\n    \n    def choose_branch(self, context):\n        return choice(self.methods)\n\nclass ModelTrainingOperator(BaseOperator):\n    def execute(self, context):\n        task_id = context[\"ti\"].task_id\n        return {\"task_id\": task_id, \"score\": randint(1, 10)}\n\nwith DAG(\n    dag_id=\"trial_dag\",\n    default_args=default_args,\n    start_date=datetime(2023,8,1),\n    schedule=\"@once\",\n    description=\"A trial dag\"\n) as dag: \n    etl_task = EmptyOperator(\n        task_id=\"etl_task\"\n    )\n\n    methods = [\"method_A\", \"method_B\", \"method_C\", \"method_D\"]\n\n    criteria_selection = CriteriaBranchingOperator(\n        task_id=\"criteria_selection\",\n        methods=methods\n    )\n\n    etl_task >> criteria_selection\n\n    deploy_model = EmptyOperator(\n        task_id=\"deploy_model\",\n        trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS\n    )\n\n    for method in methods:\n        method_task = ModelTrainingOperator(\n            task_id=method\n        )\n\n        criteria_selection >> Label(method) >> method_task >> deploy_model\n\n\n\n\nTriggers\nThis section introduce triggers to start a DAG from another DAG. We will go through 2 different methods:\nDatasets\nTriggerDAGRunOperator\nDatasets\nTo trigger a DAG using Datasets, the Dataset class must be imported first. Then define a dataset with an uri. In one of the task in the trigger_dag, set the outlets to a list of the defined dataset. Finally, in the target_dag, set the schedule to be a list of datasets that will trigger the DAG to run. See an example below.\n\nfrom airflow import Dataset, DAG \nfrom airflow.models import BaseOperator\nfrom datetime import datetime\n\ndataset = Dataset(\"/opt/airflow/dag-outputs/test_data.json\")\n\nclass TriggerOperator(BaseOperator):\n    def execute(self, context):\n        outlets = context[\"outlets\"]\n        ts = context[\"ts_nodash\"]\n        for outlet in outlets:\n            with open(outlet.uri, \"w\") as f:\n                f.write(ts)\n        \n        self.log.info(context[\"outlets\"])\n        # [2023-08-13, 21:34:06 UTC] {trial_dag.py:15} INFO - \n        #  [Dataset(uri='/opt/airflow/dag-outputs/test_data.json', extra=None)]\n\n        self.log.info(\"Triggering Dataset\")\n        # [2023-08-13, 21:34:06 UTC] {trial_dag.py:16} INFO - Triggering Dataset\n\nclass ActionOperator(BaseOperator):\n    def execute(self, context):\n        self.log.info(context[\"dag\"].schedule_interval)\n        # [2023-08-13, 21:34:06 UTC] {trial_dag.py:20} INFO - Dataset\n        \n        self.log.info(context[\"dag\"].dataset_triggers)\n        # [2023-08-13, 21:34:06 UTC] {trial_dag.py:21} INFO - \n        #  [Dataset(uri='/opt/airflow/dag-outputs/test_data.json', extra=None)]\n\nwith DAG(\n    dag_id=\"trigger_dag\",\n    schedule=\"@once\",\n    start_date=datetime(2023,8,1)\n) as dag:\n    trigger_task = TriggerOperator(\n        task_id=\"trigger_task\",\n        outlets=[dataset]\n    )\n\n    trigger_task\n\nwith DAG(\n    dag_id=\"target_dag\",\n    schedule=[dataset],\n    start_date=datetime(2023,8,1)\n) as dag:\n    action_task = ActionOperator(\n        task_id=\"action_task\"\n    )\n\n    action_task\n\nTriggerDAGRunOperator\nAnother method to trigger a DAG to run is to use the TriggerDAGRunOperator. In the trigger_dag, create a task with TriggerDAGRunOperator, set the trigger_dag_id to the DAG you wish to trigger. You can also pass some information to the target_dag using the conf argument as shown below.\nIn the target_dag, you can retrieve the conf by accessing context[\"dag_run\"].conf.\n\nfrom airflow import DAG \nfrom airflow.models import BaseOperator\nfrom airflow.operators.trigger_dagrun import TriggerDagRunOperator\nfrom datetime import datetime\n\nclass ActionOperator(BaseOperator):\n    def execute(self, context):\n        # Retrieving conf from previous dag in TriggerDagRunOperator\n        self.log.info(context[\"dag_run\"].conf)\n        # {\"upstream_dag_id\": \"trigger_dag\", \"last_update\": \"...\"}\n\nwith DAG(\n    dag_id=\"trigger_dag\",\n    start_date=datetime(2023,8,1),\n    schedule=\"@once\"\n) as dag:\n    trigger_task = TriggerDagRunOperator(\n        task_id=\"trigger_task\",\n        trigger_dag_id=\"target_dag\",\n        conf = {\n            \"upstream_dag_id\": \"{{ dag.dag_id }}\",\n            \"last_update\": \"{{ ts_nodash }}\"\n        }\n    )\n\n    trigger_task\n\nwith DAG(\n    dag_id=\"target_dag\",\n    start_date=datetime(2023,8,1),\n    schedule=\"@once\"\n) as dag:\n    action_task = ActionOperator(\n        task_id=\"action_task\"\n    )\n\n    action_task\n\nVariables\nThis section will cover how to define and use variables in Airflow context.\nSet Up Variables\nThere are two ways to set up Airflow variables.\nOn Airflow main page, go to Admin â€“> Variables and set up there.\nSet up environment variables with prefix AIRFLOW_VAR_<NAME_OF_VAR>.\nCalling Variables\n\nfrom airflow import DAG \nfrom airflow.models import BaseOperator\nfrom datetime import datetime\nfrom airflow.models import Variable\n\nclass TrialOperator(BaseOperator):\n    template_fields = [\"json_var\", \"value_var\"]\n    def __init__(self, json_var, value_var, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.json_var = json_var\n        self.value_var = value_var\n\n    def execute(self, context):\n        # Access from jinja template with type dict\n        self.log.info(self.json_var)\n\n        # Access from Variable with type dict\n        self.log.info(Variable.get(\"my_var\", deserialize_json=True))\n\n        # Access from jinja template with type str\n        self.log.info(self.value_var)\n\n        # Access from Variable with type str\n        self.log.info(Variable.get(\"my_var\", deserialize_json=False))\n\n        # The following will fail\n        self.log.info(context[\"var\"][\"json\"][\"my_var\"])\n\n        # The following will fail\n        self.log.info(context[\"var\"][\"value\"][\"my_var\"])\n\n\nwith DAG(\n    dag_id=\"trial_dag\",\n    start_date=datetime(2023,8,1),\n    schedule=\"@once\"\n) as dag:\n    task1 = TrialOperator(\n        task_id=\"task1\",\n        json_var=\"{{  var.json.my_var  }}\",\n        value_var=\"{{  var.value.my_var  }}\"\n    )\n\n    task1\n\n# [2023-08-13, 18:12:03 UTC] {trial_dag.py:16} INFO - {'name': 'Bob', 'age': 21}\n# [2023-08-13, 18:12:03 UTC] {trial_dag.py:19} INFO - {'name': 'Bob', 'age': 21}\n# [2023-08-13, 18:12:03 UTC] {trial_dag.py:22} INFO - {\"name\": \"Bob\", \"age\": 21}\n# [2023-08-13, 18:12:03 UTC] {trial_dag.py:25} INFO - {\"name\": \"Bob\", \"age\": 21}\n\nEncrypt Values\nTo encrypt the value of variables, first generate a fernet_key from the following.\n\nfrom cryptography.fernet import Fernet\nFernet.generate_key().decode()\n\n# 'bbg62_v203tply8_kXf8rD4WJ93IDYw6EVdQ7u2G-Bk='\n\nThen change the following line in airflow.cfg under [core] section.\n\n[core]\n...\nfernet_key = bbg62_v203tply8_kXf8rD4WJ93IDYw6EVdQ7u2G-Bk=\n\nFinally rebuild the Docker containers. Now all the variables created in Admin â€“> Variables are encrypted. One can check the variable table from the Postgres container.\n\nSELECT * FROM variable;\n\n--  id |   key    |          val          | description | is_encrypted \n-- ----+----------+-----------------------+-------------+--------------\n--   1 | my_var   | gAAAAABk2...wiuSFTRw= |             | t\n--   2 | my_token | gAAAAABk2...goZh6tg== |             | t\n\nMasking Values\nBy default, Airflow will hide the variable value if its key contains access_token, api_key, apikey,authorization, passphrase, passwd, password, private_key, secret or token. To extend the list, you can specify them in the airflow.cfg.\n\n[core]\nsensitive_var_conn_names = comma,separated,sensitive,names\n\n\nfrom airflow import DAG \nfrom airflow.models import BaseOperator\nfrom datetime import datetime\nfrom airflow.models import Variable\n\nclass TrialOperator(BaseOperator):\n    def execute(self, context):\n        my_token = Variable.get(\"my_token\", deserialize_json=False)\n        self.log.info(my_token)\n        self.log.info(my_token == \"123456\")\n\nwith DAG(\n    dag_id=\"trial_dag\",\n    start_date=datetime(2023,8,1),\n    schedule=\"@once\"\n) as dag:\n    task1 = TrialOperator(\n        task_id=\"task1\"\n    )\n\n    task1\n\n# [2023-08-13, 18:16:04 UTC] {trial_dag.py:9} INFO - ***\n# [2023-08-13, 18:16:04 UTC] {trial_dag.py:10} INFO - True\n\nAppendix\n\n# Context dictionary\n{\n    'conf': <***.configuration.AirflowConfigParser object at 0xffff82ce0c10>, # Not useful\n    'dag': <DAG: trial_dag>, \n    'dag_run': <DagRun trial_dag @ 2023-08-11 11:51:11.303634+00:00: manual__2023-08-11T11:51:11.303634+00:00, state:running, queued_at: 2023-08-11 11:51:11.312548+00:00. externally triggered: True>, \n    'data_interval_end': DateTime(2023, 8, 11, 11, 51, 11, 303634, tzinfo=Timezone('UTC')), \n    'data_interval_start': DateTime(2023, 8, 11, 11, 51, 11, 303634, tzinfo=Timezone('UTC')), \n    'ds': '2023-08-11', \n    'ds_nodash': '20230811', \n    'execution_date': DateTime(2023, 8, 11, 11, 51, 11, 303634, tzinfo=Timezone('UTC')), \n    'expanded_ti_count': None, \n    'inlets': [], \n    'logical_date': DateTime(2023, 8, 11, 11, 51, 11, 303634, tzinfo=Timezone('UTC')), \n    'macros': <module '***.macros' from '/home/***/.local/lib/python3.7/site-packages/***/macros/__init__.py'>, \n    'next_ds': '2023-08-11', \n    'next_ds_nodash': '20230811', \n    'next_execution_date': DateTime(2023, 8, 11, 11, 51, 11, 303634, tzinfo=Timezone('UTC')), \n    'outlets': [], \n    'params': {}, \n    'prev_data_interval_start_success': DateTime(2023, 8, 11, 11, 44, 10, 670770, tzinfo=Timezone('UTC')), \n    'prev_data_interval_end_success': DateTime(2023, 8, 11, 11, 44, 10, 670770, tzinfo=Timezone('UTC')), \n    'prev_ds': '2023-08-11', \n    'prev_ds_nodash': '20230811', \n    'prev_execution_date': DateTime(2023, 8, 11, 11, 51, 11, 303634, tzinfo=Timezone('UTC')), \n    'prev_execution_date_success': DateTime(2023, 8, 11, 11, 44, 10, 670770, tzinfo=Timezone('UTC')), \n    'prev_start_date_success': DateTime(2023, 8, 11, 11, 44, 11, 512290, tzinfo=Timezone('UTC')), \n    'run_id': 'manual__2023-08-11T11:51:11.303634+00:00', \n    'task': <Task(HelloOperator): task2>, \n    'task_instance': <TaskInstance: trial_dag.task2 manual__2023-08-11T11:51:11.303634+00:00 [running]>, \n    'task_instance_key_str': 'trial_dag__task2__20230811', \n    'test_mode': False, \n    'ti': <TaskInstance: trial_dag.task2 manual__2023-08-11T11:51:11.303634+00:00 [running]>, \n    'tomorrow_ds': '2023-08-12', \n    'tomorrow_ds_nodash': '20230812', \n    'triggering_dataset_events': <Proxy at 0xffff7ac07230 with factory <function TaskInstance.get_template_context.<locals>.get_triggering_events at 0xffff7abe78c0>>, \n    'ts': '2023-08-11T11:51:11.303634+00:00', \n    'ts_nodash': '20230811T115111', \n    'ts_nodash_with_tz': '20230811T115111.303634+0000', \n    'var': {'json': None, 'value': None}, \n    'conn': None, \n    'yesterday_ds': '2023-08-10', \n    'yesterday_ds_nodash': '20230810'\n}\n\n\n# context[\"dag\"]\n{\n    'access_control': None, \n    'add_task': <bound method DAG.add_task of <DAG: trial_dag>>, \n    'add_tasks': <bound method DAG.add_tasks of <DAG: trial_dag>>, \n    'allow_future_exec_dates': False, \n    'auto_register': True, \n    'bulk_sync_to_db': <bound method DAG.bulk_sync_to_db of <class '***.models.dag.DAG'>>, \n    'bulk_write_to_db': <bound method DAG.bulk_write_to_db of <class '***.models.dag.DAG'>>, \n    'catchup': True, \n    'clear': <bound method DAG.clear of <DAG: trial_dag>>, \n    'clear_dags': <bound method DAG.clear_dags of <class '***.models.dag.DAG'>>, \n    'cli': <bound method DAG.cli of <DAG: trial_dag>>, \n    'concurrency': 16, \n    'concurrency_reached': False, \n    'create_dagrun': <bound method DAG.create_dagrun of <DAG: trial_dag>>, \n    'dag_id': 'trial_dag', \n    'dagrun_timeout': None, \n    'dataset_triggers': [], \n    'date_range': <bound method DAG.date_range of <DAG: trial_dag>>, \n    'deactivate_stale_dags': <function DAG.deactivate_stale_dags at 0xffff7ed833b0>, \n    'deactivate_unknown_dags': <function DAG.deactivate_unknown_dags at 0xffff7ed83290>, \n    'default_args': {'owner': '***', 'retries': 0, 'retry_delay': datetime.timedelta(seconds=120)}, \n    'default_view': 'grid', \n    'description': 'A trial dag', \n    'doc_md': None, \n    'edge_info': {}, \n    'end_date': None, \n    'fileloc': '/opt/***/dags/trial_dag.py', \n    'filepath': 'trial_dag.py', \n    'folder': '/opt/***/dags', \n    'following_schedule': <bound method DAG.following_schedule of <DAG: trial_dag>>, \n    'full_filepath': '/opt/***/dags/trial_dag.py', \n    'get_active_runs': <bound method DAG.get_active_runs of <DAG: trial_dag>>, \n    'get_concurrency_reached': <bound method DAG.get_concurrency_reached of <DAG: trial_dag>>, \n    'get_dagrun': <bound method DAG.get_dagrun of <DAG: trial_dag>>, \n    'get_dagruns_between': <bound method DAG.get_dagruns_between of <DAG: trial_dag>>, \n    'get_default_view': <bound method DAG.get_default_view of <DAG: trial_dag>>, \n    'get_doc_md': <bound method DAG.get_doc_md of <DAG: trial_dag>>, \n    'get_edge_info': <bound method DAG.get_edge_info of <DAG: trial_dag>>, \n    'get_is_active': <bound method DAG.get_is_active of <DAG: trial_dag>>, \n    'get_is_paused': <bound method DAG.get_is_paused of <DAG: trial_dag>>, \n    'get_last_dagrun': <bound method DAG.get_last_dagrun of <DAG: trial_dag>>, \n    'get_latest_execution_date': <bound method DAG.get_latest_execution_date of <DAG: trial_dag>>, \n    'get_next_data_interval': <bound method DAG.get_next_data_interval of <DAG: trial_dag>>, \n    'get_num_active_runs': <bound method DAG.get_num_active_runs of <DAG: trial_dag>>, \n    'get_num_task_instances': <function DAG.get_num_task_instances at 0xffff7ed834d0>, \n    'get_run_data_interval': <bound method DAG.get_run_data_interval of <DAG: trial_dag>>, \n    'get_run_dates': <bound method DAG.get_run_dates of <DAG: trial_dag>>, \n    'get_serialized_fields': <bound method DAG.get_serialized_fields of <class '***.models.dag.DAG'>>, \n    'get_task': <bound method DAG.get_task of <DAG: trial_dag>>, \n    'get_task_instances': <bound method DAG.get_task_instances of <DAG: trial_dag>>, \n    'get_task_instances_before': <bound method DAG.get_task_instances_before of <DAG: trial_dag>>, \n    'get_template_env': <bound method DAG.get_template_env of <DAG: trial_dag>>, \n    'handle_callback': <bound method DAG.handle_callback of <DAG: trial_dag>>, \n    'has_dag_runs': <bound method DAG.has_dag_runs of <DAG: trial_dag>>, \n    'has_on_failure_callback': False, \n    'has_on_success_callback': False, \n    'has_task': <bound method DAG.has_task of <DAG: trial_dag>>, \n    'has_task_group': <bound method DAG.has_task_group of <DAG: trial_dag>>, \n    'infer_automated_data_interval': <bound method DAG.infer_automated_data_interval of <DAG: trial_dag>>, \n    'is_fixed_time_schedule': <bound method DAG.is_fixed_time_schedule of <DAG: trial_dag>>, \n    'is_paused': False, \n    'is_paused_upon_creation': None, \n    'is_subdag': False, \n    'iter_dagrun_infos_between': <bound method DAG.iter_dagrun_infos_between of <DAG: trial_dag>>, \n    'iter_invalid_owner_links': <bound method DAG.iter_invalid_owner_links of <DAG: trial_dag>>, \n    'jinja_environment_kwargs': None, \n    'last_loaded': datetime.datetime(2023, 8, 11, 12, 7, 12, 429629, tzinfo=Timezone('UTC')), \n    'latest_execution_date': datetime.datetime(2023, 8, 11, 12, 7, 10, 904583, tzinfo=Timezone('UTC')), \n    'leaves': [<Task(HelloOperator): task2>], \n    'log': <Logger ***.models.dag.DAG (INFO)>, \n    'logger': <bound method LoggingMixin.logger of <class '***.models.dag.DAG'>>, \n    'max_active_runs': 16, \n    'max_active_tasks': 16, \n    'next_dagrun_after_date': <bound method DAG.next_dagrun_after_date of <DAG: trial_dag>>, \n    'next_dagrun_info': <bound method DAG.next_dagrun_info of <DAG: trial_dag>>, \n    'normalize_schedule': <bound method DAG.normalize_schedule of <DAG: trial_dag>>, \n    'normalized_schedule_interval': None, \n    'on_failure_callback': None, \n    'on_success_callback': None, \n    'orientation': 'LR', \n    'owner': '***', \n    'owner_links': {}, \n    'param': <bound method DAG.param of <DAG: trial_dag>>, \n    'params': {}, \n    'parent_dag': None, \n    'partial': False, \n    'partial_subset': <bound method DAG.partial_subset of <DAG: trial_dag>>, \n    'pickle': <bound method DAG.pickle of <DAG: trial_dag>>, \n    'pickle_id': None, \n    'pickle_info': <bound method DAG.pickle_info of <DAG: trial_dag>>, \n    'previous_schedule': <bound method DAG.previous_schedule of <DAG: trial_dag>>, \n    'relative_fileloc': PosixPath('trial_dag.py'), \n    'render_template_as_native_obj': False, \n    'resolve_template_files': <bound method DAG.resolve_template_files of <DAG: trial_dag>>, \n    'roots': [<Task(BashOperator): task1>], \n    'run': <bound method DAG.run of <DAG: trial_dag>>, \n    'safe_dag_id': 'trial_dag', \n    'schedule_interval': '@once', \n    'set_dag_runs_state': <bound method DAG.set_dag_runs_state of <DAG: trial_dag>>, \n    'set_dependency': <bound method DAG.set_dependency of <DAG: trial_dag>>,\n    'set_edge_info': <bound method DAG.set_edge_info of <DAG: trial_dag>>, \n    'set_task_instance_state': <bound method DAG.set_task_instance_state of <DAG: trial_dag>>, \n    'sla_miss_callback': None, \n    'start_date': DateTime(2023, 8, 1, 0, 0, 0, tzinfo=Timezone('UTC')), \n    'sub_dag': <bound method DAG.sub_dag of <DAG: trial_dag>>, \n    'subdags': [], \n    'sync_to_db': <bound method DAG.sync_to_db of <DAG: trial_dag>>, \n    'tags': [], 'task': functools.partial(<***.decorators.TaskDecoratorCollection object at 0xffff7aafde10>, dag=<DAG: trial_dag>), \n    'task_count': 2, \n    'task_dict': {'task1': <Task(BashOperator): task1>, 'task2': <Task(HelloOperator): task2>}, \n    'task_group': <***.utils.task_group.TaskGroup object at 0xffff7be957d0>, \n    'task_group_dict': {}, \n    'task_ids': ['task1', 'task2'], \n    'tasks': [<Task(BashOperator): task1>, <Task(HelloOperator): task2>], \n    'template_searchpath': None, \n    'template_undefined': <class 'jinja2.runtime.StrictUndefined'>, \n    'test': <bound method DAG.test of <DAG: trial_dag>>, \n    'timetable': <***.timetables.simple.OnceTimetable object at 0xffff7bed0410>, \n    'timezone': Timezone('UTC'), \n    'topological_sort': <bound method DAG.topological_sort of <DAG: trial_dag>>, \n    'tree_view': <bound method DAG.tree_view of <DAG: trial_dag>>, \n    'user_defined_filters': None, \n    'user_defined_macros': None, \n    'validate': <bound method DAG.validate of <DAG: trial_dag>>, \n    'validate_schedule_and_params': <bound method DAG.validate_schedule_and_params of <DAG: trial_dag>>\n}\n\n\n# context[\"ti\"]\n{\n    'are_dependencies_met': <bound method TaskInstance.are_dependencies_met of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'are_dependents_done': <bound method TaskInstance.are_dependents_done of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'check_and_change_state_before_execution': <bound method TaskInstance.check_and_change_state_before_execution of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'clear_db_references': <bound method TaskInstance.clear_db_references of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'clear_next_method_args': <bound method TaskInstance.clear_next_method_args of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'clear_xcom_data': <bound method TaskInstance.clear_xcom_data of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'command_as_list': <bound method TaskInstance.command_as_list of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'current_state': <bound method TaskInstance.current_state of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'dag_id': 'trial_dag', \n    'dag_run': <DagRun trial_dag @ 2023-08-11 12:45:13.996317+00:00: manual__2023-08-11T12:45:13.996317+00:00, state:running, queued_at: 2023-08-11 12:45:14.009070+00:00. externally triggered: True>, \n    'dry_run': <bound method TaskInstance.dry_run of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'duration': None, \n    'email_alert': <bound method TaskInstance.email_alert of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'end_date': None, \n    'error': <bound method TaskInstance.error of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'execution_date': datetime.datetime(2023, 8, 11, 12, 45, 13, 996317, tzinfo=Timezone('UTC')), \n    'executor_config': {}, \n    'external_executor_id': None, \n    'filter_for_tis': <function TaskInstance.filter_for_tis at 0xffff7ed49050>, \n    'generate_command': <function TaskInstance.generate_command at 0xffff7ed3f200>, \n    'get_dagrun': <bound method TaskInstance.get_dagrun of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_email_subject_content': <bound method TaskInstance.get_email_subject_content of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_failed_dep_statuses': <bound method TaskInstance.get_failed_dep_statuses of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_num_running_task_instances': <bound method TaskInstance.get_num_running_task_instances of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_previous_dagrun': <bound method TaskInstance.get_previous_dagrun of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_previous_execution_date': <bound method TaskInstance.get_previous_execution_date of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_previous_start_date': <bound method TaskInstance.get_previous_start_date of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_previous_ti': <bound method TaskInstance.get_previous_ti of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_relevant_upstream_map_indexes': <bound method TaskInstance.get_relevant_upstream_map_indexes of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_rendered_k8s_spec': <bound method TaskInstance.get_rendered_k8s_spec of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_rendered_template_fields': <bound method TaskInstance.get_rendered_template_fields of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_template_context': <bound method TaskInstance.get_template_context of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'get_truncated_error_traceback': <function TaskInstance.get_truncated_error_traceback at 0xffff7ed46320>, \n    'handle_failure': <bound method TaskInstance.handle_failure of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'hostname': '7a02942b92a8', \n    'init_on_load': <bound method TaskInstance.init_on_load of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'init_run_context': <bound method TaskInstance.init_run_context of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'insert_mapping': <function TaskInstance.insert_mapping at 0xffff7ed37dd0>, \n    'is_eligible_to_retry': <bound method TaskInstance.is_eligible_to_retry of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'is_premature': False, \n    'is_trigger_log_context': False, \n    'job_id': '86', \n    'key': TaskInstanceKey(dag_id='trial_dag', task_id='task2', run_id='manual__2023-08-11T12:45:13.996317+00:00', try_number=1, map_index=-1), \n    'log': <Logger ***.task (INFO)>, \n    'log_url': 'http://localhost:8080/log?execution_date=2023-08-11T12%3A45%3A13.996317%2B00%3A00&task_id=task2&dag_id=trial_dag&map_index=-1', \n    'logger': <bound method LoggingMixin.logger of <class '***.models.taskinstance.TaskInstance'>>, \n    'map_index': -1, \n    'mark_success_url': 'http://localhost:8080/confirm?task_id=task2&dag_id=trial_dag&dag_run_id=manual__2023-08-11T12%3A45%3A13.996317%2B00%3A00&upstream=false&downstream=false&state=success', \n    'max_tries': 0, \n    'metadata': MetaData(), \n    'next_kwargs': None, \n    'next_method': None, \n    'next_retry_datetime': <bound method TaskInstance.next_retry_datetime of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'next_try_number': 2, \n    'operator': 'HelloOperator', \n    'overwrite_params_with_dag_run_conf': <bound method TaskInstance.overwrite_params_with_dag_run_conf of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'pid': 10659, \n    'pool': 'default_pool', \n    'pool_slots': 1, \n    'prev_attempted_tries': 1, \n    'previous_start_date_success': DateTime(2023, 8, 11, 12, 37, 0, 737891, tzinfo=Timezone('UTC')), \n    'previous_ti': None, \n    'previous_ti_success': <TaskInstance: trial_dag.task2 manual__2023-08-11T12:36:58.934316+00:00 [success]>, \n    'priority_weight': 1, \n    'queue': 'default', \n    'queued_by_job_id': 8, \n    'queued_dttm': datetime.datetime(2023, 8, 11, 12, 45, 15, 464793, tzinfo=Timezone('UTC')), \n    'raw': True, \n    'ready_for_retry': <bound method TaskInstance.ready_for_retry of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'refresh_from_db': <bound method TaskInstance.refresh_from_db of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'refresh_from_task': <bound method TaskInstance.refresh_from_task of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'registry': <sqlalchemy.orm.decl_api.registry object at 0xffff7ff9e650>, \n    'render_k8s_pod_yaml': <bound method TaskInstance.render_k8s_pod_yaml of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'render_templates': <bound method TaskInstance.render_templates of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'rendered_task_instance_fields': None, \n    'run': <bound method TaskInstance.run of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'run_as_user': None, \n    'run_id': 'manual__2023-08-11T12:45:13.996317+00:00', \n    'schedule_downstream_tasks': <bound method TaskInstance.schedule_downstream_tasks of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'set_duration': <bound method TaskInstance.set_duration of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'set_state': <bound method TaskInstance.set_state of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'start_date': datetime.datetime(2023, 8, 11, 12, 45, 15, 753572, tzinfo=Timezone('UTC')), \n    'state': 'running', \n    'task': <Task(HelloOperator): task2>, \n    'task_id': 'task2', \n    'test_mode': False, \n    'ti_selector_condition': <bound method TaskInstance.ti_selector_condition of <class '***.models.taskinstance.TaskInstance'>>, \n    'trigger_id': None, \n    'trigger_timeout': None, \n    'try_number': 1, \n    'unixname': '***', \n    'updated_at': datetime.datetime(2023, 8, 11, 12, 45, 15, 766080, tzinfo=Timezone('UTC')), \n    'xcom_pull': <bound method TaskInstance.xcom_pull of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>, \n    'xcom_push': <bound method TaskInstance.xcom_push of <TaskInstance: trial_dag.task2 manual__2023-08-11T12:45:13.996317+00:00 [running]>>\n}\n\n\n\n\n",
    "preview": "posts/2023-08-10-writing-airflow-dags/img/preview.png",
    "last_modified": "2023-08-13T22:54:30+00:00",
    "input_file": "writing-airflow-dags.knit.md"
  },
  {
    "path": "posts/2023-08-09-sample-youtube-channels-id-with-async-functions/",
    "title": "Sampling YouTube Channels' Id with Async Functions",
    "description": "A quick way to collect YouTube channels' id from YouTube home page asynchronously.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-09",
    "categories": [
      "python",
      "async",
      "web-scraping"
    ],
    "contents": "\n\nContents\nIntroduction\nAsyncio\nCoroutines\nTasks\nEvent loops\nSemaphore\n\nScraping YouTube asynchronously\n\nIntroduction\nWhen exchanging information between nodes (like making http request, or querying a database, etc.), the client side usually is required to wait for the response from the server. In synchronous programming, commands / statements are executed line by line, meaning that if a statement is required to wait for a particular amount of time, every other statements below cannot be executed because they are blocked by this long waiting statement to complete.\nFor example if we want to query two tables from two different databases and process them afterwords, in synchronous programming, we will need to wait for the first query to complete in order to start the second query. If the response time of the both queries are 10 seconds, we need to spend 20 seconds just for waiting. Asynchronous programming allows the processor to execute other tasks when a particular command is required to wait.\nThis article will first introduce the asyncio library in Python. Then we will use this method to sample channel ids from YouTube for later use.\nAsyncio\nCoroutines\nThe first thing in asynchronous programming is to create an async function. To do so in Python, we simply add the keyword async before the def keyword. Yet, you cannot simply execute an async function. When execute an async function, it always returns a coroutine object. A coroutine is an awaitable and therefore can be awaited from other coroutines. Notice that the await keyword can only be used in an async function.\nBelow shows the awaitability of synchronous (time.sleep) and asynchronous (foo) functions.\n\nimport time\nimport asyncio\n\nasync def foo1():\n    time.sleep(1)   # time.sleep is not an async function and cannot be awaited\n    print(\"Hello world!\")\n\ntype(foo1())\n\n# <stdin>:1: RuntimeWarning: coroutine 'foo1' was never awaited\n# RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n# <class 'coroutine'>\n\nasync def foo2():\n    await foo1()    # foo1 is an async function and can be awaited\n\nTo execute a coroutine object, asyncio.run is required.\n\nasyncio.run(foo1())\n\n# Hello world!\n\ntype(asyncio.run(foo1()))\n\n# Hello world!\n# <class 'NoneType'>\n\nTasks\nTasks are used to run coroutines in event loops. In other words, a coroutine is required to be wrapped into a task in order to pass to an event loop for execution. A task can be created via asyncio.create_task or loop.create_task methods. Whenever a task is created via these methods, they will execute whenever the running theread is available. Besides, if we do not await a task to be finished, the task will be dropped when the last line of the async function is executed.\n\nimport asyncio\n\nasync def foo1():\n    for i in range(10):\n        print(i)\n        await asyncio.sleep(0.5)\n\nasync def foo2():\n    await asyncio.sleep(2)\n    print(\"Hi from foo2\")\n\nasync def main():\n    task1 = asyncio.create_task(foo1())\n    task2 = asyncio.create_task(foo2())\n    \n    # The above tasks will not be executed before this line\n    # Because the running thread is not available before this\n    print(\"The thread is busy before this\")  \n    \n    await task2    # this task will be awaited. But not task1\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n\n# The thread is busy before this\n# 0\n# 1\n# 2\n# 3\n# Hi from foo2\n\nEvent loops\nAn event loop is an object to manage a list of tasks. It identifies if the running thread is available for execute other tasks. The method asyncio.run simply acquires an event loop, convert the coroutine into a task, execute it and closing the threadpool. Below shows the same example as above but better illustrate the event loop.\n\nimport asyncio\n\nasync def foo1():\n    for i in range(10):\n        print(i)\n        await asyncio.sleep(0.5)\n\nasync def foo2():\n    await asyncio.sleep(2)\n    print(\"Hi from foo2\")\n\nasync def main():\n    task1 = asyncio.create_task(foo1())\n    task2 = asyncio.create_task(foo2())\n\n    print(\"The thread is busy before this\")\n    \n    await task2\n    print(type(asyncio.get_event_loop()))\n\nif __name__ == \"__main__\":\n    loop = asyncio.get_event_loop()  # Acquire an event loop\n    loop.run_until_complete(main())  # Execute the tasks\n    loop.close()                     # Close the loop\n\nSemaphore\nBy default, an event loop will put all the tasks to execute whenever there are resources. But sometimes we want to limit the number of concurrent tasks. In this case, asyncio.Semaphore may come in handy.\n\nimport asyncio\n\nasync def async_func(task_no, sem):\n    async with sem:\n        print(f'{task_no} :Hello ...')\n        await asyncio.sleep(1)\n        print(f'{task_no}... world!')\n\nasync def main(sem):\n    tasks = [\n        asyncio.create_task(async_func(x, sem)) for x in [\"t1\", \"t2\", \"t3\"]\n    ]\n    await asyncio.gather(*tasks)\n\nif __name__ == \"__main__\":\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    sem = asyncio.Semaphore(2)\n    result = asyncio.run(main(sem))\n    loop.close()\n\n# t1 :Hello ...\n# t2 :Hello ...\n# t1... world!\n# t2... world!\n# t3 :Hello ...\n# t3... world!\n\nScraping YouTube asynchronously\nIn this section, the code snippet below sample channel ids from YouTube main page using async functions. I am using httpx instead of requests because the later library does not provide async http client. I also set the cookie \"CONSENT\": \"YES+yt.463627267.en-GB+FX+553\" to consent YouTube using cookies tracking. Yet, I remove all cookies except this one every time I make the request to avoid YouTube gives me the same channels. Lastly, I use regex to extract all the channel ids from the html.\nThe whole script only used 7.7 seconds to complete 100 requests, which is much faster to use the synchronous client.\n\nimport re\nimport httpx\nimport asyncio\nfrom asyncio import Semaphore, create_task, gather\nfrom httpx import AsyncClient\nfrom time import time\nfrom itertools import chain\n\n\nasync def async_request_youtube(sem: Semaphore, client: AsyncClient):\n    async with sem:\n        client.cookies = {\"CONSENT\": \"YES+yt.463627267.en-GB+FX+553\"} \n        client.headers = {\n            'accept': '*/*', \n            'accept-encoding': 'gzip, deflate', \n            'connection': 'keep-alive', \n            'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.5400.117 Safari/537.36'\n        }\n\n        response = await client.get(\"https://www.youtube.com\")\n        return response\n\nasync def sample_youtube_channels(n: int, sem: Semaphore):\n    async with httpx.AsyncClient() as client:\n        client.timeout = 10\n        tasks = [\n            create_task(async_request_youtube(sem, client)) for i in range(n)\n        ]\n        result = await gather(*tasks)\n    \n    return result\n\ndef request_youtube(n: int, concurrency: int = 50):\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    sem = asyncio.Semaphore(concurrency)\n    result = asyncio.run(sample_youtube_channels(n, sem))\n    loop.close()\n    return result\n\nif __name__ == \"__main__\":\n    start_ts = time()\n    result = request_youtube(100, 50)\n    channel_id_re = re.compile('\"(UC[A-z0-9_-]{22})\"')\n    channel_ids = [\n        channel_id_re.findall(res.text) for res in result\n    ]\n    channel_ids = list(set(list(chain(*channel_ids))))\n    with open(\"./youtube_channel_ids.txt\", \"w\") as f:\n        for cid in channel_ids:\n            _ = f.write(f\"{cid}\\n\")\n    end_ts = time()\n    print(f\"Time used: {end_ts - start_ts} seconds\")\n\n# Time used: 7.707166910171509 seconds\n\n\n\n\n",
    "preview": "posts/2023-08-09-sample-youtube-channels-id-with-async-functions/img/preview.png",
    "last_modified": "2023-08-13T14:36:09+00:00",
    "input_file": "sample-youtube-channels-id-with-async-functions.knit.md"
  },
  {
    "path": "posts/2023-08-08-recursive-programming-querying-nested-json-in-python/",
    "title": "Recursive Programming - Querying nested JSON in Python",
    "description": "Querying nested JSON in Python with recursive programming.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-08",
    "categories": [
      "python",
      "recursive-programming"
    ],
    "contents": "\n\nContents\nIntroduction\nRecursive function\nQuerying nested JSON dictionary\nExamples\nExample 1\nExample 2\nExample 3\n\n\nIntroduction\nAs we are requesting REST API heavily nowadays, we need to deal with JSON frequently. JSON objects can be transfer into Python dictionaries very easily by the json library. Yet, it is quite tedious to query nested JSON if it contains many layers.\n\nd = {\n    \"layer1_item1\": {\"layer2_item1\": {\"layer3_item1\": \"some_info\", \"layer3_item2\": \"another_info\"}},\n    \"layer1_item2\": {\"layer2_item1\": {\"layer3_item1\": \"some_info\", \"layer3_item2\": \"another_info\"}}\n}\n\nFor example if we want to query layer3_item2 from the above JSON,\n\nd.get(\"layer1_item1\").get(\"layer2_item1\").get(\"layer3_item2\")\n'another_info'\n\nIt involves so much brackets and quotes. I have written a recursive function to query these nested JSON dictionary in Python. But we will first take a look what a recursive functions is.\nRecursive function\nA recursive function means the function execute itself within its own definition. A simple but endless definition is shown below.\n\ndef foo():\n    foo()\n\nThe function foo calls itself within its own definition. It simply creates a loop (but an endless one in the above example).\nWe now take a look on a practical factorial function. Recall the factorial of \\(n\\) is given by\n\\[n! = n \\cdot (n - 1) \\cdot (n - 2) \\cdots 3 \\cdot 2 \\cdot 1.\\]\nIn Python, we can illustrate this by\n\ndef factorial(n):\n    if n == 1:\n        return n\n    return n * factorial(n - 1)\n\n\nfactorial(5)\n120\n\nQuerying nested JSON dictionary\nThe following function can query nested dictonary with syntax like key1.key2.key3 instead of multiple get, brackets and quotes. See some examples in the coming section.\n\nfrom typing import Any, Optional\n\ndef pjq(\n    json_dict: 'list | dict', \n    query: 'list[str] | str', \n    default: Optional[Any] = None, \n    sep: str = \".\", \n    idx_sep: str = \",\", \n    trim: bool = True, \n    prev_q: Optional[str] = None\n) -> Any:\n    query = query.split(sep) if isinstance(query, str) else query\n    # Cannot pop query index otherwise affecting the for loop in list\n    q: str = query[0]\n    query: list[str] = query[1:]\n    if json_dict == default:\n        # If `default` is set to a list of dict, it cannot go through this\n        return default\n\n    elif isinstance(json_dict, dict):\n        json_dict = json_dict.get(q, default)\n        \n        if query:\n            return pjq(json_dict, query, default, sep, idx_sep, trim=trim, prev_q=q)\n        return json_dict\n\n    elif isinstance(json_dict, list):\n        if q:\n            try:\n                idx: list[int] = [int(i) for i in q.split(idx_sep)]\n                json_dict = [json_dict[i] for i in idx]\n            except Exception:\n                return default\n        if query:\n            json_dict = [pjq(jd, query, default, sep, idx_sep, trim=trim, prev_q=q) for jd in json_dict]\n\n        if trim:\n            json_dict = json_dict[0] if len(json_dict) == 1 else json_dict\n\n        return json_dict\n\n    else:\n        return None \n\nExamples\nExample 1\n\nd = {\n    \"a\": {\n        \"b\": {\"b1\": 1, \"b2\": 2},\n        \"c\": {\"c1\": 3, \"c2\": 4}\n    }\n}\n\n\npjq(d, \"a.b.b2\")\n2\n\nExample 2\n\nd = {\n    \"a\": [\n        {\"b1\": 1, \"b2\": 2},\n        {\"b1\": 3, \"b2\": 4},\n        {\"b1\": 5, \"b2\": 6}\n    ]\n}\n\n\npjq(d, \"a.1,2.b2\")\n[4, 6]\n\n\npjq(d, \"a..b2\")\n[2, 4, 6]\n\nExample 3\n\nd = [\n    {\"a\": {\"b\": {\"x\": 1}, \"c\": {\"y\": 3}}},\n    {\"a\": {\"b\": {\"x\": 2}, \"c\": {\"y\": 4}}}\n]\n\n\npjq(d, \".a.c.y\")\n[3, 4]\n\n\n\n\n",
    "preview": "posts/2023-08-08-recursive-programming-querying-nested-json-in-python/img/preview.png",
    "last_modified": "2023-08-09T09:33:24+00:00",
    "input_file": "recursive-programming-querying-nested-json-in-python.knit.md"
  },
  {
    "path": "posts/2023-08-05-introduction-to-docker-with-airflow-and-postgres-stack/",
    "title": "Introduction to Docker with Airflow and Postgres stack",
    "description": "Deploying Docker stacks of Airflow and Postgres database with docker-compose.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-05",
    "categories": [
      "docker",
      "airflow",
      "postgres"
    ],
    "contents": "\n\nContents\nIntroduction\nDocker Compose YAML File\nService Name\nImage Name\nPorts\nEnvironment\nVolumes\nAnchor\nStarting containers\n\nAirflow and Postgres stack\nObtaining docker compose YAML template\nCustomise the YAML file (optional)\nCreate initial databases and tables (optional)\nStart the stack\n\n\nIntroduction\nDocker is a software hosting containers. A container is an isolated place on top of the running operating system (OS) where an application can run without affecting other applications on the OS. Developers have been creating images to run on Docker. An image is a template of an application, like Postgres, that are ready to run in a container. It allows users to take the image and run it in seconds in a container and save tremendous times to install the application from scratch.\nIn this document, we will install a stack of containers to run Apache Airflow, a workflow management tool, and Postgres database. In later articles, we will be writing ETL workflows on Airflow using this stack. You can find the code source from my Github repo. If you have not installed Docker and Docker Compose, please follow the documentation below:\nDocker: https://www.docker.com/products/docker-desktop/\nDocker Compose: https://docs.docker.com/compose/install/\nWe will first describe how to tell Docker what images we want to use in a docker-compose.yaml specification file. Then we will setup Airflow and Postgres as an example.\nDocker Compose YAML File\nIn this section, we will briefly introduce how to specify what images to use in a docker-compose.yaml. We first take a look on a sample of the YAML file for creating a Postgres container.\n\nversion: '3.8'           # Version of the specification file \n\nservices: \n  mypostgres:            # Name of the local container\n    image: postgres:13   # image_name:version\n    ports: \n      - \"5431:5432\"      # [local port]:[cotainer port]\n    environment:         # The key-value pairs depends on image\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-password}\n      POSTGRES_DB: airflow\n    volumes:\n      - /path/in/local:/path/in/container\n      - /another/path:/another/path\n    \n  myairflow:             # Another container\n    ...\n\nA full documentation on the sections can be found here. Below we will only introduce some of them.\nService Name\nFrom the above example, we have specified 2 containers, one is mypostgres and the other is myairflow. These are the local names of the containers, where you can see them from\n\ndocker container ls\n\nImage Name\nUnder the block mypostgres, we have specified to use the image postgres:13. We can find published images from https://hub.docker.com/ and put the imageâ€™s name in docker-compose.yaml. We can also specify the version of the image. In this example, we are using version 13.\n\n\n\nPorts\nWe have also put 5431:5432 as the port configuration, meaning that we can use the local port 5431 to connect to the port 5432 of the container. In other words, we can connect the database via the following command\n\npsql postgres://username:password@localhost:5431\n\nEnvironment\nThe environment section stores the environment variables to be passed to the container. In this example, we are passing POSTGRES_USER, POSTGRES_PASSWORD and POSTGRES_DB to the container. The image will pick up these environment variables to build the Postgres database in the container.\nWhen we are passing sensitive information like the password in this example, we can create and store key-value pairs in .env file besides the docker-compose.yaml. Then we can use the keys defind in the .env file in the specification file. The expression ${POSTGRES_PASSWORD:-password} means it will look for the key-value pair inside .env and password will be use instead if the key-value pair cannot be found. Below shows and example of the .env file.\n\nPOSTGRES_PASSWORD=\"AnotherPassword\"\n\nVolumes\nWe can share files and directories under the volumes session with the below format.\n\n/path_to_local_file:/path_to_container_file\n\nAnchor\nWhen working with a stack of containers, it is common to repeated use some configuration. For example to put the same environment variables into different containers. From the example below, we can both the services airflow-scheduler and airflow-website share the same set of configuration on image and environment from the airflow-common configuration.\n\nx-airflow-common:\n  &airflow-common\n  image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.6.3}\n  environment:\n    AIRFLOW__CORE__EXECUTOR: LocalExecutor\n    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow\n  ...\n  \nservices:\n  airflow-scheduler:\n    <<: *airflow-common\n    command: scheduler\n    ...\n    \n  airflow-webserver:\n    <<: *airflow-common\n    command: webserver\n    ...\n\nStarting containers\nWe can start a single service from the stack with the following command:\n\ndocker-compose up <service-name>\n\nOr start the whole stack with the following command:\n\ndocker-compose up --build -d\n\nThe option -d lets the stack to run in background. We can check the status of each container with the following command:\n\ndocker container ls\n\n# CONTAINER ID   IMAGE                  ...   PORTS                    NAMES\n# 4c26970df34a   postgres:13            ...   0.0.0.0:5431->5432/tcp   postgres-mypostgres-1\n# 05365378f0f0   apache/airflow:2.6.3   ...   8080/tcp                 yt-docker-stack-airflow-triggerer-1\n# cf43be7e9755   apache/airflow:2.6.3   ...   8080/tcp                 yt-docker-stack-airflow-scheduler-1\n# 12fdbf84054e   apache/airflow:2.6.3   ...   0.0.0.0:8081->8080/tcp   yt-docker-stack-airflow-webserver-1\n# 73cfeea83789   postgres:13            ...   0.0.0.0:5432->5432/tcp   yt-docker-stack-postgres-1\n\nWe can also go into the bash environment of a container with the following command:\n\ndocker exec -it <container_id | container_name> bash\n\nFinally, if we want to close the whole stack of containers, we can use the following command:\n\ndocker-compose down -v\n\nThe option -v removes all the volumes used by the stack.\nAirflow and Postgres stack\nIn this section, we will show an example to build a stack of containers with Apache Airflow and Postgres database.\nObtaining docker compose YAML template\nWe need to first obtain a docker-compose.yaml template file from here. I was using Airflow version 2.6.3 at the moment. Hence the YAML file is\nhttps://airflow.apache.org/docs/apache-airflow/2.6.3/docker-compose.yaml\nCustomise the YAML file (optional)\nIn this example, I have customised the YAML file to fit my purposes. You can find my specification file from Github.\nI will be using the LocalExecutor instead of the CeleryExecutor. Hence I have changed the environment variable AIRFLOW__CORE__EXECUTOR from CeleryyExecutor to LocalExecutor under the airflow-common section, and removed the following items:\n\n# THE FOLLOWING SECTIONS ARE REMOVED\nx-airflow-common:\n  environment:\n    AIRFLOW__CELERY__RESULT_BACKEND: ...\n    AIRFLOW__CELERY__BROKER_URL: ...\n  depends_on:\n    redis:\n      ...\n\nservices:\n  redis:\n    ...\n\n  airflow-worker:\n    ...\n  \n  flower:\n    ...\n\nI have also changed / added the followings:\n\n# THE FOLLOWING VALUES ARE CHANGED / ADDED\nx-airflow-common:\n  environment:\n    AIRFLOW__CORE__EXECUTOR: LocalExecutor\n    AIRFLOW_INPUT_DIR: '/opt/airflow/dag-inputs'\n    POSTGRES_USER: airflow\n    POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n    GAPI_KEY: ${GAPI_KEY}\n    \nservices:\n  postgres:\n    ports:\n      - \"5432:5432\"\n    environment:\n      POSTGRES_USER: airflow\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: airflow\n    volumes:\n      - postgres-db-volume:/var/lib/postgresql/data\n      - ./init/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql\n      - ./init/postgres:/mnt/sources/init\n  \n  airflow-init:\n    environment:\n      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}\n      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_PASSWORD}\n\nCreate initial databases and tables (optional)\nThe Postgres docker image will execute the files in /docker-entrypoint-initdb.d during initialisation. One can put some queries in this folder for initial execution, which is why I share some volumes in the above section.\n\nCREATE DATABASE youtube;\n\\c youtube;\n\\i /mnt/sources/init/enum_iso3166.sql;\nCREATE TABLE channel (\n    uuid UUID default gen_random_uuid() NOT NULL,\n    etag CHAR(27) NOT NULL,\n    id CHAR(24) NOT NULL,\n    title VARCHAR(100) NOT NULL,\n    \"description\" VARCHAR(5000),\n    custom_url VARCHAR(31),\n    published_at Timestamp,\n    country country_alpha2,\n    uploads CHAR(24),\n    view_count INTEGER,\n    subscriber INTEGER,\n    video_count INTEGER,\n    topic_category TEXT[],\n    updated_at TIMESTAMP NOT NULL default CURRENT_TIMESTAMP\n);\n\nStart the stack\nAs described in previous sections, I now start the stack with the following cammands:\n\ndocker-compose up airflow-init\n\ndocker-compose up --build -d\n\nAnd you can find Airflow is running on http://localhost:8080 or via docker container ls.\n\n\n\n\n\n\n",
    "preview": "posts/2023-08-05-introduction-to-docker-with-airflow-and-postgres-stack/img/preview.png",
    "last_modified": "2023-08-08T13:07:36+00:00",
    "input_file": "introduction-to-docker-with-airflow-and-postgres-stack.knit.md"
  },
  {
    "path": "posts/2023-08-03-github-actions-with-example/",
    "title": "Github Actions with Example",
    "description": "An introduction to Github Actions with an example to write a post whenever a new blog post is merged to the main branch.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-08-03",
    "categories": [
      "continuous-delivery",
      "cicd",
      "github-actions",
      "github"
    ],
    "contents": "\n\nContents\nIntroduction\nGithub Actions\nWorkflows\nEvents\nJobs\nActions\nRunners\nSecrets\nGithub Context\n\nLinkedIn API\nOAuth2\nCalling API\nIdentify User Id\nWrite Post\n\n\nAuto Posting Workflow\nThe Workflow\nThe Python Script\n\n\n\n\n\n\n\n\n\n\n\nIntroduction\nIn modern software development, an engineerâ€™s job does not end when a product is developed. Numerous times are spent on testing and deploying the product, no matter if the product is a website or a programming library or anything. Usually these tasks are repetitive and boring because these products are required to be maintained and updated. The same testing and deploying process will need to be rerun again throughout the life-cycle of the product.\nThe same problem happens on data scientists and machine learning engineers as well, where the models they have developed are also required to be tested and deployed (and updated and tested and deployed again and again). The concept of continuous integration and delivery came to automate these repetitive tasks and saves our precious time.\nThis article describes these concepts through an example â€“ write a LinkedIn post whenever a new blog post is created in this blog. We will first briefly go through what Github Actions is, then we will talk about how to write a post on LinkedIn through its API. Finally we will create a workflow to check if there is a new blog post and write a LinkedIn post if there is.\nGithub Actions\nGithub Actions is a platform for continuous integration / continuous delivery (CI/CD). One can write workflows to automate build, testing, and deployment pipelines. Each workflow is triggered by one or more events and can be run by different runners. We will describe these concepts more below.\nEach workflow must be defined in the folder of .github/workflows in a repo and it must be specified in a YAML file like below. We will go through each section of the file.\n\n\n# Workflow Name\nname: Release Process\n\non:\n  # Events\n  push:                                   # One event\n    branches:\n      - main\n\n  workflow_run:                           # Another event\n    workflows: [pages-build-deployment]\n    types: \n      - completed\n\njobs:\n  # Job\n  generate-release:                 # Job id\n    name: Create GitHub Release     # Job name\n    runs-on: ubuntu-latest          # Runner\n    steps:\n    - name: Checkout Repository     # Step1\n      uses: actions/checkout@v2     # Actions\n      \n    - name: Run release code        # Step2\n      run: |\n        cd /target/directory\n        ./run-release-code\n  \n  # Another Job\n  another-job:                      # Job id\n    name: Another Job               # Job name\n    needs: [generate-release]       # Requires the job to complete successfully\n    runs-on: ubuntu-latest          # Runner\n    steps:\n    - name: Checkout Repository     # Step1\n      uses: actions/checkout@v2     # Actions\n      \n    - name: do other stuffs         # Step2\n      run: echo $CUSTOM_VAR\n      env: \n        CUSTOM_VAR: \"${{ secrets.CUSTOM_VAR }}\" # Secret value\n\n\nWorkflows\nThe entire YAML file specified in this code chunk is a workflow. There can be multiple workflows in different YAML files stored inside .github/workflows directory. Each workflow can be triggered by one or more events, or they can be triggered manually, or at a defined schedule. Each workflow can also contains one or more jobs.\nEvents\nAn event is an activity within the repository. For example, an event can be a pull / push request. It can also be the completion of another workflow or scheduled by cron syntax.\nThe above workflow will be triggered whenever one of the two specified events occurs. These two events are\nEvery time the main branch is pushed or merged from another branch, this workflow will be started.\nWhenever another workflow pages-build-deployment is completed, this workflow will be started.\nJobs\nA job is a series of steps that will be executed on the same runner. Each step is either a shell script or an action. The steps will be executed in order and dependent on each other. By default, each job will be run by a different runner and concurrently. One can specify the dependency of jobs by the key needs. The above example shows an implementation.\nAlso, one can also specify a strategy matrix to repeat the same job for different conditions. For example, the following job will be executed 6 times, namely\n{node-version: 10, os: ubuntu-22.04}\n{node-version: 10, os: ubuntu-20.04}\n{node-version: 12, os: ubuntu-22.04}\n{node-version: 12, os: ubuntu-20.04}\n{node-version: 14, os: ubuntu-22.04}\n{node-version: 14, os: ubuntu-20.04}\n\njobs:\n  example_matrix:\n    strategy:\n      matrix:\n        os: [ubuntu-22.04, ubuntu-20.04]\n        version: [10, 12, 14]\n    runs-on: ${{ matrix.os }}\n    steps:\n      - uses: actions/setup-node@v3\n        with:\n          node-version: ${{ matrix.version }}\n\nActions\nActions are custom applications for GitHub Actions that perform complex but repetitive tasks. You can write an action from scratch or use an existing action available from the GitHub Marketplace in your workflow.\nRunners\nA runner is an OS on a virtual machine or container to execute a specific job. GitHub provides Ubuntu Linux, Microsoft Windows, and macOS runners to run the workflows. One can also host their own machine as runner.\nSecrets\nFor each step or job, one can specify an env session to define environment variables. But if we are dealing with credentials, this might not be a good choice. One can go to Settings of the repository, under Security, click Secrets and variables, then click Actions. Inside the page, one can define secrets for the repository and can access them within the env session inside a workflow as shown in the example.\nGithub Context\nContexts are a way to access information about workflow runs, variables, runner environments, jobs, and steps. For example the name of the working branch, the working directory of Github Actions, etc. The keyword secrets in the above section is also a context. See more from this page.\nLinkedIn API\nLinkedIn offers various API products for consumers to do various of things. One of which is to write posts on behalf of the users (see this documentation). To do that, we need to\nCreate a company on LinkedIn\nCreate an application on behalf of the company\nAuthenticate yourself and authorise the application to write posts on behalf of you\nThe process is similar to my previous blog post about OAuth2 for Google APIs. I will briefly describe the process here.\nOAuth2\nWe will first create a company on LinkedIn and the application.\nGo to https://developer.linkedin.com/ and click Create App (and login to your LinkedIn account)\nEnter the name of the application\nClick Create a new LinkedIn Page if you do not have a company on LinkedIn\nSelect Company\nEnter the name of the company, select the industry, company size, company type. Check the terms and click Create page\nGo back to the developer page and select the company just created\nUpload a logo for the application\nCheck the Legal agreement and click Create app\nClick Verify and follow the instruction\nClick Products, click Request access for both Share on LinkedIn and Sign in with LinkedIn\nClick Auth and copy the Client ID and Client Secret\nUnder OAuth 2.0 settings, enter the authorised redirect url\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we have the client_id, client_secret and redirect_uri ready, we can now authenticate ourselves and authorise the application. The following script will generate a url to login to your LinkedIn account. Then it will generate the access_token.\n\nimport os\nfrom urllib.parse import urlencode, urlparse\nfrom http.server import HTTPServer, BaseHTTPRequestHandler\nimport json\nimport requests\nimport webbrowser\n\nclient_id = os.getenv(\"CLIENT_ID\")\nclient_secret = os.getenv(\"CLIENT_SECRET\")\nredirect_uri = os.getenv(\"REDIRECT_URI\")\nscope = \"r_liteprofile w_member_social openid profile email\"\n\ndef parse_query(path):\n    parsed_url = urlparse(path)\n    query = parsed_url.query.split(\"&\")\n    query = [x.split(\"=\") for x in query]\n    query = {x[0]: x[1] for x in query}\n    return query\n\ndef auth_code(code, client_id, client_secret, redirect_uri):\n    params = {\n        \"grant_type\": \"authorization_code\",\n        \"code\": code,\n        \"redirect_uri\": redirect_uri,\n        \"client_id\": client_id,\n        \"client_secret\": client_secret\n    }\n    headers = {\n        \"content-type\": \"application/x-www-form-urlencoded\",\n        \"content-length\": \"0\"\n    }\n    url = \"https://www.linkedin.com/oauth/v2/accessToken\"\n    response = requests.post(url, params=params, headers=headers)\n    response.raise_for_status()\n    content = response.json()\n    return content\n\nclass NeuralHTTP(BaseHTTPRequestHandler):\n    def do_GET(self):\n        path = self.path\n        query = parse_query(path)\n\n        code = query.get(\"code\")\n        if code:\n            status_code = 200\n            content = auth_code(\n                code=query.get(\"code\"),\n                client_id=client_id,\n                client_secret=client_secret,\n                redirect_uri=redirect_uri\n            )\n            print(json.dumps(content, indent=4))\n        else:\n            status_code = 400\n            content = {\n                \"error\": \"code not found\"\n            }\n\n        self.send_response(status_code)\n        self.send_header(\"Content-Type\", \"application/json\")\n        self.end_headers()\n        self.wfile.write(bytes(json.dumps(content, indent=4), \"utf-8\"))\n    \n    def log_message(self, format, *args):\n        \"\"\"Silence log message. Can be ignored.\"\"\"\n        return\n\nif __name__ == \"__main__\":\n    with HTTPServer((\"127.0.0.1\", 8088), NeuralHTTP) as server:\n        auth_url = \"https://www.linkedin.com/oauth/v2/authorization\"\n        params = {\n            \"client_id\": client_id,\n            \"response_type\": \"code\",\n            \"redirect_uri\": redirect_uri,\n            \"scope\": scope,\n        }\n\n        url = f\"{auth_url}?{urlencode(params)}\"\n        webbrowser.open(url)\n        server.handle_request()\n\n\n# {\n#     \"access_token\": \"...\",\n#     \"expires_in\": 5183999,\n#     \"scope\": \"email,openid,profile,r_liteprofile,w_member_social\",\n#     \"token_type\": \"Bearer\",\n#     \"id_token\": \"...\"\n# }\n\nCalling API\nIdentify User Id\nTo write a post on LinkedIn, We need to first identify the authorâ€™s user_id. A GET request to https://api.linkedin.com/v2/userinfo with the access_token obtained from the above are needed.\n\nimport os\nimport requests \nimport json\n\nurl = \"https://api.linkedin.com/v2/userinfo\"\ntoken = os.getenv(\"LINKEDIN_ACCESS_TOKEN\")\n\nheaders = {\"Authorization\": f\"Bearer {token}\"}\n\nresponse = requests.get(url, headers=headers)\nresponse.raise_for_status()\ncontent = response.json()\nprint(json.dumps(content, indent=4))\n\n\n{\n    \"sub\": \"....\",\n    \"email_verified\": true,\n    \"name\": \"Wilson Yip\",\n    \"locale\": {\n        \"country\": \"US\",\n        \"language\": \"en\"\n    },\n    \"given_name\": \"Wilson\",\n    \"family_name\": \"Yip\",\n    \"email\": \"wilsonyip@elitemail.org\",\n    \"picture\": \"https://media.licdn.com/dms/image/C4E03AQGo1BKbUYmyBA/profile-displayphoto-shrink_100_100/0/1646639382257?e=1696464000&v=beta&t=6lhHrDK3vx6GOC01wIKkfVYAmCiSWoZtc8XpE0JoUmM\"\n}\n\nThe user_id is stored in the sub value.\nWrite Post\nWe will be calling the Share in LinkedIn endpoint to write a post in LinkedIn along with the specific request body to attach an article to the post. The following scripts shows an example.\n\nimport os\nimport requests \n\ndef build_post_body(\n    user_id, \n    post_content, \n    media_title, \n    media_description, \n    article_url\n):\n    body = {\n        \"author\": f\"urn:li:person:{user_id}\",\n        \"lifecycleState\": \"PUBLISHED\",\n        \"specificContent\": {\n            \"com.linkedin.ugc.ShareContent\": {\n            \"shareCommentary\": {\n                    \"text\": post_content\n                },\n                \"shareMediaCategory\": \"ARTICLE\",\n                \"media\": [\n                    {\n                        \"status\": \"READY\",\n                        \"description\": {\n                            \"text\": media_description\n                        },\n                        \"originalUrl\": article_url,\n                        \"title\": {\n                            \"text\": media_title\n                        }\n                    }\n                ]\n            }\n        },\n        \"visibility\": {\n            \"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"\n        }\n    }\n    return body\n\nif __name__ == \"__main__\":\n    linkedin_user_id = os.getenv(\"LINKEDIN_USER_ID\")    # user_id \n    linkedin_token = os.getenv(\"LINKEDIN_TOKEN\")        # access_token\n    linkedin_post_endpoint = \"https://api.linkedin.com/v2/ugcPosts\"\n\n    headers = {\n        \"X-Restli-Protocol-Version\": \"2.0.0\",\n        \"Authorization\": \"Bearer \" + linkedin_token \n    }\n\n    body = build_post_body(\n        user_id=linkedin_user_id,\n        post_content=\"Content of the LinkedIn post\",\n        media_title=\"The title of the article\",\n        media_description=\"The description of the article\",\n        article_url=\"https://www.link-to-article.com/article\"\n    )\n\n    response = requests.post(\n        url=linkedin_post_endpoint, \n        json=body, \n        headers=headers\n    )\n\nAuto Posting Workflow\nA workflow is created to write a post on LinkedIn whenever there is a new article merged to the main branch of a repository. The workflow is triggered every time after completion of the pages-build-deployment workflow, which is the workflow to build the website. Yet, there is a problem:\n\nWe need to keep tract which article was posted to LinkedIn already in order to define which article is new.\n\nFor simplicity, I have created a Google Sheet to store the article paths and the corresponding LinkedIn post_id. If an articleâ€™s path does not appear in the table, that is the new article and will further trigger the scripts.\nThe workflow is quite simple. It just runs a Python file. The Python file will check if there are any new article path, write a LinkedIn post if there is one, and update the log file.\nThe Workflow\n\nname: create-linkedin-post\n\non:\n  workflow_run:\n    workflows: [pages-build-deployment]\n    types: \n      - completed\n\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n\n    steps:\n      - name: Chekcout\n        uses: actions/checkout@v3\n      \n      - name: Install python dependencies\n        run: pip install pyyaml\n      \n      - name: Create Linkedin Post\n        run: python ./tools/cd/linkedin_post.py\n        env: \n          LINKEDIN_USER_ID: ${{ secrets.LINKEDIN_USER_ID }}\n          LINKEDIN_TOKEN: ${{ secrets.LINKEDIN_TOKEN }}\n          GCP_CLIENT_EMAIL: ${{ secrets.GCP_CLIENT_EMAIL }}\n          GCP_PRIVATE_KEY_ID: ${{ secrets.GCP_PRIVATE_KEY_ID }}\n          GCP_PRIVATE_KEY: ${{ secrets.GCP_PRIVATE_KEY }}\n          LINKEDIN_POSTS_LOG_SSID: ${{ secrets.LINKEDIN_POSTS_LOG_SSID }}\n          LINKEDIN_POSTS_LOG_RANGE: ${{ secrets.LINKEDIN_POSTS_LOG_RANGE }}\n\nThe Python Script\n\n#!/usr/bin/python\n\nimport os\nimport requests \nimport json \nfrom time import time\nimport jwt\nimport yaml\n\ndef build_post_body(\n    user_id, \n    post_content, \n    media_title, \n    media_description, \n    article_url\n):\n    body = {\n        \"author\": f\"urn:li:person:{user_id}\",\n        \"lifecycleState\": \"PUBLISHED\",\n        \"specificContent\": {\n            \"com.linkedin.ugc.ShareContent\": {\n            \"shareCommentary\": {\n                    \"text\": post_content\n                },\n                \"shareMediaCategory\": \"ARTICLE\",\n                \"media\": [\n                    {\n                        \"status\": \"READY\",\n                        \"description\": {\n                            \"text\": media_description\n                        },\n                        \"originalUrl\": article_url,\n                        \"title\": {\n                            \"text\": media_title\n                        }\n                    }\n                ]\n            }\n        },\n        \"visibility\": {\n            \"com.linkedin.ugc.MemberNetworkVisibility\": \"PUBLIC\"\n        }\n    }\n    return body\n\ndef find_latest_missing_post(page_posts, linkedin_posts):\n    page_post_paths = [x.get(\"path\") for x in page_posts]\n    linkedin_post_paths = [x.get(\"path\") for x in linkedin_posts]\n    missing_idx = [\n        i for i, x in enumerate(page_post_paths) if x not in linkedin_post_paths\n    ]\n    \n    if missing_idx:\n        missing_paths = [page_post_paths[i] for i in missing_idx]\n        missing_post_dates = [page_posts[i].get(\"date\") for i in missing_idx]\n        latest_missing_post = missing_paths[missing_post_dates.index(max(missing_post_dates))]\n        latest_missing_post = page_posts[page_post_paths.index(latest_missing_post)]\n    else:\n        latest_missing_post = None\n\n    return latest_missing_post\n\ndef read_rmd_yml(path):\n    with open(path, \"r\") as f:\n        rmd_yml = f.readlines()\n    \n    yml_idx = [i for i, x in enumerate(rmd_yml) if x == \"---\\n\"]\n    return yaml.safe_load(\"\".join(rmd_yml[(yml_idx[0]+1):(yml_idx[1])]))\n\ndef auth_gapi_token(client_email, private_key_id, private_key):\n    payload: dict = {\n        \"iss\": client_email,\n        \"scope\": \"https://www.googleapis.com/auth/drive\",\n        \"aud\": \"https://oauth2.googleapis.com/token\",\n        \"iat\": int(time()),\n        \"exp\": int(time() + 3599)\n    }\n    headers: dict[str, str] = {'kid': private_key_id}\n\n    signed_jwt: bytes = jwt.encode(\n        payload=payload,\n        key=private_key.replace(\"\\\\n\", \"\\n\"),\n        algorithm=\"RS256\",\n        headers=headers\n    )\n\n    body: dict = {\n        \"grant_type\": \"urn:ietf:params:oauth:grant-type:jwt-bearer\",\n        \"assertion\": signed_jwt\n    }\n    response: requests.Response = requests.request(\n        \"POST\", \"https://oauth2.googleapis.com/token\", json=body\n    )\n\n    response.raise_for_status()\n\n    content = response.json()\n    return content.get('access_token')\n\ndef read_gsheet(ssid, ranges, token):\n    url = f\"https://sheets.googleapis.com/v4/spreadsheets/{ssid}/values/{ranges}\"\n    headers = {\n        \"Authorization\": f\"Bearer {token}\"\n    }\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n\ndef append_gsheet(ssid, ranges, data, token):\n    url = f\"https://sheets.googleapis.com/v4/spreadsheets/{ssid}/values/{ranges}:append\"\n\n    body = {\n        \"range\": ranges,\n        \"majorDimension\": \"ROWS\",\n        \"values\": data\n    }\n    headers = {\n        \"Authorization\": f\"Bearer {token}\"\n    }\n    response = requests.post(url, params={\"valueInputOption\": \"RAW\"}, headers=headers, json=body)\n    response.raise_for_status()\n\ndef create_linkedin_post(post):\n    linkedin_user_id = os.getenv(\"LINKEDIN_USER_ID\")\n    linkedin_token = os.getenv(\"LINKEDIN_TOKEN\")\n    linkedin_post_endpoint = \"https://api.linkedin.com/v2/ugcPosts\"\n\n    rmd_file = os.listdir(f\"./_{post['path']}\")\n    rmd_file = list(filter(lambda x: \".rmd\" in x.lower(), rmd_file))[0]\n\n    rmd_yml = read_rmd_yml(f\"./_{post['path']}/{rmd_file}\")\n    post_note = \"The post was created by Github Actions.\\nhttps://github.com/wilsonkkyip/wilsonkkyip.github.io\"\n    abstract = rmd_yml[\"abstract\"] + f\"\\n\\n{post_note}\"\n\n    body = build_post_body(\n        user_id=linkedin_user_id,\n        post_content=abstract,\n        media_title=rmd_yml[\"title\"],\n        media_description=rmd_yml[\"description\"],\n        article_url=f\"https://wilsonkkyip.github.io/{post['path']}\"\n    )\n\n    headers = {\n        \"X-Restli-Protocol-Version\": \"2.0.0\",\n        \"Authorization\": \"Bearer \" + linkedin_token \n    }\n\n    response = requests.post(\n        url=linkedin_post_endpoint, \n        json=body, \n        headers=headers\n    )\n\n    content = response.json()\n\n    return content\n\n\ndef main():\n    gcp_client_email = os.getenv(\"GCP_CLIENT_EMAIL\")\n    gcp_private_key_id = os.getenv(\"GCP_PRIVATE_KEY_ID\")\n    gcp_private_key = os.getenv(\"GCP_PRIVATE_KEY\")\n\n    log_ssid = os.getenv(\"LINKEDIN_POSTS_LOG_SSID\")\n    log_range = os.getenv(\"LINKEDIN_POSTS_LOG_RANGE\")\n\n    gcp_token = auth_gapi_token(\n        gcp_client_email, gcp_private_key_id, gcp_private_key\n    )\n\n    logs = read_gsheet(log_ssid, log_range, gcp_token)\n    linkedin_posts = [\n        {logs[\"values\"][0][0]: x[0], logs[\"values\"][0][1]: x[1]} for x in logs[\"values\"][1:]\n    ]\n\n    with open(\"./posts/posts.json\", \"r\") as file:\n        page_posts = json.loads(file.read())\n\n    missing_post = find_latest_missing_post(page_posts, linkedin_posts)\n\n    if missing_post:\n        response = create_linkedin_post(missing_post)\n        appending_data = [[missing_post[\"path\"], response.get(\"id\")]]\n        append_gsheet(log_ssid, log_range, appending_data, gcp_token)\n\nif __name__ == \"__main__\": \n    main()\n\n\n\n\n",
    "preview": "posts/2023-08-03-github-actions-with-example/img/github-actions.png",
    "last_modified": "2023-08-03T17:50:00+00:00",
    "input_file": "github-actions-with-example.knit.md"
  },
  {
    "path": "posts/2023-07-29-rust-gapi-oauth2/",
    "title": "Google OAuth2 Implementation on Rust Reqwest",
    "description": "An implementation of Google OAuth2 procedures on Rust reqwest for Server-side Web Apps and Service Accounts.",
    "author": [
      {
        "name": "Wilson Yip",
        "url": {}
      }
    ],
    "date": "2023-07-29",
    "categories": [
      "rust",
      "oauth2",
      "google-oauth"
    ],
    "contents": "\n\nContents\nIntroduction\nOAuth2 Procedures\nCreate Google Cloud Project\nSelect Required Library\nCreate OAuth Client ID Secrets\nConfigure OAuth Consent Screen\nCreate OAuth Client ID for Users\n\nService Account\n\nRust Reqwest Implementation\nAuthorise Client Application (Client ID)\nAuthorise Client Application (Refresh Token)\nAuthorise Service Account\n\nExamples\n\n\n\n\nIntroduction\nIt comes to me on many occasions that Google APIs are required to complete my tasks. API keys may be an easy choice for those non-sensitive scopes (for example calling YouTube API for some public videos and channels). But when it comes to handling files in Google Drive, things become complicated as the service requires authentication and authorisation. This article aims to provide a solution on obtaining an authorised token to be put in http requestsâ€™ header for calling the Google APIsâ€™ sensitive scopes in Rust environment.\nA Github repo was created for the purpose. It can be used in CLI environment and imported as rust crate as well. Below will first briefly describe the OAuth2 procedures, then walk through some important script, and finally will show some examples using of the crate.\nOAuth2 Procedures\nWhen I first encountered OAuth2, I was confused about what scopes and endpoints are because both scopes and endpoints are represented by url-like strings in Google APIs. In a nutshell, endpoints represent what services you want to use. For example there is a specific endpoint for reading the metadata of a file in Google Drive; there is another endpoint for you to update the file. On the other hand, scopes are the abilities of your authorised token. For example, is your token able to read the files from Google Drive? It depends on whether your token contains the specific scope.\nGoogle separates the authorisation method for server-to-server interactions and user-to-server interactions. We will use a service account for the prior situation and a client secret for the later one. Both can be represented by a JSON file. To obtain these JSON files, we first need to create a Google Cloud Project. Then within the project, we can create the secret JSON files.\n\n\n\n\n\n\nCreate Google Cloud Project\nGo to https://console.developers.google.com and click Select a project.\nClick New Project.\nEnter the Project name and click Create.\n\n\n\n\n\n\n\n\n\n\n\n\nSelect Required Library\nUnder APIs and service, click Library.\nSearch the API library(ies) you wish to use. In this example, we choose Google Drive API.\nClick the library you want.\nClick Enable.\n\n\n\n\n\n\n\n\n\n\n\n\nCreate OAuth Client ID Secrets\nNow we have created a project and picked the required libraries. This section will show how to obtain a client_secret of the application for users to authorise. In order to do so, we need to first configure an OAuth consent screen to inform users about the name of the application and which scopes will be used by the application when they do the authorisation. Then we will create the application secret (or client_secret) for this application.\nConfigure OAuth Consent Screen\nUnder APIs and services, Credentials, click Configure consent screen or OAuth consent screen.\nIf you are within an organisation, you can pick Internal or External as User Type. Otherwise, you can only pick External.\nFor internal apps, it is only available to users within the organisation. But the app is not required to have any privacy policy.\nFor external apps, you can add at most 100 test users for testing the application before published. But the refresh_token obtained from the authorisation and authentication process is only valid for 1 week only.\n\nEnter the App name and User support email.\nScroll down and enter the developer contact information and click Save and continue.\nClick Add or remove scopes.\nSelect the scopes you want to use.\nClick Save and continue.\n(For external apps only) Click Add users as test users for the application.\n(For external apps only) Enter the email address(es) for the test user(s). Then click Add.\nClick Save and continue.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCreate OAuth Client ID for Users\nUnder APIs and services, Credentials, click Create Credentials, then click OAuth client ID.\nSelect Web application as Application type and enter the name of the application.\nScroll down and enter the Authorised redirect URIs. Please put a slash (/) at the end of the uri. Then click Create.\nFinally click Download JSON.\n\n\n\n\n\n\n\n\n\n\n\n\nService Account\nThis section describe how to obtain a service account JSON. If you wish to handle usersâ€™ data, please follow this section.\nUnder APIs and services, Credentials, click Create Credentials, then click Service Account.\nInsert the name, account id, and description of the service account. Then click Done.\nClick the newly created service account email.\nClick Keys, then clickAdd key and Create new key.\nSelect JSON as key type and click Create to download the service account JSON.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRust Reqwest Implementation\nNow we have obtained the secret JSON (either a client_seceret or a service_account or both). Depends on which type of secret we have, the authorisation methods are different.\nAuthorise Client Application (Client ID)\nFor authorising a client application (see this figure), we need to\nBuild a url with the follow query parameters:\nclient_id (the identification of the client application)\nredirect_uri (those we specified in step 3 in this section, put 1 uri here only)\nscope (the scopes the application wants to use; space-delimited if more than one is used)\naccess_type (either online of offline. A refresh_token will be obtained in later step for acquiring updated access token without another consent from the users)\n\nYou can specified more parameters for different configuration. See more from here.\nSend a request to Google OAuth page using the above url. Google will also ask for user consent in this stage.\nGoogle returns an authorisation code to the redirect_uri we specified above.\nSend another request to Google with the authorisation code obtained from the last step to exchange an access_token (and refresh_token if specified in step 1).\nThis access_token can use used to access the authorised endpoints.\n\n\n\nFigure 1: Authorise client application\n\n\n\nAmong the JSON obtained from this section, we create the following struct for the key-value pairs.\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct ClientSecret {\n    pub client_id: String,\n    pub project_id: String,\n    pub auth_uri: String,\n    pub token_uri: String,\n    pub auth_provider_x509_cert_url: String,\n    pub client_secret: String,\n    pub redirect_uris: Vec<String>\n}\n\nThen we implement a method to the above struct to build the url for step 1.\n\npub fn auth_url(&self, scope: &str) -> String {\n    let params: HashMap<_,_> = HashMap::from([\n        (\"response_type\", \"code\"),\n        (\"access_type\", \"offline\"), // set 'offline' to obtain 'refresh_token'\n        (\"prompt\", \"consent\"),\n        (\"client_id\", &self.client_id),\n        (\"redirect_uri\", &self.redirect_uris[0]),\n        (\"scope\", &scope),\n        (\"state\", &self.client_id)\n    ]);\n\n    let url = reqwest::Url::parse_with_params(\n        &self.auth_uri, params\n    ).expect(\"Failed to parse auth url.\").to_string();\n    \n    return url;\n}\n\nNow we need to print out the above url and set up a http server to listen from Googleâ€™s response with the authorisation code to finish step 3.\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct AuthCode {\n    pub code: String,\n    pub scope: String\n}\n\npub fn auth_code(&self, scope: &str, port: u32) -> Result<AuthCode, std::io::Error> {\n    let auth_url: String = self.auth_url(scope);\n    println!(\"Please visit this URL to authorize this application: {}\", auth_url);\n\n    let listener: TcpListener = \n        TcpListener::bind(format!(\"localhost:{}\", port))\n            .expect(\"Failed to bind to port\");\n    \n    let (mut stream, _) = listener.accept().unwrap();\n    let mut buf = [0;2048];\n    stream.read(&mut buf).unwrap();\n\n    let buf_str: String = String::from_utf8_lossy(&buf[..]).to_string();\n    let buf_vec: Vec<&str> = buf_str\n        .split(\" \")\n        .collect::<Vec<&str>>();\n\n    let args: String = buf_vec[1].to_string();\n    let callback_url: Url = Url::parse(\n        (format!(\"http://localhost:{}\", port) + &args).as_str()\n    ).expect(\"Failed to parse callback URL\");\n    let query: HashMap<_,_> = callback_url.query_pairs().into_owned().collect();\n    let output = AuthCode {\n        code: query.get(\"code\").unwrap().to_string(),\n        scope: query.get(\"scope\").unwrap().to_string()\n    };\n    return Ok(output);\n}\n\nFor step 4, the following function will prepare a POST request to Google to exchange the authorisation code for the access_token (and refresh_token).\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct ClientSecretTokenResponse {\n    pub access_token: String,\n    pub expires_in: i64,\n    pub refresh_token: String,\n    pub scope: String,\n    pub token_type: String\n}\n\npub async fn auth_token(&self, code: &str) -> Result<ClientSecretTokenResponse, reqwest::Error> {\n    let body: Value = serde_json::json!({\n        \"client_id\": self.client_id,\n        \"client_secret\": self.client_secret,\n        \"code\": code,\n        \"grant_type\": \"authorization_code\",\n        \"redirect_uri\": self.redirect_uris[0]\n    });\n\n    let response = reqwest::Client::new()\n        .post(self.token_uri.as_str())\n        .json(&body)\n        .send()\n        .await?;\n\n    let content: ClientSecretTokenResponse = response.json()\n        .await.expect(\"Failed to parse http response\");\n\n    return Ok(content);\n}\n\nAuthorise Client Application (Refresh Token)\nWhen we obtained the refresh_token from the above, we can further request a new access_token when the previous one is expired. To do do, we first define a struct and implement an auth function to it.\n\npub const OAUTH_TOKEN_URL: &str = \"https://oauth2.googleapis.com/token\";\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct Token {\n    pub access_token: String,\n    pub expires_in: i64\n}\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct UserSecret {\n    pub client_id: String,\n    pub client_secret: String,\n    pub refresh_token: String\n}\n\npub async fn auth(&self) -> Result<Token, reqwest::Error> {\n    // Prepare auth body\n    let mut body: Value = serde_json::to_value(&self)\n        .expect(\"Could not convert UserSecret to Value\");\n    body[\"grant_type\"] = Value::String(\"refresh_token\".to_string());\n\n    // Auth request\n    let response: reqwest::Response = reqwest::Client::new()\n        .post(OAUTH_TOKEN_URL)\n        .json(&body)\n        .send()\n        .await?;\n\n    // Parse response to output\n    let content: Token = response.json().await?;\n\n    return Ok(content)\n}\n\nAuthorise Service Account\nFor authorising a service account (see this figure), we need to\nPrepare a JWT token. The token is separated into 3 parts:\nHeader: consist of the algorithm name and the privated_key_id (from the secret JSON).\nClaim: consist of client_email, scope, aud, iat and exp.\nKey: the private_key from the secret JSON.\n\nUse the JWT token to exchange the access_token.\n\n\n\nFigure 2: Authorise service account\n\n\n\nBelow shows the implementation.\n\n#[derive(Debug, Deserialize, Serialize)]\npub struct ServiceSecret {\n    pub client_email: String,\n    pub private_key_id: String,\n    pub private_key: String\n}\n\npub async fn auth(&self, scope: &str) -> Result<Token, reqwest::Error> {\n    // Auth Service Account\n    // https://developers.google.com/identity/protocols/oauth2/service-account\n\n    // Prepare JWT claim\n    let claim: serde_json::Value = serde_json::json!({\n        \"iss\": self.client_email.to_string(),\n        \"scope\": scope.to_string(),\n        \"aud\": \"https://oauth2.googleapis.com/token\".to_string(),\n        \"iat\": chrono::offset::Utc::now().timestamp(),\n        \"exp\": chrono::offset::Utc::now().timestamp() + 3600\n    });\n\n    // Prepare JWT header\n    let header: Header = Header{\n        alg: Algorithm::RS256,\n        kid: Some(self.private_key_id.to_string()),\n        ..Default::default()\n    };\n\n    // Prepare JWT key\n    let key: EncodingKey = EncodingKey::from_rsa_pem(\n        &self.private_key\n            .to_string()\n            .replace(\"\\\\n\", \"\\n\").as_bytes()\n    ).expect(\"Cannot build `EncodingKey`.\");\n\n    // Generate JWT\n    let token: String = encode(\n        &header, &claim, &key\n    ).expect(\"Cannot encode `token`.\");\n\n    // Auth JWT\n    let response: Response = reqwest::Client::new()\n        .post(OAUTH_TOKEN_URL)\n        .json(&serde_json::json!({\n            \"grant_type\": \"urn:ietf:params:oauth:grant-type:jwt-bearer\",\n            \"assertion\": token\n        }))\n        .send()\n        .await?;\n    \n    // Prepare output\n    let content: Token = match response.status() {\n        StatusCode::OK => response.json().await.expect(\"Unable to parse HTTP response JSON.\"),\n        StatusCode::UNAUTHORIZED => {\n            println!(\"{}\", response.text().await.unwrap());\n            panic!(\"HTTP request failed: Unauthorized.\");\n        },\n        _ => {\n            println!(\"{}\", response.text().await.unwrap());\n            panic!(\"HTTP request failed.\");\n        }\n    };\n\n    return Ok(content);\n}\n\nExamples\nA main.rs was also written in the Github repo to provide accessibility from command prompt.\n\ncargo run \n\n# Usage: gapi-oauth <SERVICE> <JSON_PATH> [SCOPE] [PORT]\n# \n# SERVICE: `user`, `service`, or `consent`\n# JSON_PATH: The path to the JSON file containing the credentials.\n# SCOPE: Only required for `service` and `consent`\n# PORT: Only required for `consent`\n\n\ncargo run user /path/to/client_token.json\n\n# {\n#   \"access_token\": \"...\",\n#   \"expires_in\": 3599\n# }\n\n\ncargo run user /path/to/service_acc.json 'https://www.googleapis.com/auth/drive'\n\n# {\n#   \"access_token\": \"...\",\n#   \"expires_in\": 3599\n# }\n\n\ncargo run consent /path/to/client_secret.json 'https://www.googleapis.com/auth/drive' 8088\n\n# Please visit this URL to authorize this application: \n# https://accounts.google.com/o/oauth2/auth?client_id=&prompt=consent&...\n# \n# {\n#   \"access_token\": \"...\",\n#   \"refresh_token\": \"...\",\n#   \"scopes\": [\n#     \"https://www.googleapis.com/auth/drive\"\n#   ],\n#   \"expiry\": \"2023-07-30T17:51:13.123456Z\",\n#   \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n#   \"token_uri\": \"https://oauth2.googleapis.com/token\",\n#   \"client_id\": \"...\",\n#   \"client_secret\": \"...\"\n# }\n\nIt can also be used as crate. After constructing the UserSecret or ServiceSecret, simply use the corresponding auth method to return the access_token.\n\nuse crate::auth_users::UserSecret;\nuse crate::auth_service::ServiceSecret;\n\n#[tokio::test]\nasync fn test_auth_user() {\n    let client_id = std::env::var(\"USER_CLIENT_ID\")\n        .expect(\"No USER_CLIENT_ID in env var\")\n        .as_str().to_string();\n    let client_secret = std::env::var(\"USER_CLIENT_SECRET\")\n        .expect(\"No USER_CLIENT_SECRET in env var\")\n        .as_str().to_string();\n    let refresh_token = std::env::var(\"USER_REFRESH_TOKEN\")\n        .expect(\"No USER_REFRESH_TOKEN in env var\")\n\n    // Construct UserSecret\n    let client_token = UserSecret {\n        client_id: client_id,\n        client_secret: client_secret,\n        refresh_token: refresh_token,\n    };\n\n    // Auth to Token, will panic if failed.\n    let _token = client_token.auth().await\n        .expect(\"Unable to authenticate\");\n}\n\n#[tokio::test]\nasync fn test_auth_service() {\n    let client_email = std::env::var(\"SERVICE_CLIENT_EMAIL\")\n        .expect(\"No SERVICE_CLIENT_EMAIL in env var\")\n        .as_str().to_string();\n    let private_key = std::env::var(\"SERVICE_PRIVATE_KEY\")\n        .expect(\"No SERVICE_PRIVATE_KEY in env var\")\n        .as_str().to_string();\n    let private_key_id = std::env::var(\"SERVICE_PRIVATE_KEY_ID\")\n        .expect(\"No SERVICE_PRIVATE_KEY_ID in env var\")\n        .as_str().to_string();\n\n    let service_secret = ServiceSecret {\n        client_email: client_email,\n        private_key: private_key,\n        private_key_id: private_key_id,\n    };\n\n    let scopes: Vec<String> = vec![\n        \"https://www.googleapis.com/auth/drive\".to_string(),\n        \"https://www.googleapis.com/auth/youtube\".to_string()\n    ];\n\n    let scope = scopes.join(\" \");\n\n    let _token = service_secret.auth(&scope).await\n        .expect(\"Unable to authenticate\");\n}\n\n\n\n\n",
    "preview": "posts/2023-07-29-rust-gapi-oauth2/img/auth_client_id.png",
    "last_modified": "2023-08-04T09:24:09+00:00",
    "input_file": "rust-gapi-oauth2.knit.md"
  }
]
